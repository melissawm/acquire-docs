{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Acquire Docs","text":""},{"location":"#guides","title":"Guides","text":"Get Started <p>Install Acquire and use simulated cameras</p> Get Started API Reference <p>Information on classes and methods</p> API Reference Tutorials <p>Guides on using Acquire for specific tasks</p> Tutorials For contributors <p>Learn how to contribute code or documentation to Acquire</p> For contributors"},{"location":"#about-acquire","title":"About Acquire","text":"<p>Acquire (<code>acquire-imaging</code> on PyPI) provides high-speed, multi-camera, video streaming and image acquisition with a programming interface for streaming video data directly to napari, Python and cloud-friendly file formats.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install Acquire on Windows, macOS, or Ubuntu, simply run the following command:</p> <pre><code>python -m pip install acquire-imaging\n</code></pre>"},{"location":"#supported-cameras-and-file-formats","title":"Supported Cameras and File Formats","text":"<p>Acquire supports the following cameras (currently only on Windows):</p> <ul> <li>Hamamatsu Orca Fusion BT (C15440-20UP)</li> <li>Vieworks VC-151MX-M6H00</li> <li>FLIR Blackfly USB3 (BFLY-U3-23S6M-C)</li> <li>FLIR Oryx 10GigE (ORX-10GS-51S5M-C)</li> </ul> <p>For testing and demonstration purposes, Acquire also provides a few simulated video sources. For more information on supported cameras and video sources, check out this tutorial.</p> <p>Acquire supports the following output file formats:</p> <ul> <li>Tiff</li> <li>OME-Zarr for Zarr v2</li> <li>Zarr v3</li> </ul> <p>Acquire also supports raw and trash storage devices. For more information on supported file formats and storage devices, check out this tutorial.</p>"},{"location":"#citing-acquire","title":"Citing Acquire","text":"<pre><code>cff-version: 1.2.0\ntitle: Acquire: a multi-camera video streaming software focusing on microscopy\nmessage: &gt;-\n  If you use this software, please cite it using the\n  metadata from this file.\ntype: software\nauthors:\n  - given-names: Nathan\n    family-names: Clack\n    email: nclack@chanzuckerberg.com\n    affiliation: Chan-Zuckerberg Initiative Foundation\n    orcid: 'https://orcid.org/0000-0001-6236-9282'\n  - given-names: Alan\n    family-names: Liddell\n    email: aliddell@chanzuckerberg.com\n    affiliation: Chan-Zuckerberg Initiative Foundation\n  - given-names: Andrew\n    family-names: Sweet\n    email: andrewdsweet@gmail.com\n    affiliation: Chan-Zuckerberg Initiative Foundation\nrepository-code: 'https://github.com/acquire-project/acquire-python'\nrepository-artifact: 'https://pypi.org/project/acquire-imaging/'\nabstract: &gt;-\n  acquire-imaging is a library focusing on multi-camera video\n  streaming for microscopy.\nlicense: Apache-2.0\n</code></pre>"},{"location":"#acquire-license","title":"Acquire License","text":"<p><code>Acquire</code> is provided under an Apache 2.0 license. You can learn more about the Apache license in the documentation here.</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>Information on the classes in the <code>acquire</code> module along with the attributes and methods associated with them.</p>"},{"location":"api_reference/#acquire.AvailableData","title":"<code>acquire.AvailableData</code>","text":"<p>The AvailableData class represents the collection of frames that have been captured since the last call to <code>runtime.get_available_data()</code>.</p> <p><code>AvailableData</code> objects should be set to have a short lifetime, since these objects reserve space on the video queue and will eventually block camera acquisition to ensure no data is overwritten before it can be processed.</p>"},{"location":"api_reference/#acquire.AvailableData.frames","title":"<code>frames() -&gt; Iterator[VideoFrame]</code>","text":"<p>Returns an iterator over the video frames in the available data.</p> <p>Returns:</p> Type Description <code>Iterator[VideoFrame]</code> <p>An iterator over the video frames in the available data.</p>"},{"location":"api_reference/#acquire.AvailableData.get_frame_count","title":"<code>get_frame_count() -&gt; int</code>","text":"<p>Returns the total number of video frames in the available data.</p> <p>Call <code>get_frame_count()</code> to query the number of frames in a <code>AvailableData</code> object.</p> <p>Returns:</p> Type Description <code>int</code> <p>The total number of video frames in the AvailableData object.</p>"},{"location":"api_reference/#acquire.AvailableDataContext","title":"<code>acquire.AvailableDataContext</code>","text":"<p>The <code>AvailableDataContext</code> class is the context manager for available data for the given VideoStream ID.</p>"},{"location":"api_reference/#acquire.Camera","title":"<code>acquire.Camera(*args: None, **kwargs: Any)</code>","text":"<p>The <code>Camera</code> class is used to describe cameras or other video sources.</p> <p>Attributes:</p> Name Type Description <code>identifier</code> <code>Optional[DeviceIdentifier]</code> <p>An optional attribute which contains an instance of the <code>DeviceIdentifier</code> class. <code>DeviceIdentifier</code> has <code>id</code> and <code>kind</code> attributes assigned by <code>acquire</code> if the device is natively supported. Otherwise, it is of type <code>None</code>.</p> <code>settings</code> <code>CameraProperties</code> <p>An instance of the <code>CameraProperties</code> class which contains the settings for the camera.</p> <p>Initializes a Camera object with optional arguments.</p>"},{"location":"api_reference/#acquire.CameraCapabilities","title":"<code>acquire.CameraCapabilities</code>","text":"<p>The <code>CameraCapabilities</code> class is used to describe the camera's supported properties.</p> <p>Attributes:</p> Name Type Description <code>exposure_time_us</code> <code>Property</code> <p>An instance of the <code>Property</code> class that captures the range and type of supported values in microseconds for the camera's exposure time, which is how long in microseconds the camera collects light from the sample for a single frame.</p> <code>line_interval_us</code> <code>Property</code> <p>An instance of the <code>Property</code> class that captures the range and type of supported values in microseconds for a rolling shutter camera to scan one line.</p> <code>readout_direction</code> <code>Property</code> <p>An instance of the <code>Property</code> class that specifies whether the data is read out of the camera forwards or backwards and if that direction can be chosen by the user.</p> <code>binning</code> <code>Property</code> <p>An instance of the <code>Property</code> class that captures the range and type of support values for binning, which is combining adjacent pixels by averaging in each direction, and whether the binning factor can be chosen by the user.</p> <code>offset</code> <code>OffsetCapabilities</code> <p>An instance of the <code>OffsetShapeCapabilities</code> class that represents the horizontal and vertical offset for the region of interest on the camera chip.</p> <code>shape</code> <code>ShapeCapabilities</code> <p>An instance of the <code>OffsetShapeCapabilities</code> class that represents the width and height of the region of interest on the camera chip.</p> <code>supported_pixel_types</code> <code>List[SampleType]</code> <p>A list containing instances of the <code>SampleType</code> class representing each of the supported pixel types, such as 8-bit unsigned integer (uint8).</p> <code>digital_lines</code> <code>DigitalLineCapabilities</code> <p>An instance of the <code>DigitalLineCapabilities</code> class which indicates the number and names of the available lines. Up to 8 lines are supported with the last line typically being the camera software trigger.</p> <code>triggers</code> <code>TriggerCapabilities</code> <p>An instance of the <code>TriggerCapabilities</code> class which indicate what kinds of triggers (start acquisition, start exposure, or start a frame) are supported.</p>"},{"location":"api_reference/#acquire.CameraProperties","title":"<code>acquire.CameraProperties(*args: None, **kwargs: Any)</code>","text":"<p>The <code>CameraProperties</code> class is used to set the desired camera properties for acquisition.</p> <p>Attributes:</p> Name Type Description <code>exposure_time_us</code> <code>float</code> <p>How long in microseconds your camera should collect light from the sample. However, for simulated cameras, this is just a waiting period before generating the next frame.</p> <code>line_interval_us</code> <code>float</code> <p>The time to scan one line in microseconds in a rolling shutter camera.</p> <code>binning</code> <code>float</code> <p>How many adjacent pixels in each direction to combine by averaging. For example, if <code>binning</code> is set to 2, a 2x2 square of pixels will be combined by averaging. If <code>binning</code> is set to 1, no pixels will be combined.</p> <code>pixel_type</code> <code>SampleType</code> <p>An instance of the <code>SampleType</code> class which specifies the numerical data type, for example Uint16, a 16-bit unsigned integer type.</p> <code>readout_direction</code> <code>Direction</code> <p>An instance of the <code>Direction</code> class which specifies whether the data is readout forwards or backwards.</p> <code>offset</code> <code>Tuple[int, int]</code> <p>A tuple of two integers representing the (x, y) offset in pixels of the image region of interest on the camera.</p> <code>shape</code> <code>Tuple[int, int]</code> <p>A tuple of two integers representing the (x, y)size in pixels of the image region of interest on the camera.</p> <code>input_triggers</code> <code>InputTriggers</code> <p>An instance of the <code>InputTriggers</code> class, which describes the trigger signals for starting acquisition, camera exposure, and acquiring a frame.</p> <code>output_triggers</code> <code>OutputTriggers</code> <p>An instance of the <code>OutputTriggers</code> class, which describes the trigger signals for the camera exposure, acquiring a frame, as well as any wait times for sending the trigger signal.</p> <p>Initializes a CameraProperties object with optional arguments.</p>"},{"location":"api_reference/#acquire.Capabilities","title":"<code>acquire.Capabilities(*args: None, **kwargs: Any)</code>","text":"<p>The <code>Capabilities</code> class contains representations of each of the 2 supported VideoStream objects.</p> <p>Attributes:</p> Name Type Description <code>video</code> <code>Tuple[VideoStreamCapabilities, VideoStreamCapabilities]</code> <p>A tuple containing two <code>VideoStreamCapabilities</code> instances since <code>acquire</code> supports simultaneous streaming from 2 video sources.</p> <p>Initializes a Capabilities object with optional arguments.</p>"},{"location":"api_reference/#acquire.DeviceIdentifier","title":"<code>acquire.DeviceIdentifier(*args: None, **kwargs: Any)</code>","text":"<p>Represents an identifier for a supported device, including its unique id and type, such as a camera or storage.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Tuple[int, int]</code> <p>A tuple of <code>(driver_id, device_id)</code> containing two Uint8 integers that serve to identify each driver and device uniquely for a given run.</p> <code>kind</code> <code>DeviceKind</code> <p>An instance of the <code>DeviceKind</code> class that represents the type or kind of the device.</p> <code>name</code> <code>str</code> <p>A string representing the name or label of the device.</p> <p>Initializes a DeviceIdentifier object with optional arguments.</p>"},{"location":"api_reference/#acquire.DeviceIdentifier.none","title":"<code>none() -&gt; DeviceIdentifier</code>","text":"<p>Returns a \"None\" type DeviceIdentifier.</p> <p>Useful when a DeviceIdentifier is not needed.</p>"},{"location":"api_reference/#acquire.DeviceKind","title":"<code>acquire.DeviceKind(*args: None, **kwargs: Any)</code>","text":"<p>This class represents the types of devices in a given system.</p> <p>Attributes:</p> Name Type Description <code>Camera</code> <code>DeviceKind</code> <p>Enum-type class variable of <code>DeviceKind</code> that specifies a device is a camera.</p> <code>NONE</code> <code>DeviceKind</code> <p>Enum-type class variable of <code>DeviceKind</code> for if a device's kind is unavailable.</p> <code>Signals</code> <code>DeviceKind</code> <p>Enum-type class variable of <code>DeviceKind</code> that specifies a device is a signal.</p> <code>StageAxis</code> <code>DeviceKind</code> <p>Enum-type class variable of <code>DeviceKind</code> that specifies a device is a stage.</p> <code>Storage</code> <code>DeviceKind</code> <p>Enum-type class variable of <code>DeviceKind</code> that specifies a device is for storage.</p> <p>Initializes the DeviceKind class.</p>"},{"location":"api_reference/#acquire.DeviceManager","title":"<code>acquire.DeviceManager</code>","text":"<p>The <code>DeviceManager</code> class manages selection of available devices in the system.</p> <p>Regular expressions are accepted for the name argument.</p>"},{"location":"api_reference/#acquire.DeviceManager.devices","title":"<code>devices() -&gt; List[DeviceIdentifier]</code>","text":"<p>Returns a list of all available device identifiers.</p>"},{"location":"api_reference/#acquire.DeviceManager.select","title":"<code>select(kind: DeviceKind, name: Optional[str] = None) -&gt; Optional[DeviceIdentifier]</code>","text":"<p>Selects a specified device.</p> <p>Call this method to choose the first available device of a given type or to select a specific device by name.</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>DeviceKind</code> <p>The type of device to select.</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the device to select. Regular expressions supported.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[DeviceIdentifier]</code> <p>The selected device identifier, or None if the specified device is</p> <code>Optional[DeviceIdentifier]</code> <p>not available.</p>"},{"location":"api_reference/#acquire.DeviceManager.select_one_of","title":"<code>select_one_of(kind: DeviceKind, names: List[str]) -&gt; Optional[DeviceIdentifier]</code>","text":"<p>Selects the first device in the list of devices that is of one of the specified kinds.</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>DeviceKind</code> <p>The type of device to select.</p> required <code>names</code> <code>List[str]</code> <p>A list of device names to choose from. Regular expressions supported.</p> required <p>Returns:</p> Type Description <code>Optional[DeviceIdentifier]</code> <p>Optional[DeviceIdentifier]: The selected device identifier, or None if none of the specified devices are available.</p>"},{"location":"api_reference/#acquire.DeviceState","title":"<code>acquire.DeviceState</code>","text":"<p>The <code>DeviceState</code> class represents the acquisition status of a device.</p> <p>Attributes:</p> Name Type Description <code>Closed</code> <code>DeviceState</code> <p>Enum-type class variable of <code>DeviceState</code> that specifies when a device is not ready for configuration.</p> <code>AwaitingConfiguration</code> <code>DeviceState</code> <p>Enum-type class variable of <code>DeviceState</code> that specifies when a device is ready for configuration.</p> <code>Armed</code> <code>DeviceState</code> <p>Enum-type class variable of <code>DeviceState</code> that specifies when a device is ready to stream data.</p> <code>Running</code> <code>DeviceState</code> <p>Enum-type class variable of <code>DeviceState</code> that specifies when a device is streaming data.</p>"},{"location":"api_reference/#acquire.DigitalLineCapabilities","title":"<code>acquire.DigitalLineCapabilities</code>","text":"<p>The <code>DigitalLineCapabilities</code> class represents the digital lines supported by the device.</p> <p>Attributes:</p> Name Type Description <code>line_count</code> <code>int</code> <p>Integer number representing the number of digital lines supported.</p> <code>names</code> <code>Tuple[str, ...]</code> <p>Tuple of strings to name each of the digital lines, typically the last one is the camera software trigger.</p>"},{"location":"api_reference/#acquire.DimensionType","title":"<code>acquire.DimensionType(*args: None, **kwargs: Any)</code>","text":"<p>Used to specify the physical meaning of a dimension, such as space or time dimension.</p> <p>When downsampling, Space and Time dimensions are downsampled by the same factor. Channel and Other dimensions are not downsampled.</p> <p>This value is also reflected in the dimension metadata of an OME-Zarr dataset.</p> <p>Attributes:</p> Name Type Description <code>Space</code> <code>DimensionType</code> <p>Enum-type class variable of <code>DimensionType</code> that indicates a spatial dimension.</p> <code>Channel</code> <code>DimensionType</code> <p>Enum-type class variable of <code>DimensionType</code> that indicates a color channel dimension.</p> <code>Time</code> <code>DimensionType</code> <p>Enum-type class variable of <code>DimensionType</code> that indicates a time dimension.</p> <code>Other</code> <code>DimensionType</code> <p>Enum-type class variable of <code>DimensionType</code> that indicates the dimension is not a space, channel, or time.</p>"},{"location":"api_reference/#acquire.Direction","title":"<code>acquire.Direction</code>","text":"<p>The direction that data is read for streaming.</p> <p>Attributes:</p> Name Type Description <code>Backward</code> <code>Direction</code> <p>Enum-type class variable of <code>Direction</code> that specifies when data is streamed backward.</p> <code>Forward</code> <code>Direction</code> <p>Enum-type class variable of <code>Direction</code> that specifies when data is streamed forward.</p>"},{"location":"api_reference/#acquire.InputTriggers","title":"<code>acquire.InputTriggers</code>","text":"<p>The <code>InputTriggers</code> class represents input triggers for a camera device.</p> <p>Attributes:</p> Name Type Description <code>acquisition_start</code> <code>Trigger</code> <p>An instance of the <code>Trigger</code> class representing the trigger for starting acquisition.</p> <code>exposure</code> <code>Trigger</code> <p>An instance of the <code>Trigger</code> class representing the trigger for exposure.</p> <code>frame_start</code> <code>Trigger</code> <p>An instance of the <code>Trigger</code> class representing the trigger for starting a frame.</p>"},{"location":"api_reference/#acquire.OffsetCapabilities","title":"<code>acquire.OffsetCapabilities</code>","text":""},{"location":"api_reference/#acquire.OutputTriggers","title":"<code>acquire.OutputTriggers</code>","text":"<p>The <code>OutputTriggers</code> class represents output triggers for a camera device.</p> <p>Attributes:</p> Name Type Description <code>exposure</code> <code>Trigger</code> <p>An instance of the <code>Trigger</code> class representing the trigger for exposure.</p> <code>frame_start</code> <code>Trigger</code> <p>An instance of the <code>Trigger</code> class representing the trigger for starting a frame.</p> <code>trigger_wait</code> <code>Trigger</code> <p>An instance of the <code>Trigger</code> class representing the trigger for waiting before continuing acquisition.</p>"},{"location":"api_reference/#acquire.PID","title":"<code>acquire.PID(*args: None, **kwargs: Any)</code>","text":"<p>The <code>PID</code> class represents proportional-integral-derivative (PID) values.</p> <p>Attributes:</p> Name Type Description <code>derivative</code> <code>float</code> <p>The derivative value for the PID.</p> <code>integral</code> <code>float</code> <p>The integral value for the PID.</p> <code>proportional</code> <code>float</code> <p>The proportional value for the PID.</p> <p>Initializes a PID object with optional arguments.</p>"},{"location":"api_reference/#acquire.Properties","title":"<code>acquire.Properties(*args: None, **kwargs: Any)</code>","text":"<p>The <code>Properties</code> class represents properties related to video streams.</p> <p>Attributes:</p> Name Type Description <code>video</code> <code>Tuple[VideoStream, VideoStream]</code> <p>A tuple containing two <code>VideoStream</code> instances since <code>acquire</code> supports simultaneous streaming from 2 video sources. <code>VideoStream</code> objects have 2 attributes <code>camera</code> and <code>storage</code> to set the source and sink for the stream.</p> <p>Initializes a <code>Properties</code> object with optional arguments.</p>"},{"location":"api_reference/#acquire.Property","title":"<code>acquire.Property(*args: None, **kwargs: Any)</code>","text":"<p>Indicates the type of and whether the property can be overwritten.</p> <p>For numerical values, it also captures the accepted range of values.</p> <p>Attributes:</p> Name Type Description <code>writable</code> <code>bool</code> <p>A boolean indicating whether the property can be written.</p> <code>low</code> <code>float</code> <p>Floating point number for the lower bound of the property, if applicable.</p> <code>high</code> <code>float</code> <p>Floating point number for the upper bound of the property, if applicable.</p> <code>kind</code> <code>PropertyType</code> <p>An instance of the <code>PropertyType</code> class which indicates the type of the property (fixed precision, floating-point, enum, or string).</p> <p>Initializes a Property object with optional arguments.</p>"},{"location":"api_reference/#acquire.PropertyType","title":"<code>acquire.PropertyType</code>","text":"<p>The <code>PropertyType</code> class indicates the type of the property (fixed precision, floating-point, enum, or string).</p> <p>Attributes:</p> Name Type Description <code>FixedPrecision</code> <code>PropertyType</code> <p>Enum-type class variable of <code>PropertyType</code> that indicates fixed precision or integer values.</p> <code>FloatingPrecision</code> <code>PropertyType</code> <p>Enum-type class variable of <code>PropertyType</code> that indicates floating point precision values.</p> <code>Enum</code> <code>PropertyType</code> <p>Enum-type class variable of <code>PropertyType</code> that indicates enum-type values.</p> <code>String</code> <code>PropertyType</code> <p>Enum-type class variable of <code>PropertyType</code> that indicates string values.</p>"},{"location":"api_reference/#acquire.Runtime","title":"<code>acquire.Runtime(*args: None, **kwargs: Any)</code>","text":"<p>Coordinates runtime.</p> <p>The <code>Runtime</code> class coordinates the devices with the storage disc including selecting the devices, setting their properties, and starting and stopping acquisition.</p> <p>Initializes the Runtime object with optional arguments.</p>"},{"location":"api_reference/#acquire.Runtime.abort","title":"<code>abort() -&gt; None</code>","text":"<p>Aborts the runtime, terminating it immediately.</p> <p>Call <code>abort()</code> to immediately end data acqusition. All objects are deleted to free up disk space upon shutdown of <code>Runtime</code>.</p>"},{"location":"api_reference/#acquire.Runtime.device_manager","title":"<code>device_manager() -&gt; DeviceManager</code>","text":"<p>Returns the DeviceManager instance associated with this Runtime.</p> <p>Call <code>device_manager()</code> to return the <code>DeviceManager</code> object associated with this <code>Runtime</code> instance.</p>"},{"location":"api_reference/#acquire.Runtime.execute_trigger","title":"<code>execute_trigger(stream_id: int) -&gt; None</code>","text":"<p>Executes a trigger for the given stream ID.</p> <p>Call <code>execute_trigger</code> with a specific <code>stream_id</code>, 0 or 1, to execute a trigger for that video source.</p>"},{"location":"api_reference/#acquire.Runtime.get_available_data","title":"<code>get_available_data(stream_id: int) -&gt; AvailableDataContext</code>","text":"<p>Returns the AvailableDataContext instance for the given stream ID.</p> <p>Call <code>get_available_data</code> with a specific <code>stream_id</code>, 0 or 1, to return the context manager, <code>AvailableDataContext</code>, associated with the 1st or 2nd video source, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>stream_id</code> <code>int</code> <p>The ID of the stream for which available data is requested.</p> required <p>Returns:</p> Name Type Description <code>AvailableDataContext</code> <code>AvailableDataContext</code> <p>Context manager for available data for the given VideoStream ID.</p>"},{"location":"api_reference/#acquire.Runtime.get_capabilities","title":"<code>get_capabilities() -&gt; Capabilities</code>","text":"<p>Returns the current capabilites of the runtime as an instance of Capabilities.</p> <p>Call <code>get_capabilities()</code> to return the <code>Capabilities</code> object associated with this <code>Runtime</code> instance.</p>"},{"location":"api_reference/#acquire.Runtime.get_configuration","title":"<code>get_configuration() -&gt; Properties</code>","text":"<p>Returns the current configuration properties of the runtime.</p> <p>Call <code>get_configuration()</code> to return the <code>Properties</code> object associated with this <code>Runtime</code> instance.</p>"},{"location":"api_reference/#acquire.Runtime.get_state","title":"<code>get_state() -&gt; DeviceState</code>","text":"<p>Returns the current state of the device.</p> <p>Call <code>get_state()</code> to return the <code>DeviceState</code> object associated with this <code>Runtime</code> instance.</p>"},{"location":"api_reference/#acquire.Runtime.set_configuration","title":"<code>set_configuration(properties: Properties) -&gt; Properties</code>","text":"<p>Applies the provided configuration properties to the runtime.</p> <p>Call <code>set_configuration</code> with a <code>Properties</code> object to change the properties of this <code>Runtime</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>properties</code> <code>Properties</code> <p>The properties to be set.</p> required <p>Returns:</p> Type Description <code>Properties</code> <p>The updated configuration properties.</p>"},{"location":"api_reference/#acquire.Runtime.start","title":"<code>start() -&gt; None</code>","text":"<p>Starts the runtime, allowing it to collect data.</p> <p>Call <code>start()</code> to begin data acquisition.</p>"},{"location":"api_reference/#acquire.Runtime.stop","title":"<code>stop() -&gt; None</code>","text":"<p>Stops the runtime, ending data collection after the max number of frames is collected.</p> <p>Call <code>stop()</code> to end data acquisition once the max number of frames specified in <code>acquire.VideoStream.max_frame_count</code> is collected. All objects are deleted to free up disk space upon shutdown of <code>Runtime</code>.</p>"},{"location":"api_reference/#acquire.SampleRateHz","title":"<code>acquire.SampleRateHz(*args: None, **kwargs: Any)</code>","text":"<p>The <code>SampleRateHz</code> class represents the sampling rate in hertz.</p> <p>Attributes:</p> Name Type Description <code>numerator</code> <code>int</code> <p>The numerator part of the sampling rate fraction.</p> <code>denominator</code> <code>int</code> <p>The denominator part of the sampling rate fraction.</p> <p>Initializes a SampleRateHz object with optional arguments.</p>"},{"location":"api_reference/#acquire.SampleType","title":"<code>acquire.SampleType</code>","text":"<p>The <code>SampleType</code> class defines the type of the values in the streamed data.</p> <p>Attributes:</p> Name Type Description <code>F32</code> <code>SampleType</code> <p>Enum-type class variable of <code>SampleType</code> that specifies values of 32-bit floating point type.</p> <code>I16</code> <code>SampleType</code> <p>Enum-type class variable of <code>SampleType</code> that specifies values of 16-bit signed integer type.</p> <code>I8</code> <code>SampleType</code> <p>Enum-type class variable of <code>SampleType</code> that specifies values of 8-bit signed integer type.</p> <code>U16</code> <code>SampleType</code> <p>Enum-type class variable of <code>SampleType</code> that specifies values of 16-bit unsigned integer type.</p> <code>U8</code> <code>SampleType</code> <p>Enum-type class variable of <code>SampleType</code> that specifies values of 8-bit unsigned integer type.</p> <code>U10</code> <code>SampleType</code> <p>Enum-type class variable of <code>SampleType</code> that specifies values of 10-bit unsigned integer type.</p> <code>U12</code> <code>SampleType</code> <p>Enum-type class variable of <code>SampleType</code> that specifies values of 12-bit unsigned integer type.</p> <code>U14</code> <code>SampleType</code> <p>Enum-type class variable of <code>SampleType</code> that specifies values of 14-bit unsigned integer type.</p>"},{"location":"api_reference/#acquire.ShapeCapabilities","title":"<code>acquire.ShapeCapabilities</code>","text":"<p>Represents the size of the offset or the shape of the region of interest on the camera.</p> <p>The sum of the offset and shape is the size of the full camera chip.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>Property</code> <p>An instance of the <code>Property</code> class which represents the width of the region of interest on the camera or the horizontal offset of the region of interest on the camera chip.</p> <code>y</code> <code>Property</code> <p>An instance of the <code>Property</code> class which represents the height of the region of interest on the camera or the vertical offset of the region of interest on the camera chip.</p>"},{"location":"api_reference/#acquire.SignalIOKind","title":"<code>acquire.SignalIOKind</code>","text":"<p>The <code>SignalIOKind</code> class defines the signal type, input or output, for a trigger.</p> <p>Attributes:</p> Name Type Description <code>Input</code> <code>SignalIOKind</code> <p>Enum-type class variable of <code>SignalIOKind</code> that specifies signal coming in to the device.</p> <code>Output</code> <code>SignalIOKind</code> <p>Enum-type class variable of <code>SignalIOKind</code> that specifies signal sent out of the device.</p>"},{"location":"api_reference/#acquire.SignalType","title":"<code>acquire.SignalType</code>","text":"<p>The <code>SignalType</code> class specifies whether a signal is analog or digital.</p> <p>Attributes:</p> Name Type Description <code>Analog</code> <code>SignalType</code> <p>Enum-type class variable of <code>SignalType</code> that specifies a signal is analog.</p> <code>Digital</code> <code>SignalType</code> <p>Enum-type class variable of <code>SignalType</code> that specifies a signal is digital.</p>"},{"location":"api_reference/#acquire.Storage","title":"<code>acquire.Storage</code>","text":"<p>The <code>Storage</code> class represents storage devices and their settings.</p> <p>Attributes:</p> Name Type Description <code>identifier</code> <code>Optional[DeviceIdentifier]</code> <p>An optional attribute which contains an instance of the <code>DeviceIdentifier</code> class that describes the storage device if that device is natively supported. Otherwise, it is of type <code>None</code>.</p> <code>settings</code> <code>StorageProperties</code> <p>An instance of the <code>StorageProperties</code> class which contains the settings for the data storage.</p>"},{"location":"api_reference/#acquire.StorageCapabilities","title":"<code>acquire.StorageCapabilities</code>","text":"<p>The <code>StorageCapabilities</code> class represents what types of data handling is supported by the storage device.</p> <p>Attributes:</p> Name Type Description <code>chunking_is_supported</code> <code>bool</code> <p>A boolean indicating whether chunking is supported for this storage device.</p> <code>shard_is_supported</code> <code>bool</code> <p>A boolean indicating whether sharding is supported for this storage device.</p> <code>multiscale_is_supported</code> <code>bool</code> <p>A boolean indicating whether multiscale storage is supported.</p>"},{"location":"api_reference/#acquire.StorageDimension","title":"<code>acquire.StorageDimension</code>","text":"<p>Represents the type and size of the dimension for storage.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>A string representing the name or label of the storage dimension.</p> <code>kind</code> <code>DimensionType</code> <p>An instance of the <code>DimensionType</code> specifying if the storage dimension is space, channel, time, or a different physical dimension</p> <code>array_size_px</code> <code>int</code> <p>The size of the output array along this dimension, in pixels. The final (i.e., append) dimension must have size 0.</p> <code>chunk_size_px</code> <code>int</code> <p>The size of a chunk along this dimension, in pixels.</p> <code>shard_size_chunks</code> <code>int</code> <p>Integer number of chunks per shard. Shards enable aggregating multiple chunks into a single file. This value is ignored if sharding is not supported by the storage device.</p>"},{"location":"api_reference/#acquire.StorageProperties","title":"<code>acquire.StorageProperties</code>","text":"<p>The <code>StorageProperties</code> class represents properties for data storage.</p> <p>Attributes:</p> Name Type Description <code>external_metadata_json</code> <code>Optional[str]</code> <p>An optional attribute of the metadata JSON filename as a string.</p> <code>filename</code> <code>Optional[str]</code> <p>An optional attribute representing the filename for storing the image data.</p> <code>first_frame_id</code> <code>int</code> <p>An integer representing the ID of the first frame for a given acquisition.</p> <code>pixel_scale_um</code> <code>Tuple[float, float]</code> <p>A tuple of two floats representing the pixel size of the camera in micrometers.</p> <code>acquisition_dimensions</code> <code>List[StorageDimension]</code> <p>A list of instances of the <code>StorageDimension</code> class, one for each acquisition dimension. The fastest changing dimension should be first in the list and the append dimension should be last. This value is only applicable for Zarr storage devices.</p> <code>enable_multiscale</code> <code>bool</code> <p>A boolean indicating whether multiscale storage is enabled.</p>"},{"location":"api_reference/#acquire.Trigger","title":"<code>acquire.Trigger(*args: None, **kwargs: Any)</code>","text":"<p>The <code>Trigger</code> class represents a trigger signal.</p> <p>Attributes:</p> Name Type Description <code>edge</code> <code>TriggerEdge</code> <p>An instance of the <code>TriggerEdge</code> class specifying if the trigger is on the rising or falling edge trigger signal.</p> <code>enable</code> <code>bool</code> <p>A boolean indicating whether the trigger is enabled.</p> <code>line</code> <code>int</code> <p>An integer representing the max value of the trigger signal.</p> <code>kind</code> <code>SignalIOKind</code> <p>An instance of the <code>SignalIOKind</code> class specifying if the signal is input or output.</p> <p>Initializes a Trigger object with optional arguments.</p>"},{"location":"api_reference/#acquire.TriggerCapabilities","title":"<code>acquire.TriggerCapabilities</code>","text":"<p>Specifies what types of events the trigger can initiate.</p> <p>Attributes:</p> Name Type Description <code>acquisition_start</code> <code>TriggerInputOutputCapabilities</code> <p>An instance of the <code>TriggerInputOutputCapabilities</code> class indicating which lines, either input or output, are supported for starting acquisition.</p> <code>exposure</code> <code>TriggerInputOutputCapabilities</code> <p>An instance of the <code>TriggerInputOutputCapabilities</code> class indicating which lines, either input or output, are supported for starting exposure.</p> <code>frame_start</code> <code>TriggerInputOutputCapabilities</code> <p>An instance of the <code>TriggerInputOutputCapabilities</code> class indicating which lines, either input or output, are supported for starting a frame.</p>"},{"location":"api_reference/#acquire.TriggerEdge","title":"<code>acquire.TriggerEdge</code>","text":"<p>The <code>TriggerEdge</code> class represents what edge of the trigger function initiates the trigger.</p> <p>Attributes:</p> Name Type Description <code>Falling</code> <code>TriggerEdge</code> <p>Enum-type class variable of <code>TriggerEdge</code> that defines the falling edge of the trigger.</p> <code>NotApplicable</code> <code>TriggerEdge</code> <p>Enum-type class variable of <code>TriggerEdge</code> that defines if a trigger does not have a rising or falling edge.</p> <code>Rising</code> <code>TriggerEdge</code> <p>Enum-type class variable of <code>TriggerEdge</code> that defines the rising edge of the trigger.</p> <code>AnyEdge</code> <code>TriggerEdge</code> <p>Enum-type class variable of <code>TriggerEdge</code> that defines any edge of the trigger.</p> <code>LevelLow</code> <code>TriggerEdge</code> <p>Enum-type class variable of <code>TriggerEdge</code> that defines the low level of the trigger.</p> <code>LevelHigh</code> <code>TriggerEdge</code> <p>Enum-type class variable of <code>TriggerEdge</code> that defines the high level of the trigger.</p>"},{"location":"api_reference/#acquire.TriggerInputOutputCapabilities","title":"<code>acquire.TriggerInputOutputCapabilities</code>","text":"<p>Specifies which of the up to 8 supported digital lines can be used for either input or output triggering.</p> <p>The 2 attributes, input and output, each are read-only values and 8-bit integers from the conversion of the 8 binary digit representation of the digital lines to a decimal integer.</p> <p>Attributes:</p> Name Type Description <code>input</code> <code>int</code> <p>8-bit integer representing which digital lines can be used for input triggering. For example, if lines 0 and 2 were available for input triggers, the 8 binary digit representation of the lines is 00000101, which is 5 in the decimal system.</p> <code>output</code> <code>int</code> <p>8-bit integer representing which digital lines can be used for output triggering. For example, if lines 3 and 5 were available for output triggers, the 8 binary digit representation of the lines is 00101000, which is 40 in the decimal system.</p> <p>Examples:</p> <p>If lines 0 and 2 were available for input triggers, the 8 binary digit representation would be 0b00000101, since the 8 available lines are zero indexed. 00000101 binary is 5 in the decimal system, so the input attribute would have a value of 5.</p>"},{"location":"api_reference/#acquire.VideoFrame","title":"<code>acquire.VideoFrame</code>","text":"<p>The <code>VideoFrame</code> class represents data from acquisition of a frame.</p>"},{"location":"api_reference/#acquire.VideoFrame.data","title":"<code>data() -&gt; NDArray[Any]</code>","text":"<p>Returns the data of the video frame as an NDArray.</p> <p>Call <code>data()</code> to create an NDArray of the <code>VideoFrame</code> data.</p>"},{"location":"api_reference/#acquire.VideoFrame.metadata","title":"<code>metadata() -&gt; VideoFrameMetadata</code>","text":"<p>Returns the metadata associated with the video frame.</p> <p>Call <code>metadata()</code> to create a <code>VideoFrameMetadata</code> object containing the metadata of <code>VideoFrame</code>.</p>"},{"location":"api_reference/#acquire.VideoFrameMetadata","title":"<code>acquire.VideoFrameMetadata</code>","text":"<p>The <code>VideoFrameMetadata</code> class represents metadata related to a video frame.</p> <p>Attributes:</p> Name Type Description <code>frame_id</code> <code>int</code> <p>An integer representing the ID of the video frame.</p> <code>timestamps</code> <code>VideoFrameTimestamps</code> <p>An instance of the <code>VideoFrameTimestamps</code> class specifying the video timestamps based on the hardware clock and the acquisition clock.</p>"},{"location":"api_reference/#acquire.VideoFrameTimestamps","title":"<code>acquire.VideoFrameTimestamps</code>","text":"<p>The <code>VideoFrameTimestamps</code> class represents timestamps related to a video frame.</p> <p>Attributes:</p> Name Type Description <code>hardware</code> <code>int</code> <p>An integer representing hardware timestamps.</p> <code>acq_thread</code> <code>int</code> <p>An integer representing timestamps from the acquisition thread.</p>"},{"location":"api_reference/#acquire.VideoStream","title":"<code>acquire.VideoStream</code>","text":"<p>The <code>VideoStream</code> class represents a video stream.</p> <p>Attributes:</p> Name Type Description <code>camera</code> <code>Camera</code> <p>An instance of the <code>Camera</code> class representing the camera device for the video stream.</p> <code>storage</code> <code>Storage</code> <p>An instance of the <code>Storage</code> class representing the storage device for the video stream.</p> <code>max_frame_count</code> <code>int</code> <p>An integer representing the maximum number of frames to acquire.</p> <code>frame_average_count</code> <code>int</code> <p>An integer representing the number of frames to average, if any, before streaming. The default value is 0, which disables this feature. Setting this to 1 will also prevent averaging.</p>"},{"location":"api_reference/#acquire.VideoStreamCapabilities","title":"<code>acquire.VideoStreamCapabilities</code>","text":"<p>The <code>VideoStreamCapabilities</code> class captures the capabilities for a video stream.</p> <p>Attributes:</p> Name Type Description <code>camera</code> <code>CameraCapabilities</code> <p>An instance of the CameraCapabilities class which represents the capabilities for the camera in this video stream.</p> <code>storage</code> <code>StorageCapabilities</code> <p>An instance of the StorageCapabilities class which represents the capabilities for the storage device in this video stream.</p> <code>max_frame_count</code> <code>Property</code> <p>An instance of the Property class.</p> <code>frame_average_count</code> <code>Property</code> <p>An instance of the Property class.</p>"},{"location":"api_reference/#acquire.VoltageRange","title":"<code>acquire.VoltageRange</code>","text":"<p>The <code>VoltageRange</code> class represents a range of voltage values.</p> <p>Attributes:</p> Name Type Description <code>mn</code> <code>float</code> <p>A float representing the minimum voltage value.</p> <code>mx</code> <code>float</code> <p>A float representing the maximum voltage value.</p>"},{"location":"api_reference/#acquire._get_runtime","title":"<code>acquire._get_runtime() -&gt; Runtime</code>","text":"<p>Potentially create and get the global acquire runtime.</p>"},{"location":"api_reference/#acquire.core_api_version","title":"<code>acquire.core_api_version() -&gt; str</code>","text":""},{"location":"api_reference/#acquire.gui","title":"<code>acquire.gui(viewer: napari.Viewer, frame_count: int = 100, stream_count: int = 2) -&gt; None</code>","text":"<p>Napari dock-widget plugin entry-point</p> <p>This instances a magicgui dock widget that streams video to a layer.</p>"},{"location":"api_reference/#acquire.setup","title":"<code>acquire.setup(runtime: Runtime, camera: Union[str, List[str]] = 'simulated: radial sin', storage: Union[str, List[str]] = 'Tiff', output_filename: Optional[str] = 'out.tif') -&gt; Properties</code>","text":"<p>Set up the runtime with a camera and storage device.</p>"},{"location":"api_reference/#acquire.setup_one_streams","title":"<code>acquire.setup_one_streams(runtime: Runtime, frame_count: int) -&gt; Properties</code>","text":""},{"location":"api_reference/#acquire.setup_two_streams","title":"<code>acquire.setup_two_streams(runtime: Runtime, frame_count: int) -&gt; Properties</code>","text":""},{"location":"get_started/","title":"Getting Started with Acquire","text":"<p>Acquire (<code>acquire-imaging</code> on PyPI) is a Python package providing a multi-camera video streaming library focused on performant microscopy, with support for up to two simultaneous, independent, video streams.</p> <p>This tutorial covers Acquire installation and shows an example of using Acquire with its provided simulated cameras to demonstrate the acquisition process.</p>"},{"location":"get_started/#installation","title":"Installation","text":"<p>To install Acquire on Windows, macOS, or Ubuntu, simply run the following command:</p> <pre><code>python -m pip install acquire-imaging\n</code></pre> <p>We recommend installing <code>Acquire</code> in a fresh conda environment or virtualenv. For example, to install <code>Acquire</code> in a conda environment named <code>acquire</code>:</p> <pre><code>conda create -n acquire python=3.10 # follow the prompts and proceed with the defaults\nconda activate acquire\npython -m pip install acquire-imaging\n</code></pre> <p>or with virtualenv:</p> <pre><code>$ python -m venv venv\n$ . ./venv/bin/activate # or on Windows: .\\venv\\Scripts\\Activate.bat or .\\venv\\Scripts\\Activate.ps1\n(venv) $ python -m pip install acquire-imaging\n</code></pre> <p>Once you have Acquire installed, simply call <code>import acquire</code> in your script, notebook, or module to start utilizing the package.</p> <pre><code>import acquire\n</code></pre>"},{"location":"get_started/#supported-cameras-and-file-formats","title":"Supported Cameras and File Formats","text":"<p>Acquire supports the following cameras (currently only on Windows):</p> <ul> <li>Hamamatsu Orca Fusion BT (C15440-20UP)</li> <li>Vieworks VC-151MX-M6H00</li> <li>FLIR Blackfly USB3 (BFLY-U3-23S6M-C)</li> <li>FLIR Oryx 10GigE (ORX-10GS-51S5M-C)</li> </ul> <p>Acquire also supports the following output file formats:</p> <ul> <li>Tiff</li> <li>Zarr</li> </ul> <p>For testing and demonstration purposes, Acquire provides a few simulated cameras, as well as raw and trash output devices. To see all the devices that Acquire supports, you can run the following script:</p> <pre><code>import acquire\n\nfor device in acquire.Runtime().device_manager().devices():\n    print(device)\n</code></pre>"},{"location":"get_started/#tutorial-prerequisites","title":"Tutorial Prerequisites","text":"<p>We will be writing to and reading from the Zarr format, using the Dask library to load and inspect the data, and visualizing the data using napari.</p> <p>You can install these prerequisites with:</p> <pre><code>python -m pip install dask \"napari[all]\" zarr\n</code></pre>"},{"location":"get_started/#setup-for-acquisition","title":"Setup for Acquisition","text":"<p>We will use one of Acquire's simulated cameras to generate data and use Zarr for our output file format, which is called \"storage device\" in <code>Acquire</code>.</p> <p>To begin, instantiate <code>Runtime</code> and <code>DeviceManager</code> and list the currently supported devices.</p> <p><pre><code>import acquire\n\nruntime = acquire.Runtime()\ndm = runtime.device_manager()\n\nfor device in dm.devices():\n    print(device)\n</code></pre> The runtime is the main entry point in Acquire. Through the runtime, you configure your devices, start acquisition, check acquisition status, inspect data as it streams from your cameras, and terminate acquisition.</p> <p>Let's configure our devices now. To do this, we'll get a copy of the current runtime configuration. We can update the configuration with identifiers from the the runtime's device manager, but these devices won't instantiate until we start acquisition.</p> <p>Acquire supports up to two video streams. These streams consist of a source (i.e., a camera), optionally a filter, and a sink (an output, like a Zarr dataset or a Tiff file). Before configuring the streams, grab the current configuration of the <code>Runtime</code> object with:</p> <pre><code>config = runtime.get_configuration()\n</code></pre> <p>Video streams are configured independently. Configure the first video stream by setting properties on <code>config.video[0]</code> and the second video stream with <code>config.video[1]</code>. We'll be using simulated cameras, one generating a radial sine pattern and one generating a random pattern.</p> <pre><code>config.video[0].camera.identifier = dm.select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# how many adjacent pixels in each direction to combine by averaging; here, 1 means not to combine\nconfig.video[0].camera.settings.binning = 1\n\n# how long (in microseconds) your camera should collect light from the sample; for simulated cameras,\n# this is just a waiting period before generating the next frame\nconfig.video[0].camera.settings.exposure_time_us = 5e4  # 50 ms\n\n# the data type representing each pixel; here we choose unsigned 8-bit integer\nconfig.video[0].camera.settings.pixel_type = acquire.SampleType.U8\n\n# the shape, in pixels, of the image; width first, then height\nconfig.video[0].camera.settings.shape = (1024, 768)\n</code></pre> <pre><code>config.video[1].camera.identifier = dm.select(acquire.DeviceKind.Camera, \"simulated: uniform random\")\n\n# how many adjacent pixels in each direction to combine by averaging; here, 1 means not to combine\nconfig.video[1].camera.settings.binning = 1\n\n# how long (in microseconds) your camera should collect light from the sample; for simulated cameras,\n# this is just a waiting period before generating the next frame\nconfig.video[1].camera.settings.exposure_time_us = 1e4  # 10 ms\n\n# the data type representing each pixel; here we choose unsigned 8-bit integer\nconfig.video[1].camera.settings.pixel_type = acquire.SampleType.U8\n\n# the shape, in pixels, of the image; width first, then height\nconfig.video[1].camera.settings.shape = (1280, 720)\n</code></pre> <p>Now we'll configure each output, or sink device. For both simulated cameras, we'll be writing to Zarr, a format which supports chunked arrays.</p> <pre><code>config.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"Zarr\")\n\n# what file or directory to write the data to\nconfig.video[0].storage.settings.filename = \"output1.zarr\"\n\n# where applicable, how large should a chunk file get before opening the next chunk file\nconfig.video[0].storage.settings.chunking.max_bytes_per_chunk = 32 * 2**20  # 32 MiB chunk sizes\n</code></pre> <pre><code>config.video[1].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"Zarr\")\n\n# what file or directory to write the data to\nconfig.video[1].storage.settings.filename = \"output2.zarr\"\n\n# where applicable, how large should a chunk file get before opening the next chunk file\nconfig.video[1].storage.settings.chunking.max_bytes_per_chunk = 64 * 2**20  # 64 MiB chunk sizes\n</code></pre> <p>Finally, let's specify how many frames to generate for each camera before stopping our simulated acquisition. We also need to register our configuration with the runtime using the <code>set_configuration</code> method.</p> <p>If you want to let the runtime just keep acquiring effectively forever, you can set <code>max_frame_count</code> to <code>2**64 - 1</code>.</p> <pre><code>config.video[0].max_frame_count = 100 # collect 100 frames\nconfig.video[1].max_frame_count = 150 # collect 150 frames\n\nconfig = runtime.set_configuration(config)\n</code></pre> <p>Note</p> <p>If you run this tutorial multiple times, you can clear output from previous runs with:</p> <pre><code>import os\nimport shutil\n\nif config.video[0].storage.settings.filename in os.listdir(\".\"):\n    shutil.rmtree(config.video[0].storage.settings.filename)\n\nif config.video[1].storage.settings.filename in os.listdir(\".\"):\n    shutil.rmtree(config.video[1].storage.settings.filename)\n</code></pre>"},{"location":"get_started/#acquire-data","title":"Acquire Data","text":"<p>To start aquiring data:</p> <pre><code>runtime.start()\n</code></pre> <p>Acquisition happens in a separate thread, so at any point we can check on the status by calling the  <code>get_state</code> method.</p> <pre><code>runtime.get_state()\n</code></pre> <p>Finally, once we're done acquiring, we call <code>runtime.stop()</code>. This method will wait until you've reached the number of frames to collect specified in <code>config.video[0].max_frame_count</code> or <code>config.video[1].max_frame_count</code>, whichever is larger.</p> <pre><code>runtime.stop()\n</code></pre>"},{"location":"get_started/#visualizing-the-data-with-napari","title":"Visualizing the Data with napari","text":"<p>Let's take a look at what we've written. We'll load each Zarr dataset as a Dask array and inspect its dimensions, then we'll use napari to view it.</p> <pre><code>import dask.array as da\nimport napari\n\ndata1 = da.from_zarr(config.video[0].storage.settings.filename, component=\"0\")\ndata1\n\ndata2 = da.from_zarr(config.video[1].storage.settings.filename, component=\"0\")\n\nviewer1 = napari.view_image(data1)\n\nviewer2 = napari.view_image(data2)\n</code></pre>"},{"location":"get_started/#conclusion","title":"Conclusion","text":"<p>For more examples of using Acquire, check out our tutorials page.</p>"},{"location":"for_contributors/","title":"For contributors","text":"<p>Thank you for your interest in contributing to <code>Acquire</code>! We welcome contributions to the code base and to documentation.</p> <ul> <li><code>Acquire</code> project</li> <li><code>acquire-imaging</code></li> <li><code>Acquire</code> documentation</li> </ul>"},{"location":"for_contributors/docs_contribution_quickstart/","title":"Acquire Docs Contribution Quickstart","text":"<ol> <li>Make sure you have a fresh environment with the latest mkdocs and mkdocs-material installed. You can install them with <code>pip install -r requirements.txt</code> from the root of the repository.</li> <li>Your pages should be written as markdown files, using the basic markdown syntax or following the mkdocs or material for mkdocs syntax.</li> <li>Pages can be added to the top level menu or submenus by editing the <code>mkdocs.yml</code> file. The order of the pages in the menu is determined by the order of the pages in the <code>mkdocs.yml</code> file. Subpages can be added by creating subfolders in the <code>docs/</code> folder (see, for example, the <code>docs/tutorials/</code> folder).</li> <li>To add images, place them in the <code>docs/images/</code> folder and reference them in your markdown files using the relative path <code>../images/your_image.png</code>.</li> <li>Custom CSS configuration goes into the <code>docs/stylesheets/custom.css</code> file.</li> <li>To build the website locally, after activating your environment (either using <code>conda activate &lt;your-environment&gt;</code> or <code>source activate &lt;your-env&gt;</code>, for example), run <code>mkdocs serve</code> to start a local server. You can then view the website at the URL indicated on your console.</li> </ol>"},{"location":"for_contributors/update_version/","title":"How to update the documentation version","text":"<p>After every new release of Acquire Python, the documentation version needs to be manually updated. This can be done by issuing the following command on the root of the Acquire docs repository:</p> <pre><code>mike deploy --push --update-aliases &lt;version-tag&gt; stable\n</code></pre> <p>where <code>&lt;version-tag&gt;</code> is the tag of the new release. This will</p> <ul> <li>create/update the alias <code>stable</code> for the <code>&lt;version-tag&gt;</code> release of the docs;</li> <li>update the version switcher dropdown accordingly (autogenerating the   <code>versions.json</code> file, which is only present in the deployed pages), and</li> <li>deploy the new version of the documentation to the <code>gh-pages</code> branch of the   repository (if the <code>--push</code> option is used, as above.)</li> </ul> <p>The default version of the documentation pages is <code>stable</code>, but this can be changed to another version by using the <code>mike set-default &lt;identifier&gt;</code> command.</p> <p>Note</p> <p>In order to provide downloadable <code>.py</code> files to the tutorials, make sure you run <code>bash .github/workflows/convert.sh</code> before deploying a new version with <code>mike</code>.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>These tutorials will help you explore the main use cases of Acquire and show examples of using the API. Please submit an issue on Github if you'd like to request a tutorial, or if you are also interested in contributing to a tutorial to this documentation please visit our contribution guide.</p>"},{"location":"tutorials/chunked/","title":"Chunking Data for Zarr Storage","text":"<p>This tutorial will provide an example of writing chunked data to a Zarr storage device.</p> <p>Zarr has additional capabilities relative to the basic storage devices, namely chunking, compression, and multiscale storage. To enable chunking, set the attributes in an instance of the <code>ChunkingProperties</code> class. You can learn more about the Zarr capabilities in <code>Acquire</code> here.</p>"},{"location":"tutorials/chunked/#configure-runtime","title":"Configure <code>Runtime</code>","text":"<p>To start, we'll create a <code>Runtime</code> object and configure the streaming process, selecting <code>Zarr</code> as the storage device to enable chunking the data.</p> <p><pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# Initialize the device manager\ndm = runtime.device_manager()\n\n# Grab the current configuration\nconfig = runtime.get_configuration()\n\n# Select the radial sine simulated camera as the video source\nconfig.video[0].camera.identifier = dm.select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# Set the storage to Zarr to take advantage of chunking\nconfig.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"Zarr\")\n\n# Set the time for collecting data for a each frame\nconfig.video[0].camera.settings.exposure_time_us = 5e4  # 50 ms\n\n# size of image region of interest on the camera (x, y)\nconfig.video[0].camera.settings.shape = (1920, 1080)\n\n# specify the pixel datatype as a uint8\nconfig.video[0].camera.settings.pixel_type = acquire.SampleType.U8\n\n# Set the max frame count\nconfig.video[0].max_frame_count = 10 # collect 10 frames\n\n# Set the output file to out.zarr\nconfig.video[0].storage.settings.filename = \"out.zarr\"\n</code></pre> Below we'll configure the chunking specific settings and update all settings with the <code>set_configuration</code> method.</p> <pre><code># Chunk size may need to be optimized for each acquisition.\n# See Zarr documentation for further guidance:\n# https://zarr.readthedocs.io/en/stable/tutorial.html#chunk-optimizations\nconfig.video[0].storage.settings.chunking.max_bytes_per_chunk = 32 * 2**20 # 32 MB\n\n# x, y dimensions of each chunk\n# 1/2 of the width and height of the image, generating 4 chunks\nconfig.video[0].storage.settings.chunking.tile.width = 1920 // 2\nconfig.video[0].storage.settings.chunking.tile.height = 1080 // 2\n\n# Update the configuration with the chosen parameters\nconfig = runtime.set_configuration(config)\n</code></pre>"},{"location":"tutorials/chunked/#collect-and-inspect-the-data","title":"Collect and Inspect the Data","text":"<pre><code># collect data\nruntime.start()\nruntime.stop()\n</code></pre> <p>You can inspect the Zarr file directory to check that the data saved as expected. Alternatively, you can inspect the data programmatically with:</p> <pre><code># Utilize the zarr library to open the data\nimport zarr\n\n# create a zarr Group object\ngroup = zarr.open(config.video[0].storage.settings.filename)\n\n# check for the expected # of directories in the zarr container\nassert len(group) == 1\n\n# inspect the characteristics of the data\ngroup[\"0\"]\n</code></pre> <p>The output will be: <pre><code>&lt;zarr.core.Array '/0' (10, 1, 1080, 1920) uint8&gt;\n</code></pre> As expected, we have only 1 top level directory, corresponding to the single array in the group. We would expect more than 1 array only if we were writing multiscale data. The overall array shape is (10, 1, 1080, 1920), corresponding to 10 frames, 1 channel, and a height and width of 1080 and 1920, respectively, per frame.</p> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/compressed/","title":"Writing to Compressed Zarr Files","text":"<p>This tutorial will provide an example of writing compressed data to a Zarr file.</p> <p><code>Acquire</code> supports streaming compressed data to the <code>ZarrBlosc1*</code> storage devices. Compression is done via Blosc. Supported codecs are lz4 and zstd, available with ZarrBlosc1Lz4ByteShuffle and ZarrBlosc1ZstdByteShuffle devices, respectively. For a comparison of these codecs, please refer to the Blosc docs. You can learn more about the Zarr capabilities in <code>Acquire</code> here.</p>"},{"location":"tutorials/compressed/#configure-runtime","title":"Configure <code>Runtime</code>","text":"<p>To start, we'll create a <code>Runtime</code> object and configure the streaming process, selecting <code>ZarrBlosc1ZstdByteShuffle</code> as the storage device to enable compressing the data.</p> <pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# Initialize the device manager\ndm = runtime.device_manager()\n\n# Grab the current configuration\nconfig = runtime.get_configuration()\n\n# Select the radial sine simulated camera as the video source\nconfig.video[0].camera.identifier = dm.select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# Set the storage to ZarrBlosc1ZstdByteShuffle to avoid saving the data\nconfig.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"ZarrBlosc1ZstdByteShuffle\")\n\n# Set the time for collecting data for a each frame\nconfig.video[0].camera.settings.exposure_time_us = 5e4  # 50 ms\n\nconfig.video[0].camera.settings.shape = (1024, 768)\n\n# Set the max frame count\nconfig.video[0].max_frame_count = 100 # collect 100 frames\n\n# Set the output file to out.zarr\nconfig.video[0].storage.settings.filename = \"out.zarr\"\n\n# Update the configuration with the chosen parameters\nconfig = runtime.set_configuration(config)\n</code></pre>"},{"location":"tutorials/compressed/#inspect-acquired-data","title":"Inspect Acquired Data","text":"<p>Now that the configuration is set to utilize the <code>ZarrBlosc1ZstdByteShuffle</code> storage device, we can acquire data, which will be compressed before it is stored to <code>out.zarr</code>. Since we did not specify the size of chunks, the data will be saved as a single chunk that is the size of the image data. You may specify chunk sizes using the <code>TileShape</code> class. For example, using <code>acquire.StorageProperties.chunking.tile.width</code> to set the width of the chunks.</p> <pre><code># acquire data\nruntime.start()\nruntime.stop()\n</code></pre> <p>We'll use the Zarr Python package to read the data in <code>out.zarr</code> file.</p> <pre><code># We'll utilize the Zarr python package to read the data\nimport zarr\n\n# load from Zarr\ncompressed = zarr.open(config.video[0].storage.settings.filename)\n</code></pre> <p>We'll print some of the data properties to illustrate how the data was compressed. Since we have not enabled multiscale output, <code>out.zarr</code> will only have one top level array<code>\"0\"</code>.</p> <pre><code># All of the data is stored in the \"0\" directory since the data was stored as a single chunk.\ndata = compressed[\"0\"]\n\nprint(data.compressor.cname)\nprint(data.compressor.clevel)\nprint(data.compressor.shuffle)\n</code></pre> <p>Output:</p> <pre><code>zstd\n1\n1\n</code></pre> <p>As expected, the data was compressed using the <code>zstd</code> codec.</p> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/configure/","title":"Configure an Acquisition","text":"<p>This tutorial will provide an in-depth explanation of setting configuration properites and demonstrate the relationships between various <code>Acquire</code> classes, such as <code>CameraProperties</code> and <code>StorageProperties</code>, used in the configuration process. In this example, we'll only configure one video source.</p>"},{"location":"tutorials/configure/#initialize-runtime","title":"Initialize <code>Runtime</code>","text":"<p><code>Runtime</code> is the main entry point in <code>Acquire</code>. Through the runtime, you configure your devices, start acquisition, check acquisition status, inspect data as it streams from your cameras, and terminate acquisition. The <code>device_manager</code> method in <code>Runtime</code> creates an instance of the <code>DeviceManager</code> class. The <code>get_configuration</code> method in <code>Runtime</code> creates an instance of the <code>Properties</code> class. To configure the acquisition, we'll use those two methods to grab the configuration and to initialize a <code>DeviceManager</code> object to set the attributes of <code>Properties</code> and related classes.</p> <pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# Initialize the device manager\ndm = runtime.device_manager()\n\n# Grab the current configuration\nconfig = runtime.get_configuration()\n</code></pre>"},{"location":"tutorials/configure/#utilize-devicemanager","title":"Utilize <code>DeviceManager</code>","text":"<p><code>DeviceManager</code> contains a <code>devices</code> method which creates a list of <code>DeviceIdentifier</code> objects each representing a discovered camera or storage device. Each <code>DeviceIdentifier</code> has an attribute <code>kind</code> that is a <code>DeviceKind</code> object, which has attributes specifying whether the device is a camera or storage device, as well as <code>Signals</code> and <code>StageAxes</code> attributes. The <code>Signals</code> and <code>StageAxes</code> attributes would apply to device kinds such as stages, which are not yet supported by <code>Acquire</code>.</p> <p><code>DeviceManager</code> has 2 methods for selecting devices for the camera and storage. For more information on these methods, check out the Device Selection tutorial. We'll use the <code>select</code> method in this example to choose a specific device for the camera and storage.</p> <pre><code># Select the radial sine simulated camera as the video source\nconfig.video[0].camera.identifier = dm.select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# Set the storage to Tiff\nconfig.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"Tiff\")\n</code></pre>"},{"location":"tutorials/configure/#properties-class-explanation","title":"<code>Properties</code> Class Explanation","text":"<p>Using <code>Runtime</code>'s <code>get_configuration</code> method we created <code>config</code>, an instance of the <code>Properties</code> class. <code>Properties</code> contains only one attribute <code>video</code> which is a tuple of <code>VideoStream</code> objects since <code>Acquire</code> currently supports 2 camera streaming. To configure the first video stream, we'll index this tuple to select the first <code>VideoStream</code> object <code>config.video[0]</code>.</p> <p><code>VideoStream</code> objects have 2 attributes <code>camera</code> and <code>storage</code> which are instances of the <code>Camera</code> and <code>Storage</code> classes, respectively, and will be used to set the attributes of the selected camera device <code>simulated: radial sin</code> and storage device <code>Tiff</code>. The other attributes of <code>VideoStream</code> are integers that specify the maximum number of frames to collect and how many frames to average, if any, before storing the data. The <code>frame_average_count</code> has a default value of <code>0</code>, which disables this feature. We'll specify the max frame count, but keep the frame averaging disabled with:</p> <pre><code># Set the maximum number of frames to collect to 100\nconfig.video[0].max_frame_count = 100\n</code></pre>"},{"location":"tutorials/configure/#configure-camera","title":"Configure <code>Camera</code>","text":"<p><code>Camera</code> class objects have 2 attributes, <code>settings</code>, a <code>CameraProperties</code> object, and an optional attribute <code>identifier</code>, which is a <code>DeviceIdentifier</code> object.</p> <p><code>CameraProperties</code> has 5 attributes that are numbers and specify the exposure time and line interval in microseconds, how many pixels, if any, to bin (set to 1 by default to disable), and tuples for the image size and location on the camera chip. The other attributes are all instances of different classes. The <code>pixel_type</code> attribute is a <code>SampleType</code> object which indicates the data type of the pixel values in the image, such as Uint8. The <code>readout_direction</code> attribute is a <code>Direction</code> object specifying whether the data is read forwards or backwards from the camera. The <code>input_triggers</code> attribute is an <code>InputTriggers</code> object that details the characteristics of any input triggers in the system. The <code>output_triggers</code> attribute is an <code>OutputTriggers</code> object that details the characteristics of any output triggers in the system. All of the attributes of <code>InputTriggers</code> and <code>OutputTriggers</code> objects are instances of the <code>Trigger</code> class. The <code>Trigger</code> class is described in this tutorial.</p> <p>We'll configure some camera settings below.</p> <pre><code># Set the time for collecting data for a each frame\nconfig.video[0].camera.settings.exposure_time_us = 5e4  # 50 ms\n\n# (x, y) size of the image in pixels\nconfig.video[0].camera.settings.shape = (1024, 768)\n\n# Specify the pixel type as Uint32\nconfig.video[0].camera.settings.pixel_type = acquire.SampleType.U32\n</code></pre>"},{"location":"tutorials/configure/#configure-storage","title":"Configure <code>Storage</code>","text":"<p><code>Storage</code> objects have 2 attributes, <code>settings</code>, a <code>StorageProperties</code> object, and an optional attribute <code>identifier</code>, which is an instance of the <code>DeviceIdentifier</code> class described above.</p> <p><code>StorageProperties</code> has 2 attributes <code>external_metadata_json</code> and <code>filename</code> which are strings of the filename or filetree of the output metadata in JSON format and image data in whatever format corresponds to the selected storage device, respectively. <code>first_frame_id</code> is an integer ID that corresponds to the first frame of the current acquisition and is typically 0. <code>pixel_scale_um</code> is the camera pixel size in microns. <code>enable_multiscale</code> is a boolean used to specify if the data should be saved as an image pyramid. See the multiscale tutorial for more information. The <code>chunking</code> attribute is an instance of the <code>ChunkingProperties</code> class, used for Zarr storage. See the chunking tutorial for more information.</p> <p>We'll specify the name of the output image file below.</p> <pre><code># Set the output file to out.tiff\nconfig.video[0].storage.settings.filename = \"out.tiff\"\n</code></pre>"},{"location":"tutorials/configure/#update-configuration-settings","title":"Update Configuration Settings","text":"<p>None of the configuration settings are updated in <code>Runtime</code> until the <code>set_configuration</code> method is called. We'll be creating a new <code>Properties</code> object with the <code>set_configuration</code> method. For simplicity, we'll reuse <code>config</code> for the name of that object as well, but note that <code>new_config = runtime.set_configuration(config)</code> also works here.</p> <pre><code># Update the configuration with the chosen parameters\nconfig = runtime.set_configuration(config)\n</code></pre> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/drivers/","title":"Test Camera Drivers","text":"<p>This tutorial will cover testing that your cameras, or video sources, has been properly identified.</p> <p>Acquire supports the following cameras (currently only on Windows):</p> <ul> <li>Hamamatsu Orca Fusion BT (C15440-20UP)</li> <li>Vieworks VC-151MX-M6H00</li> <li>FLIR Blackfly USB3 (BFLY-U3-23S6M-C)</li> <li>FLIR Oryx 10GigE (ORX-10GS-51S5M-C)</li> </ul> <p>Acquire provides the following simulated cameras:</p> <ul> <li>simulated: uniform random - Produces uniform random noise for each pixel.</li> <li>simulated: radial sin - Produces an animated radial sine wave pattern.</li> <li>simulated: empty - Produces no data, leaving a blank image. This camera simulates acquiring as fast as possible.</li> </ul> <p>Acquire will only identify cameras whose drivers are present on your machine. The <code>DeviceManager</code> class manages selection of cameras and storage. We can create a <code>DeviceManager</code> object using the following:</p> <pre><code>import acquire\n\n# Instantiate a Runtime object\nruntime = acquire.Runtime()\n\n# Instantiate a DeviceManager object for the Runtime\ndm = runtime.device_manager()\n</code></pre> <p><code>DeviceManager</code> objects have <code>device</code> methods which lists the identifiers for discovered devices. You can iterate over this list to determine which cameras were discovered.</p> <pre><code>for device in dm.devices():\n    print(device)\n</code></pre> <p>The output of this code is below. All discovered devices, both cameras and storage devices, will be listed. In this tutorial, no cameras were connected to the machine, so only simulated cameras were found. Note that the storage devices also printed.</p> <pre><code>&lt;DeviceIdentifier Camera \"simulated: uniform random\"&gt;\n&lt;DeviceIdentifier Camera \"simulated: radial sin\"&gt;\n&lt;DeviceIdentifier Camera \"simulated: empty\"&gt;\n&lt;DeviceIdentifier Storage \"raw\"&gt;\n&lt;DeviceIdentifier Storage \"tiff\"&gt;\n&lt;DeviceIdentifier Storage \"trash\"&gt;\n&lt;DeviceIdentifier Storage \"tiff-json\"&gt;\n&lt;DeviceIdentifier Storage \"Zarr\"&gt;\n&lt;DeviceIdentifier Storage \"ZarrBlosc1ZstdByteShuffle\"&gt;\n&lt;DeviceIdentifier Storage \"ZarrBlosc1Lz4ByteShuffle\"&gt;\n</code></pre> <p>For cameras that weren't discovered you will see an error like the one below. These errors will not affect performance and can be ignored.</p> <pre><code>ERROR acquire.runtime 2023-10-20 19:03:17,917 runtime.rs:40 C:\\actions-runner\\_work\\acquire-driver-hdcam\\acquire-driver-hdcam\\src\\acquire-core-libs\\src\\acquire-device-hal\\device\\hal\\loader.c:114 - driver_load(): Failed to load driver at \"acquire-driver-hdcam\".\n</code></pre> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/framedata/","title":"Accessing Data during Acquisition","text":"<p>This tutorial will provide an example of accessing data from a video source during acquisition.</p>"},{"location":"tutorials/framedata/#configure-runtime","title":"Configure <code>Runtime</code>","text":"<p>To start, we'll create a <code>Runtime</code> object and configure the streaming process.</p> <pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# Initialize the device manager\ndm = runtime.device_manager()\n\n# Grab the current configuration\nconfig = runtime.get_configuration()\n\n# Select the radial sine simulated camera as the video source\nconfig.video[0].camera.identifier = dm.select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# Set the storage to trash to avoid saving the data\nconfig.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"Trash\")\n\n# Set the time for collecting data for a each frame\nconfig.video[0].camera.settings.exposure_time_us = 5e4  # 50 ms\n\nconfig.video[0].camera.settings.shape = (1024, 768)\n\n# Set the max frame count to 2**(64-1) the largest number supported by Uint64 for essentially infinite acquisition\nconfig.video[0].max_frame_count = 100 # collect 100 frames\n\n# Update the configuration with the chosen parameters\nconfig = runtime.set_configuration(config)\n</code></pre>"},{"location":"tutorials/framedata/#working-with-availabledata-objects","title":"Working with <code>AvailableData</code> objects","text":"<p>During Acquisition, the <code>AvailableData</code> object is the streaming interface, and this class has a <code>frames</code> method which iterates over the <code>VideoFrame</code> objects in <code>AvailableData</code>. Once we start acquisition, we'll utilize this iterator method to list the frames. To increase the likelihood of <code>AvailableData</code> containing data, we'll utilize the time python package to introduce a delay before we create our <code>AvailableData</code> object</p> <pre><code># package for introducing time delays\nimport time\n\n# start acquisition\nruntime.start()\n\n# time delay of 0.5 seconds\ntime.sleep(0.5)\n\n# grab the packet of data available on disk for video stream 0.\n# This is an AvailableData object.\navailable_data = runtime.get_available_data(0)\n</code></pre> <p>There may not be data available, in which case our variable <code>available_data</code> would be <code>None</code>. To avoid errors associated with this circumstance, we'll only grab data if <code>available_data</code> is not <code>None</code>.</p> <p>Once <code>get_available_data()</code> is called the <code>AvailableData</code> object will be locked into memory, so the circular buffer that stores the available data will overflow if <code>AvailableData</code> isn\u2019t released, so we'll delete the object with <code>del available_data</code> if there is no data available.</p> <p><pre><code># NoneType if there is no available data.\n# We can only grab frames if data is available.\nif available_data is not None:\n\n\n    # frames is an iterator over available_data\n    # we'll use this iterator to make a list of the frames\n    video_frames = list(available_data.frames())\n\nelse:\n    # delete the available_data variable\n    # if there is no data in the packet to free up RAM\n    del available_data\n</code></pre> <code>video_frames</code> is a list with each element being an instance of the <code>VideoFrame</code> class. <code>VideoFrame</code> has a <code>data</code> method which provides the frame as an <code>NDArray</code>. The shape of this NDArray corresponds to the image dimensions used internally by Acquire with (planes, height, width, channels). Since we have a single channel, both the first and the last dimensions will be 1. The interior dimensions are height and width, respectively.</p> <p><pre><code># grab the first VideoStream object in frames and convert it to an NDArray\nfirst_frame = video_frames[0].data()\n\nprint(first_frame.shape)\n</code></pre> Output: <pre><code>(1, 768, 1024, 1)\n</code></pre></p> <p>We can use the <code>numpy.squeeze</code> method to grab the desired NDArray image data from <code>first_frame</code> since the other dimensions are 1. This is equivalent to <code>image = first_frame[0][:, :, 0]</code>.</p> <p><pre><code>image = first_frame.squeeze()\n\n\nprint(image.shape)\n</code></pre> Output: <pre><code>(768, 1024)\n</code></pre> Finally, delete the <code>available_data</code> to unlock the region in the circular buffer.</p> <pre><code># delete the available_data to free up disk space\ndel available_data\n\n# stop runtime\nruntime.stop()\n</code></pre> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/livestream/","title":"Livestream to napari","text":"<p>The below script can be used to livestream data to the napari viewer. You may also utilize the <code>Acquire</code> napari plugin, which is provided in the package upon install. You can access the plugin in the napari plugins menu once <code>Acquire</code> is installed. You can review the plugin code here. You may also stream using other packages such at <code>matplotlib</code>.</p> <pre><code>\"\"\"\nThis script will livestream data to the [napari viewer](https://napari.org/stable/). You may also utilize the `Acquire` napari plugin, which is provided in the `acquire-imaging` package on PyPI upon install. You can access the plugin in the napari plugins menu once `Acquire` is installed. You can review the [plugin code here](https://github.com/acquire-project/acquire-python/blob/main/python/acquire/__init__.py).\n\"\"\"\n\nimport acquire\nruntime = acquire.Runtime()\n\n# Initialize the device manager\ndm = runtime.device_manager()\n\n# Grab the current configuration\nconfig = runtime.get_configuration()\n\n# Select the uniform random camera as the video source\nconfig.video[0].camera.identifier = dm.select(acquire.DeviceKind.Camera, \".*random.*\")\n\n# Set the storage to trash to avoid saving the data\nconfig.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"Trash\")\n\n# Set the time for collecting data for a each frame\nconfig.video[0].camera.settings.exposure_time_us = 5e4  # 500 ms\n\nconfig.video[0].camera.settings.shape = (300, 200)\n\n# Set the max frame count to 100 frames\nconfig.video[0].max_frame_count = 100\n\n# Update the configuration with the chosen parameters\nconfig = runtime.set_configuration(config)\n\n# import napari and open a viewer to stream the data\nimport napari\nviewer = napari.Viewer()\n\nimport time\nfrom napari.qt.threading import thread_worker\n\ndef update_layer(args) -&gt; None:\n    (new_image, stream_id) = args\n    print(f\"update layer: {new_image.shape=}, {stream_id=}\")\n    layer_key = f\"Video {stream_id}\"\n    try:\n        layer = viewer.layers[layer_key]\n        layer._slice.image._view = new_image\n        layer.data = new_image\n        # you can use the private api with layer.events.set_data() to speed up by 1-2 ms/frame\n\n    except KeyError:\n        viewer.add_image(new_image, name=layer_key)\n\n@thread_worker(connect={\"yielded\": update_layer})\ndef do_acquisition():\n    time.sleep(5)\n    runtime.start()\n\n    nframes = [0, 0]\n    stream_id = 0\n\n    def is_not_done() -&gt; bool:\n        return (nframes[0] &lt; config.video[0].max_frame_count) or (\n                nframes[1] &lt; config.video[1].max_frame_count\n                )\n\n    def next_frame(): #-&gt; Optional[npt.NDArray[Any]]:\n        \"\"\"Get the next frame from the current stream.\"\"\"\n        if nframes[stream_id] &lt; config.video[stream_id].max_frame_count:\n            if packet := runtime.get_available_data(stream_id):\n                n = packet.get_frame_count()\n                nframes[stream_id] += n\n                f = next(packet.frames())\n                return f.data().squeeze().copy()\n        return None\n\n    stream = 1\n    # loop to continue to update the data in napari while acquisition is running\n    while is_not_done():\n        if (frame := next_frame()) is not None:\n            yield frame, stream_id\n        time.sleep(0.1)\n\ndo_acquisition()\n\nnapari.run()\n</code></pre> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/multiscale/","title":"Multiscale Data Acqusition","text":"<p>This tutorial will provide an example of writing multiscale data to a Zarr file.</p> <p>Zarr has additional capabilities relative to Acquire's basic storage devices, namely chunking, compression, and multiscale storage. To enable chunking and multiscale storage, set those attributes in instances of the <code>ChunkingProperties</code> and <code>StorageProperties</code> classes, respectively. You can learn more about the Zarr capabilities in <code>Acquire</code> here.</p>"},{"location":"tutorials/multiscale/#configure-runtime","title":"Configure <code>Runtime</code>","text":"<p>To start, we'll create a <code>Runtime</code> object and begin to configure the streaming process, selecting <code>Zarr</code> as the storage device so that writing multiscale data is possible.</p> <pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# Initialize the device manager\ndm = runtime.device_manager()\n\n# Grab the current configuration\nconfig = runtime.get_configuration()\n\n# Select the radial sine simulated camera as the video source\nconfig.video[0].camera.identifier = dm.select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# Set the storage to Zarr to have the option to save multiscale data\nconfig.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"Zarr\")\n\n# Set the time for collecting data for a each frame\nconfig.video[0].camera.settings.exposure_time_us = 5e4  # 50 ms\n\n# Set the size of image region of interest on the camera (x, y)\nconfig.video[0].camera.settings.shape = (1920, 1080)\n\n# Set the max frame count\nconfig.video[0].max_frame_count = 5 # collect 5 frames\n\n# Set the image data type as a Uint8\nconfig.video[0].camera.settings.pixel_type = acquire.SampleType.U8\n\n# Set the scale of the pixels\nconfig.video[0].storage.settings.pixel_scale_um = (1, 1) # 1 micron by 1 micron\n\n# Set the output file to out.zarr\nconfig.video[0].storage.settings.filename = \"out.zarr\"\n</code></pre> <p>To complete configuration, we'll configure the multiscale specific settings and update all settings with the <code>set_configuration</code> method.</p> <pre><code># Chunk size may need to be optimized for each acquisition.\n# See Zarr documentation for further guidance:\n# https://zarr.readthedocs.io/en/stable/tutorial.html#chunk-optimizations\nconfig.video[0].storage.settings.chunking.max_bytes_per_chunk = 16 * 2**20 # 16 MB\n\n# x, y dimensions of each chunk\n# 1/3 of the width and height of the image, generating 9 chunks\nconfig.video[0].storage.settings.chunking.tile.width = (config.video[0].camera.settings.shape[0] // 3)\nconfig.video[0].storage.settings.chunking.tile.height = (config.video[0].camera.settings.shape[1] // 3)\n\n# turn on multiscale mode\nconfig.video[0].storage.settings.enable_multiscale = True\n\n# Update the configuration with the chosen parameters\nconfig = runtime.set_configuration(config)\n</code></pre>"},{"location":"tutorials/multiscale/#collect-and-inspect-the-data","title":"Collect and Inspect the Data","text":"<pre><code># collect data\nruntime.start()\nruntime.stop()\n</code></pre> <p>You can inspect the Zarr file directory to check that the data saved as expected. This zarr file should have multiple subdirectories, one for each resolution in the multiscale data. Alternatively, you can inspect the data programmatically with:</p> <p><pre><code># Utilize the zarr python library to read the data\nimport zarr\n\n# Open the data to create a zarr Group\ngroup = zarr.open(\"out.zarr\")\n</code></pre> With multiscale mode enabled, an image pyramid will be formed by rescaling the data by a factor of 2 progressively until the rescaled image is smaller than the specified zarr chunk size in both dimensions. In this example, the original image dimensions are (1920, 1080), and we chunked the data using tiles 1/3 of the size of the image, namely (640, 360). To illustrate this point, we'll inspect the sizes of the various levels in the multiscale data and compare it to our specified chunk size.</p> <pre><code>group[\"0\"], group[\"1\"], group[\"2\"]\n</code></pre> <p>The output will be:</p> <pre><code>(&lt;zarr.core.Array '/0' (10, 1, 1080, 1920) uint8&gt;,\n &lt;zarr.core.Array '/1' (5, 1, 540, 960) uint8&gt;,\n &lt;zarr.core.Array '/2' (2, 1, 270, 480) uint8&gt;)\n</code></pre> <p>Here, the <code>\"0\"</code> directory contains the full-resolution array of frames of size 1920 x 1080, with a single channel, saving all 10 frames. The <code>\"1\"</code> directory contains the first rescaled array of frames of size 960 x 540, averaging every two frames, taking the frame count from 10 to 5. The <code>\"2\"</code> directory contains a further rescaled array of frames of size 480 x 270, averaging every four frames, taking the frame count from 10 to 2. Notice that both the frame width and frame height are now smaller than the chunk width and chunk height of 640 and 360, respectively, so this should be the last array in the group.</p> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/props_json/","title":"Properties from a JSON file","text":"<p>This tutorial will provide an example of saving and subsequently loading a <code>Properties</code> object from a JSON file.</p>"},{"location":"tutorials/props_json/#initialize-runtime","title":"Initialize Runtime","text":"<p>To start, we'll import <code>Acquire</code> and create a <code>Runtime</code> object, which coordinates the streaming process.</p> <pre><code>import acquire\nruntime = acquire.Runtime()\n</code></pre>"},{"location":"tutorials/props_json/#configure-camera","title":"Configure Camera","text":"<p>All camera settings are captured by an instance of the <code>Properties</code> class, which will be associated with a given camera acquisition.</p> <pre><code># Instantiate a Properties object for the Runtime\nconfig = runtime.get_configuration()\n</code></pre> <p>You can update any of the settings in this instance of <code>Properties</code>. To save any updated settings, use the <code>set_configuration</code> method.  For this tutorial, we'll simply specify a camera, and then save these new settings. Note that more settings must be provided before this <code>Properties</code> object could be used for an acquistion. Check out this tutorial for more information on configuring an acquisition.</p> <pre><code># set the radial sine simulated camera as the first video stream\nconfig.video[0].camera.identifier = runtime.device_manager().select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# save the updated settings\nconfig = runtime.set_configuration(config)\n</code></pre>"},{"location":"tutorials/props_json/#save-properties-to-a-json-file","title":"Save Properties to a JSON file","text":"<p>We'll utilize the json library to write our properties to a JSON file to save for subsequent acquisition.</p> <pre><code>import json\n\n# cast the properties to a dictionary\nconfig = config.dict()\n\n# convert the dictionary to json with \"human-readable\" formatting\nconfig = json.dumps(config, indent=4, sort_keys=True)\n\n# save the properties to file \"sample_props.json\" in the current directory\nwith open(\"sample_props.json\", \"w\") as outfile:\n    outfile.write(config)\n</code></pre>"},{"location":"tutorials/props_json/#example-json-file","title":"Example JSON file","text":"<p>The resulting sample_props.json file is below:</p> <pre><code>{\n  \"video\": [\n    {\n      \"camera\": {\n        \"identifier\": {\n          \"id\": [\n            0,\n            1\n          ],\n          \"kind\": \"Camera\",\n          \"name\": \"simulated: radial sin\"\n        },\n        \"settings\": {\n          \"binning\": 1,\n          \"exposure_time_us\": 0.0,\n          \"input_triggers\": {\n            \"acquisition_start\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"exposure\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"frame_start\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            }\n          },\n          \"line_interval_us\": 0.0,\n          \"offset\": [\n            0,\n            0\n          ],\n          \"output_triggers\": {\n            \"exposure\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"frame_start\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"trigger_wait\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            }\n          },\n          \"pixel_type\": \"U16\",\n          \"readout_direction\": \"Forward\",\n          \"shape\": [\n            1,\n            1\n          ]\n        }\n      },\n      \"frame_average_count\": 0,\n      \"max_frame_count\": 18446744073709551615,\n      \"storage\": {\n        \"identifier\": {\n          \"id\": [\n            0,\n            0\n          ],\n          \"kind\": \"NONE\",\n          \"name\": \"\"\n        },\n        \"settings\": {\n          \"chunking\": {\n            \"max_bytes_per_chunk\": 16777216,\n            \"tile\": {\n              \"height\": 0,\n              \"planes\": 0,\n              \"width\": 0\n            }\n          },\n          \"enable_multiscale\": false,\n          \"external_metadata_json\": \"\",\n          \"filename\": \"\",\n          \"first_frame_id\": 0,\n          \"pixel_scale_um\": [\n            0.0,\n            0.0\n          ]\n        },\n        \"write_delay_ms\": 0.0\n      }\n    },\n    {\n      \"camera\": {\n        \"identifier\": {\n          \"id\": [\n            0,\n            0\n          ],\n          \"kind\": \"NONE\",\n          \"name\": \"\"\n        },\n        \"settings\": {\n          \"binning\": 1,\n          \"exposure_time_us\": 0.0,\n          \"input_triggers\": {\n            \"acquisition_start\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"exposure\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"frame_start\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            }\n          },\n          \"line_interval_us\": 0.0,\n          \"offset\": [\n            0,\n            0\n          ],\n          \"output_triggers\": {\n            \"exposure\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"frame_start\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"trigger_wait\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            }\n          },\n          \"pixel_type\": \"U16\",\n          \"readout_direction\": \"Forward\",\n          \"shape\": [\n            0,\n            0\n          ]\n        }\n      },\n      \"frame_average_count\": 0,\n      \"max_frame_count\": 18446744073709551615,\n      \"storage\": {\n        \"identifier\": {\n          \"id\": [\n            0,\n            0\n          ],\n          \"kind\": \"NONE\",\n          \"name\": \"\"\n        },\n        \"settings\": {\n          \"chunking\": {\n            \"max_bytes_per_chunk\": 16777216,\n            \"tile\": {\n              \"height\": 0,\n              \"planes\": 0,\n              \"width\": 0\n            }\n          },\n          \"enable_multiscale\": false,\n          \"external_metadata_json\": \"\",\n          \"filename\": \"\",\n          \"first_frame_id\": 0,\n          \"pixel_scale_um\": [\n            0.0,\n            0.0\n          ]\n        },\n        \"write_delay_ms\": 0.0\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"tutorials/props_json/#load-properties-from-a-json-file","title":"Load Properties from a JSON file","text":"<p>You can load the settings in the JSON file to a <code>Properties</code> object and set this configuration for your <code>Runtime</code> as shown below:</p> <pre><code>import acquire\nimport json\n\n# create a Runtime object\nruntime = acquire.Runtime()\n\n# Instantiate a `Properties` object from the settings in sample_props.json\nconfig = acquire.Properties(**json.load(open('sample_props.json')))\n\n# save the properties for this instance of Runtime\nconfig = runtime.set_configuration(config)\n</code></pre> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/select/","title":"Device Selection","text":"<p>This tutorial illustrates the difference between the <code>select</code> and <code>select_one_of</code> methods in the <code>DeviceManager</code> class. <code>select</code> chooses the first discovered device of a specific kind, camera or storage device. You can also, optionally, select a specific device by passing the device name as a string to <code>select</code>. Whereas, <code>select_one_of</code> requires that you specify both the kind of device to select and a list of possible device names. <code>select_one_of</code> will iterate through the list and select the first device in the list of names that is discovered on your machine.</p> <p>To start, instantiate <code>Runtime</code> and <code>DeviceManager</code> objects and subsequently print the discovered devices.</p> <pre><code>import acquire\n\n# Instantiate a Runtime object\nruntime = acquire.Runtime()\n\n# Instantiate a DeviceManager object for the Runtime\nmanager = runtime.device_manager()\n\n# List devices discovered by DeviceManager\nfor device in manager.devices():\n    print(device)\n</code></pre> <p>Output of the above code is below:</p> <pre><code>&lt;DeviceIdentifier Camera \"simulated: uniform random\"&gt;\n&lt;DeviceIdentifier Camera \"simulated: radial sin\"&gt;\n&lt;DeviceIdentifier Camera \"simulated: empty\"&gt;\n&lt;DeviceIdentifier Storage \"raw\"&gt;\n&lt;DeviceIdentifier Storage \"tiff\"&gt;\n&lt;DeviceIdentifier Storage \"trash\"&gt;\n&lt;DeviceIdentifier Storage \"tiff-json\"&gt;\n&lt;DeviceIdentifier Storage \"Zarr\"&gt;\n&lt;DeviceIdentifier Storage \"ZarrBlosc1ZstdByteShuffle\"&gt;\n&lt;DeviceIdentifier Storage \"ZarrBlosc1Lz4ByteShuffle\"&gt;\n</code></pre> <p>All identified devices will be listed, and in the case of this tutorial, no cameras were connected to the machine, so only simulated cameras were found. Note that discovered storage devices will also print.</p> <p>The order of those printed devices matters. Below are two examples of how the <code>select</code> method works. In the first, without a specific device name provided, <code>select</code> will choose the first device of the specified kind in the list of discovered devices. In the second example, a specific device name is provided, so <code>select</code> will grab that device if it was discovered by <code>Runtime</code>.</p> <p><pre><code># specify that the device should be a camera and not a storage device\nkind = acquire.DeviceKind.Camera\n\n# 1st example: select the first camera in the list of discovered devices\nselected = manager.select(kind)\n\n# 2nd example: select a specific camera\nspecific = manager.select(kind, \"simulated: empty\")\n\n# print the 2 devices\nprint(selected)\nprint(specific)\n</code></pre> The output of the code is below: <pre><code>&lt;DeviceIdentifier Camera \"simulated: uniform random\"&gt;\n&lt;DeviceIdentifier Camera \"simulated: empty\"&gt;\n</code></pre></p> <p>The <code>select_one_of</code> method allows more flexibility since you provide a list of names of acceptable devices for it to iterate through until a discovered device is located.</p> <p><pre><code># specify that the device should be a camera and not a storage device\nkind = acquire.DeviceKind.Camera\n\nselected = manager.select_one_of(kind, [\"Hamamatsu_DCAMSDK4_v22126552\",\n    \"simulated: radial sin\", \"simulated: empty\"])\n\n# print which camera was selected\nprint(selected)\n</code></pre> The output of the code is below. The Hamamatsu camera was not discovered by <code>Runtime</code>, so <code>select_one_of</code> iterates until it finds a device discovered by <code>Runtime</code>. In this case, the next item in the list is a simulated camera that was discovered by <code>Runtime</code>. <pre><code>&lt;DeviceIdentifier Camera \"simulated: radial sin\"&gt;\n</code></pre></p> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/setup/","title":"Utilizing the Setup Method","text":"<p>This tutorial will provide an example of utilizing the setup method to configure <code>Runtime</code> and specify some basic properties.</p>"},{"location":"tutorials/setup/#setup-function-definition","title":"Setup Function Definition","text":"<pre><code>def setup(\n    runtime: Runtime,\n    camera: Union[str, List[str]],\n    storage: Union[str, List[str]],\n    output_filename: Optional[str],\n) -&gt; Properties\n</code></pre> <p>The <code>setup</code> function can be used as a shorthand to simplify the <code>Runtime</code> configuration process. <code>setup</code> takes a <code>Runtime</code> object and strings of the camera and storage device names and returns a <code>Properties</code> object. You may also optionally specify the filename for writing the data.</p>"},{"location":"tutorials/setup/#example","title":"Example","text":"<p><pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# use setup to get configuration and set the camera, storage, and filename\nconfig = acquire.setup(runtime, \"simulated: radial sin\", \"Zarr\", \"out.zarr\")\n</code></pre> You can subsequently use <code>config</code> to specify additional settings and set those configurations before beginning acquisition.</p> <p>Without using setup, the process would take a few additional lines of codes. The below code is equivalent to the example above.</p> <pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# Grab the current configuration\nconfig = runtime.get_configuration()\n\n# Select the radial sine simulated camera as the video source\nconfig.video[0].camera.identifier = runtime.device_manager().select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# Set the storage to Zarr to have the option to save multiscale data\nconfig.video[0].storage.identifier = runtime.device_manager().select(acquire.DeviceKind.Storage, \"Zarr\")\n\n# Set the output file to out.zarr\nconfig.video[0].storage.settings.filename = \"out.zarr\"\n</code></pre> <p>In either case, we can update the configuration settings using:</p> <pre><code>config = runtime.set_configuration(config)\n</code></pre> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/start_stop/","title":"Multiple Acquisitions","text":"<p>This tutorial will provide an example of starting, stopping, and restarting acquisition, or streaming from a video source.</p>"},{"location":"tutorials/start_stop/#configure-streaming","title":"Configure Streaming","text":"<p>To start, we'll create a <code>Runtime</code> object and configure the streaming process. To do this, we'll utilize the setup method. More information on that method is detailed in this tutorial.</p> <pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# Grab current configuration and\n# Choose Video Source and Storage Device\nconfig = acquire.setup(runtime, \"simulated: radial sin\", \"Tiff\")\n\n# Specify settings\nconfig.video[0].storage.settings.filename == \"out.tif\"\nconfig.video[0].camera.settings.shape = (192, 108)\nconfig.video[0].camera.settings.exposure_time_us = 10e4\nconfig.video[0].max_frame_count = 10\n\n# Update the configuration with the chosen parameters\nconfig = runtime.set_configuration(config)\n</code></pre>"},{"location":"tutorials/start_stop/#start-stop-and-restart-acquisition","title":"Start, Stop, and Restart Acquisition","text":"<p>During Acquisition, the <code>AvailableData</code> object is the streaming interface. Upon shutdown, <code>Runtime</code> deletes all of the objects created during acquisition to free up resources, and you must stop acquisition by calling <code>runtime.stop()</code> to shutdown after the max frames is collected or <code>runtime.abort()</code> to shutdown immediately) between acquisitions. Otherwise, an exception will be raised.</p> <p>To understand how acquisition works, we'll start, stop, and repeat acquisition and print the <code>DeviceState</code>, which can be <code>Armed</code>, <code>AwaitingConfiguration</code>, <code>Closed</code>, or <code>Running</code>, as well as print the <code>AvailableData</code> object throughout the process.</p> <p>If acquisition has ended, all of the objects are deleted, including <code>AvailableData</code> objects, so those will be <code>None</code> when not acquiring data. In addition, if enough time hasn't elapsed since acquisition started, <code>AvailableData</code> will also be <code>None</code>. We'll utilize the <code>time</code> python package to introduce time delays to account for these facts.</p> <pre><code># package used to introduce time delays\nimport time\n\n# start acquisition\nruntime.start()\n\nprint(runtime.get_state())\nprint(runtime.get_available_data(0))\n\n# wait 0.5 seconds to allow time for data to be acquired\ntime.sleep(0.5)\n\nprint(runtime.get_state())\nprint(runtime.get_available_data(0))\n\n# stop acquisition\nruntime.stop()\n\nprint(runtime.get_state())\nprint(runtime.get_available_data(0))\n\n# start acquisition\nruntime.start()\n\n# time delay of 5 sec &gt; 1 sec acquisition time\ntime.sleep(5)\n\nprint(runtime.get_state())\nprint(runtime.get_available_data(0))\n\n# stop acquisition\nruntime.stop()\n</code></pre> <p>The output will be:</p> <p><pre><code>DeviceState.Running\nNone\nDeviceState.Running\n&lt;builtins.AvailableData object at 0x00000218D685E5B0&gt;\nDeviceState.Armed\nNone\nDeviceState.Armed\n&lt;builtins.AvailableData object at 0x00000218D685E3D0&gt;\n</code></pre> 1. The first time we print is immediately after starting acquisition, so no time has elapsed for data collection as compared to the camera exposure time, so while the camera is running, <code>Running</code>, there is no data available.</p> <ol> <li> <p>The next print happens after waiting 0.5 seconds, so acquisition is still runnning and now there is acquired data available.</p> </li> <li> <p>The subsequent print is following calling <code>runtime.stop()</code> which waits until the specified max number of frames is collected and then terminates acquisition. Thus, the device is no longer running and there is no available data, since all objects were deleted by calling the <code>stop</code> method. The device is in an <code>Armed</code> state ready for the next acquisition.</p> </li> <li> <p>The final print occurs after waiting 5 seconds following the start of acquisition. This waiting period is longer than the 1 second acqusition time (0.1 seconds/frame and 10 frames), so the device is no longer collecting data. However, <code>runtime.stop()</code> hasn't been called, so the <code>AvailableData</code> object has not yet been deleted.</p> </li> </ol> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/storage/","title":"Storage Device Selection","text":"<p>This tutorial describes the storage device options in <code>Acquire</code>.</p>"},{"location":"tutorials/storage/#description-of-storage-devices","title":"Description of Storage Devices","text":"<p>To start, we'll create a <code>Runtime</code> object and print the storage device options.</p> <p><pre><code>import acquire\n\n# Instantiate a Runtime object\nruntime = acquire.Runtime()\n\n# Instantiate a DeviceManager object for the Runtime\ndm = runtime.device_manager()\n\n# Print devices in DeviceManager of kind Storage\nfor device in dm.devices():\n    if device.kind == acquire.DeviceKind.Storage:\n        print(device)\n</code></pre> The output of that script will be:</p> <pre><code>&lt;DeviceIdentifier Storage \"raw\"&gt;\n&lt;DeviceIdentifier Storage \"tiff\"&gt;\n&lt;DeviceIdentifier Storage \"trash\"&gt;\n&lt;DeviceIdentifier Storage \"tiff-json\"&gt;\n&lt;DeviceIdentifier Storage \"Zarr\"&gt;\n&lt;DeviceIdentifier Storage \"ZarrBlosc1ZstdByteShuffle\"&gt;\n&lt;DeviceIdentifier Storage \"ZarrBlosc1Lz4ByteShuffle\"&gt;\n</code></pre> <p><code>Acquire</code> supports streaming data to bigtiff and Zarr V2.</p> <p>Zarr has additional capabilities relative to the basic storage devices, namely chunking, compression, and multiscale storage. You can learn more about the Zarr capabilities in <code>Acquire</code> here.</p> <ul> <li> <p>raw - Streams to a raw binary file.</p> </li> <li> <p>tiff - Streams to a bigtiff file. Metadata is stored in the <code>ImageDescription</code> tag for each frame as a <code>JSON</code> string.</p> </li> <li> <p>trash - Writes nothing. Discards incoming data. Useful for live streaming applications.</p> </li> <li> <p>tiff-json - Stores the video stream in a bigtiff, and stores metadata in a <code>JSON</code> file. Both are located in a folder identified by the <code>filename</code> property.</p> </li> <li> <p>Zarr - Streams data to a Zarr V2 file with associated metadata.</p> </li> <li> <p>ZarrBlosc1ZstdByteShuffle - Streams compressed data (zstd codec) to a Zarr V2 file with associated metadata.</p> </li> <li> <p>ZarrBlosc1Lz4ByteShuffle - Streams compressed data (lz4 codec) to a Zarr V2 file with associated metadata.</p> </li> </ul>"},{"location":"tutorials/storage/#configure-the-storage-device","title":"Configure the Storage Device","text":"<p>In the example below, the the <code>tiff</code> storage device is selected, and the data from one video source will be streamed to a file <code>out.tif</code>.</p> <pre><code># get the current configuration\nconfig = runtime.get_configuration()\n\n# Select the tiff storage device\nconfig.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"tiff\")\n\n# Set the data filename to out.tif in your current directory (provide the whole filetree to save to a different directory)\nconfig.video[0].storage.settings.filename = \"out.tif\"\n</code></pre> <p>Before proceeding, complete the <code>Camera</code> setup and call <code>set_configuration</code> to save those new configuration settings.</p> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/trig_json/","title":"Triggers from a JSON file","text":"<p>This tutorial will provide an example of saving and subsequently loading a <code>Trigger</code> object from a JSON file.</p>"},{"location":"tutorials/trig_json/#initialize-runtime","title":"Initialize Runtime","text":"<p>To start, we'll import <code>Acquire</code> and create a <code>Runtime</code> object, which coordinates the streaming process.</p> <pre><code>import acquire\nruntime = acquire.Runtime()\n</code></pre>"},{"location":"tutorials/trig_json/#create-a-trigger-object","title":"Create a Trigger Object","text":"<p><code>Trigger</code> objects have 4 attributes: edge, enable, line, and kind. In this example, will only adjust the edge attribute.</p> <pre><code># Instantiate a Trigger object\ntrig = acquire.Trigger()\n\n# change the edge attribute from the default Rising to Falling\ntrig.edge = acquire.TriggerEdge.Falling\n</code></pre>"},{"location":"tutorials/trig_json/#save-properties-to-a-json-file","title":"Save Properties to a JSON file","text":"<p>We'll utilize the json library to write our <code>Trigger</code> to a JSON file to save for subsequent acquisition.</p> <pre><code>import json\n\n# cast the properties to a dictionary\ntrig = trig.dict()\n\n# convert the dictionary to json with \"human-readable\" formatting\ntrig = json.dumps(trig, indent=4, sort_keys=True)\n\n# save the trigger to file \"sample_trig.json\" in the current directory\nwith open(\"sample_trig.json\", \"w\") as outfile:\n    outfile.write(trig)\n</code></pre>"},{"location":"tutorials/trig_json/#example-json-file","title":"Example JSON file","text":"<p>The resulting sample_trig.json file is below:</p> <pre><code>{\n  \"edge\": \"Falling\",\n  \"enable\": false,\n  \"kind\": \"Input\",\n  \"line\": 0\n}\n</code></pre>"},{"location":"tutorials/trig_json/#load-properties-from-a-json-file","title":"Load Properties from a JSON file","text":"<p>You can load the trigger attributes in the JSON file to a <code>Trigger</code> object as shown below:</p> <pre><code># Instantiate a `Trigger` object from the settings in sample_trig.json\ntrig = acquire.Trigger(**json.load(open('sample_trig.json')))\n</code></pre> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/trigger/","title":"Finite Triggered Acquisition","text":"<p>Acquire (<code>acquire-imaging</code> on PyPI) is a Python package providing a multi-camera video streaming library focused on performant microscopy, with support for up to two simultaneous, independent, video streams.</p> <p>This tutorial shows an example of setting up triggered acquisition of a finite number of frames with one of Acquire's supported devices and saving the data to a Zarr file.</p>"},{"location":"tutorials/trigger/#initialize-acquisition","title":"Initialize Acquisition","text":"<p>To start, we'll import <code>Acquire</code> and create an acquisition <code>Runtime</code> object, which initializes the driver adaptors needed for the supported cameras.</p> <pre><code>import acquire\nruntime = acquire.Runtime()\n</code></pre>"},{"location":"tutorials/trigger/#configure-camera","title":"Configure Camera","text":"<p>All camera settings can be captured by an instance of the <code>Properties</code> class, which will be associated with a given camera acquisition. The settings can be stored in a dictionary (e.g: <code>Properties.dict()</code>). These settings can be saved to a JSON file to be subsequently loaded, (e.g. <code>Properties(**json.load(open('acquire.json')))</code>), using the json library. Check out this tutorial for a more detailed example, but in brief, you would use something like:</p> <pre><code>config = runtime.get_configuration()\n\nimport json\nwith open(\"/path/to/acquire.json\", \"w\") as f:\n    json.dump(config.dict(), f)\n</code></pre> <p>The current configuration settings can be checked and assigned to an instance of the <code>Properties</code> class with:</p> <pre><code>config = runtime.get_configuration()\n</code></pre> <p>Since <code>Acquire</code> supports 2 video streams, each camera, or source, must be configured separately. In this example, we will only use 1 source for the acquisition, so we will only need to configure <code>config.video[0]</code>. To set the first video stream to Hamamatsu Orca Fusion BT (C15440-20UP), you can use the following with a regular expression to grab the Hamamatsu camera:</p> <pre><code>config.video[0].camera.identifier = runtime.device_manager().select(acquire.DeviceKind.Camera, 'Hamamatsu C15440.*')\n</code></pre> <p>Next we'll choose the settings for the Hamamatsu camera. The <code>CameraProperties</code> class describes the available settings, which include exposure time (in microseconds), binning, pixel data type (e.g. u16), and how many frames to acquire.</p> <p>Every property can be set using the following syntax, but in this example, we will only change a few of the available settings. Check out this tutorial for an explanation of camera properties.</p> <pre><code>config.video[0].camera.settings.binning = 1 # no pixels will be combined\nconfig.video[0].camera.settings.shape = (1700, 512) # shape of the image to be acquired in pixels\nconfig.video[0].camera.settings.offset = (302, 896) # centers the image region of interest on the camera sensor\nconfig.video[0].camera.settings.pixel_type = acquire.SampleType.U16 # sets the pixel data type to a 16-bit unsigned integer\nconfig.video[0].max_frame_count = 10 # finite acquisition of 10 frames. Use 0 for infinite acquisition.\n</code></pre> <p>Triggers can also be set in the <code>CameraProperties</code> object. The parameters can be stored in a dictionary (e.g: <code>Trigger.dict()</code>). You can construct a <code>Trigger</code> from a JSON file (e.g.  <code>acquire.Trigger(**json.loads(open('trigger.json')))</code> ), using the json library. Check out this tutorial for a more detailed example, but in brief, you would use something like:</p> <pre><code>trig = acquire.Trigger()\n\nimport json\nwith open(\"/path/to/trigger.json\", \"w\") as f:\n    json.dump(trig.dict(), f)\n</code></pre> <p>In this example, we'll only utilize output triggers. By default, the camera's internal triggering is used, but you may explicitly disable external input triggers using:</p> <pre><code>config.video[0].camera.settings.input_triggers = acquire.InputTriggers() # default: disabled\n</code></pre> <p>Output triggers can be set to begin exposure, start a new frame, or wait before acquiring. We can enable an exposure trigger to start on the rising edge with:</p> <pre><code>config.video[0].camera.settings.output_triggers.exposure = acquire.Trigger(\n    enable=True, line=1, edge=\"Rising\"\n)\n</code></pre>"},{"location":"tutorials/trigger/#select-storage","title":"Select Storage","text":"<p><code>Storage</code> objects have identifiers which specify the file type (e.g. Zarr or tiff) and settings described by an instance of the <code>StorageProperties</code> class. We can set the file type to Zarr and set the file name to \"out\" with:</p> <pre><code>config.video[0].storage.identifier = runtime.device_manager().select(acquire.DeviceKind.Storage,'zarr')\nconfig.video[0].storage.settings.filename=\"out.zarr\"\n</code></pre>"},{"location":"tutorials/trigger/#save-configuration","title":"Save configuration","text":"<p>None of these settings will be updated in the <code>Properties</code> object until you call the <code>set_configuration</code> method. This method updates what the current configuration settings are on the device.</p> <p>We'll set the configuration with:</p> <pre><code>config = runtime.set_configuration(config)\n</code></pre> <p>You can optionally print out these settings using the Rich python library to save for your records with:</p> <pre><code>from rich.pretty import pprint\npprint(config.dict())\n</code></pre> <p>Check out this tutorial for a more detailed example of saving <code>Properties</code>.</p>"},{"location":"tutorials/trigger/#acquire-data","title":"Acquire data","text":"<p>To begin acquisition:</p> <pre><code>runtime.start()\n</code></pre> <p>You can stop acquisition with <code>runtime.stop()</code> to stop after the max number of frames is collected or <code>runtime.abort()</code> to immediately stop acquisition. You must call one of these methods at the end of an acquisition, as <code>Runtime</code> deletes all of the objects created during acquisition to free up resources upon shutdown. Otherwise, an exception will be raised when trying to restart acquisition.</p> <p>Download this tutorial as a Python script</p>"}]}