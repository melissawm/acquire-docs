{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Acquire Docs","text":""},{"location":"#guides","title":"Guides","text":"Get Started <p>Install Acquire and use simulated cameras</p> Get Started API Reference <p>Information on classes and methods</p> API Reference Tutorials <p>Guides on using Acquire for specific tasks</p> Tutorials For contributors <p>Learn how to contribute code or documentation to Acquire</p> For contributors"},{"location":"#about-acquire","title":"About Acquire","text":"<p>Acquire (<code>acquire-imaging</code> on PyPI) provides high-speed, multi-camera, video streaming and image acquisition with a programming interface for streaming video data directly to napari, Python and cloud-friendly file formats.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install Acquire on Windows, macOS, or Ubuntu, simply run the following command:</p> <pre><code>python -m pip install acquire-imaging\n</code></pre>"},{"location":"#supported-cameras-and-file-formats","title":"Supported Cameras and File Formats","text":"<p>Acquire supports the following cameras (currently only on Windows):</p> <ul> <li>Hamamatsu Orca Fusion BT (C15440-20UP)</li> <li>Vieworks VC-151MX-M6H00</li> <li>FLIR Blackfly USB3 (BFLY-U3-23S6M-C)</li> <li>FLIR Oryx 10GigE (ORX-10GS-51S5M-C)</li> </ul> <p>For testing and demonstration purposes, Acquire also provides a few simulated video sources. For more information on supported cameras and video sources, check out this tutorial.</p> <p>Acquire supports the following output file formats:</p> <ul> <li>Tiff</li> <li>OME-Zarr for Zarr v2</li> <li>Zarr v3</li> </ul> <p>Acquire also supports raw and trash storage devices. For more information on supported file formats and storage devices, check out this tutorial.</p>"},{"location":"#citing-acquire","title":"Citing Acquire","text":"<pre><code>cff-version: 1.2.0\ntitle: Acquire: a multi-camera video streaming software focusing on microscopy\nmessage: &gt;-\n  If you use this software, please cite it using the\n  metadata from this file.\ntype: software\nauthors:\n  - given-names: Nathan\n    family-names: Clack\n    email: nclack@chanzuckerberg.com\n    affiliation: Chan-Zuckerberg Initiative Foundation\n    orcid: 'https://orcid.org/0000-0001-6236-9282'\n  - given-names: Alan\n    family-names: Liddell\n    email: aliddell@chanzuckerberg.com\n    affiliation: Chan-Zuckerberg Initiative Foundation\n  - given-names: Andrew\n    family-names: Sweet\n    email: andrewdsweet@gmail.com\n    affiliation: Chan-Zuckerberg Initiative Foundation\nrepository-code: 'https://github.com/acquire-project/acquire-python'\nrepository-artifact: 'https://pypi.org/project/acquire-imaging/'\nabstract: &gt;-\n  acquire-imaging is a library focusing on multi-camera video\n  streaming for microscopy.\nlicense: Apache-2.0\n</code></pre>"},{"location":"#acquire-license","title":"Acquire License","text":"<p><code>Acquire</code> is provided under an Apache 2.0 license. You can learn more about the Apache license in the documentation here.</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>Information on the classes in <code>acquire-imaging</code> along with the attributes and methods associated with them.</p>"},{"location":"api_reference/#class-availabledata","title":"Class <code>AvailableData</code>","text":"<p>The <code>AvailableData</code> class represents the collection of frames that have been captured since the last call to runtime.get_available_data(). <code>AvailableData</code> objects should be set to have a short lifetime, since these objects reserve space on the video queue and will eventually block camera acquisition to ensure no data is overwritten before it can be processed.</p> <pre><code>class AvailableData:\n    def frames(self) -&gt; Iterator[VideoFrame]:\n        \"\"\"Returns an iterator over the video frames in the available data.\"\"\"\n\n    def get_frame_count(self) -&gt; int:\n        \"\"\"Returns the total number of video frames in the available data.\"\"\"\n\n    def __iter__(self) -&gt; Iterator[VideoFrame]:\n        \"\"\"Returns an iterator over the video frames in the available data.\"\"\"\n</code></pre> <ul> <li> <p>The <code>frames</code> method provides an iterator over these frames.</p> </li> <li> <p>Call <code>get_frame_count()</code> to query the number of frames in an <code>AvailableData</code> object.</p> </li> <li> <p>The <code>__iter__</code> method enables <code>AvailableData</code> objects to be iterated.</p> </li> </ul>"},{"location":"api_reference/#class-camera","title":"Class <code>Camera</code>","text":"<p>The <code>Camera</code> class is used to describe cameras or other video sources.</p> <pre><code>class Camera:\n    identifier: Optional[DeviceIdentifier]\n    settings: CameraProperties\n\n    def __init__(self, *args: None, **kwargs: Any) -&gt; None: ...\n    \"\"\"Initializes a Camera object with optional arguments.\"\"\"\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the Camera attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>identifier</code>: An optional attribute which contains an instance of the <code>DeviceIdentifier</code> class. <code>DeviceIdentifier</code> has <code>id</code> and <code>kind</code> attributes assigned by <code>Acquire</code> if the device is natively supported. Otherwise, it is of type <code>None</code>.</p> </li> <li> <p><code>settings</code>: An instance of the <code>CameraProperties</code> class which contains the settings for the camera.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>Camera</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-cameraproperties","title":"Class <code>CameraProperties</code>","text":"<p>The <code>CameraProperties</code> class is used to set the desired camera properties for acquisition.</p> <pre><code>class CameraProperties:\n    exposure_time_us: float\n    line_interval_us: float\n    binning: float\n    pixel_type: SampleType\n    readout_direction: Direction\n    offset: Tuple[int, int]\n    shape: Tuple[int, int]\n    input_triggers: InputTriggers\n    output_triggers: OutputTriggers\n\n    def __init__(self, *args: None, **kwargs: Any) -&gt; None: ...\n    \"\"\"Initializes a CameraProperties object with optional arguments.\"\"\"\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the CameraProperties attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>exposure_time_us</code>: How long in microseconds your camera should collect light from the sample. However, for simulated cameras, this is just a waiting period before generating the next frame.</p> </li> <li> <p><code>line_interval_us</code>: The time to scan one line in microseconds in a rolling shutter camera.</p> </li> <li> <p><code>binning</code>: How many adjacent pixels in each direction to combine by averaging. For example, if <code>binning</code> is set to 2, a 2x2 square of pixels will be combined by averaging. If <code>binning</code> is set to 1, no pixels will be combined.</p> </li> <li> <p><code>pixel_type</code>: An instance of the <code>SampleType</code> class which specifies the numerical data type, for example Uint16, a 16-bit unsigned integer type.</p> </li> <li> <p><code>readout_direction</code>: An instance of the <code>Direction</code> class which specifies whether the data is readout forwards or backwards.</p> </li> <li> <p><code>offset</code>: A tuple of two integers representing the (x, y) offset in pixels of the image region of interest on the camera.</p> </li> <li> <p><code>shape</code>: A tuple of two integers representing the (x, y)size in pixels of the image region of interest on the camera.</p> </li> <li> <p><code>input_triggers</code>: An instance of the <code>InputTriggers</code> class, which describes the trigger signals for starting acquisition, camera exposure, and acquiring a frame.</p> </li> <li> <p><code>output_triggers</code>: An instance of the <code>OutputTriggers</code> class, which describes the trigger signals for the camera exposure, acquiring a frame, as well as any wait times for sending the trigger signal.</p> </li> <li> <p>The <code>dict</code> method create a dictionary of a <code>CameraProperties</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-chunkingproperties","title":"Class <code>ChunkingProperties</code>","text":"<p>The <code>ChunkingProperties</code> class represents properties related to data chunking for storage in a Zarr container.</p> <pre><code>class ChunkingProperties:\n    max_bytes_per_chunk: int\n    tile: TileShape\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the ChunkingProperties attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>max_bytes_per_chunk</code>: The maximum number of bytes per data chunk.</p> </li> <li> <p><code>tile</code>: An instance of the <code>TileShape</code> class representing the shape of the data chunk tile.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>ChunkingProperties</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-deviceidentifier","title":"Class <code>DeviceIdentifier</code>","text":"<p>The <code>DeviceIdentifier</code> class represents an identifier for a supported device, including its unique id and type, such as a camera or storage.</p> <pre><code>class DeviceIdentifier:\n    id: Tuple[int, int]\n    kind: DeviceKind\n    name: str\n\n    def __init__(self, *args: None, **kwargs: Any) -&gt; None: ...\n    \"\"\"Initializes a DeviceIdentifier object with optional arguments.\"\"\"\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the DeviceIdentifier attributes.\"\"\"\n\n    @staticmethod\n    def none() -&gt; DeviceIdentifier: ...\n    \"\"\"Returns a \"None\" type DeviceIdentifier.\n    Useful when a DeviceIdentifier is not needed.\"\"\"\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two DeviceIdentifier objects are equal.\"\"\"\n\n    def __ge__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this DeviceIdentifier is greater than or equal to another.\"\"\"\n\n    def __gt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this DeviceIdentifier is greater than another.\"\"\"\n\n    def __le__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this DeviceIdentifier is less than or equal to another.\"\"\"\n\n    def __lt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this DeviceIdentifier is less than another.\"\"\"\n\n    def __ne__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two DeviceIdentifier objects are not equal.\"\"\"\n</code></pre> <ul> <li> <p><code>id</code>: A tuple (driver_id, device_id) containing two Uint8 integers that serve to identify each driver and device uniquely for a given run.</p> </li> <li> <p><code>kind</code>: An instance of the <code>DeviceKind</code> class that represents the type or kind of the device.</p> </li> <li> <p><code>name</code>: A string representing the name or label of the device.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>DeviceIdentifier</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-devicekind","title":"Class <code>DeviceKind</code>","text":"<p>The <code>DeviceKind</code> class represents the types of devices in a given system.</p> <pre><code>class DeviceKind:\n    Camera: ClassVar[DeviceKind] = DeviceKind.Camera\n    NONE: ClassVar[DeviceKind] = DeviceKind.NONE\n    Signals: ClassVar[DeviceKind] = DeviceKind.Signals\n    StageAxis: ClassVar[DeviceKind] = DeviceKind.StageAxis\n    Storage: ClassVar[DeviceKind] = DeviceKind.Storage\n\n    def __init__(self, *args: None, **kwargs: Any) -&gt; None:\n        \"\"\"Initializes the DeviceKind class.\"\"\"\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two DeviceKind objects are equal.\"\"\"\n\n    def __ge__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this DeviceKind is greater than or equal to another.\"\"\"\n\n    def __gt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this DeviceKind is greater than another.\"\"\"\n\n    def __int__(self) -&gt; int:\n        \"\"\"Converts the DeviceKind to an integer.\"\"\"\n\n    def __le__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this DeviceKind is less than or equal to another.\"\"\"\n\n    def __lt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this DeviceKind is less than another.\"\"\"\n\n    def __ne__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two DeviceKind objects are not equal.\"\"\"\n</code></pre> <ul> <li> <p><code>Camera</code>: Enum-type class variable of <code>DeviceKind</code> that specifies a device is a camera.</p> </li> <li> <p><code>NONE</code>: Enum-type class variable of <code>DeviceKind</code> for if a device's kind is unavailable.</p> </li> <li> <p><code>Signals</code>: Enum-type class variable of <code>DeviceKind</code> that specifies a device is a signal.</p> </li> <li> <p><code>StageAxis</code>: Enum-type class variable of <code>DeviceKind</code> that specifies a device is a stage.</p> </li> <li> <p><code>Storage</code>: Enum-type class variable of <code>DeviceKind</code> that specifies a device is for storage.</p> </li> </ul>"},{"location":"api_reference/#class-devicemanager","title":"Class <code>DeviceManager</code>","text":"<p>The <code>DeviceManager</code> class manages selection of available devices in the system. Regular expressions are accepted for the name argument.</p> <pre><code>class DeviceManager:\n    def devices(self) -&gt; List[DeviceIdentifier]:\n        \"\"\"Returns a list of all available device identifiers.\"\"\"\n\n    @overload\n    def select(self, kind: DeviceKind, name: Optional[str]) -&gt; Optional[DeviceIdentifier]:\n        \"\"\"Selects a specified device.\n\n        Args:\n            kind (DeviceKind): The type of device to select.\n            name (Optional[str]): The name of the device to select. Regular expressions supported.\n\n        Returns:\n            Optional[DeviceIdentifier]: The selected device identifier, or None if the specified device is not available.\n        \"\"\"\n\n    def select_one_of(self, kind: DeviceKind, names: List[str]) -&gt; Optional[DeviceIdentifier]:\n        \"\"\"Selects the first device in the list of devices that is of one of the specified kinds.\n\n        Args:\n            kind (DeviceKind): The type of device to select.\n            names (List[str]): A list of device names to choose from. Regular expressions supported.\n\n        Returns:\n            Optional[DeviceIdentifier]: The selected device identifier, or None if none of the specified devices are available.\n        \"\"\"\n</code></pre> <ul> <li> <p>Call <code>devices</code> to list the <code>DeviceIdentifier</code> of each available device.</p> </li> <li> <p>Call <code>select</code> to choose the first available device of a given type or to select a specific device by name.</p> </li> <li> <p>Call <code>select_one_of</code> to choose one device from a list of acceptable devices of a given kind.</p> </li> </ul>"},{"location":"api_reference/#class-devicestate","title":"Class <code>DeviceState</code>","text":"<p>The <code>DeviceState</code> class represents the acquisition status of a device.</p> <pre><code>class DeviceState:\n    Closed: ClassVar[DeviceState] = DeviceState.Closed\n    AwaitingConfiguration: ClassVar[DeviceState] = DeviceState.AwaitingConfiguration\n    Armed: ClassVar[DeviceState] = DeviceState.Armed\n    Running: ClassVar[DeviceState] = DeviceState.Running\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two DeviceState objects are equal.\"\"\"\n\n    def __ge__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this DeviceState is greater than or equal to another.\"\"\"\n\n    def __gt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this DeviceState is greater than another.\"\"\"\n\n    def __int__(self) -&gt; int:\n        \"\"\"Converts the DeviceState to an integer.\"\"\"\n\n    def __le__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this DeviceState is less than or equal to another.\"\"\"\n\n    def __lt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this DeviceState is less than another.\"\"\"\n\n    def __ne__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two DeviceState objects are not equal.\"\"\"\n</code></pre> <ul> <li> <p><code>Closed</code>: Enum-type class variable of <code>DeviceState</code> that species when a device is not ready for configuration.</p> </li> <li> <p><code>AwaitingConfiguration</code>: Enum-type class variable of <code>DeviceState</code> that species when a device is ready for configuration.</p> </li> <li> <p><code>Armed</code>: Enum-type class variable of <code>DeviceState</code> that species when a device ready to stream data.</p> </li> <li> <p><code>Running</code>: Enum-type class variable of <code>DeviceState</code> that species when a device is streaming data.</p> </li> </ul>"},{"location":"api_reference/#class-direction","title":"Class <code>Direction</code>","text":"<p>The <code>Direction</code> class represents the direction that data is read for streaming.</p> <pre><code>class Direction:\n    Backward: ClassVar[Direction] = Direction.Backward\n    Forward: ClassVar[Direction] = Direction.Forward\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two Direction objects are equal.\"\"\"\n\n    def __ge__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this Direction is greater than or equal to another.\"\"\"\n\n    def __gt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this Direction is greater than another.\"\"\"\n\n    def __int__(self) -&gt; int:\n        \"\"\"Converts the Direction to an integer.\"\"\"\n\n    def __le__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this Direction is less than or equal to another.\"\"\"\n\n    def __lt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this Direction is less than another.\"\"\"\n\n    def __ne__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two Direction objects are not equal.\"\"\"\n</code></pre> <ul> <li> <p><code>Backward</code>: Enum-type class variable of <code>Direction</code> that species when data is streamed backward.</p> </li> <li> <p><code>Forward</code>: Enum-type class variable of <code>Direction</code> that species when data is streamed forward.</p> </li> </ul>"},{"location":"api_reference/#class-inputtriggers","title":"Class <code>InputTriggers</code>","text":"<p>The <code>InputTriggers</code> class represents input triggers for a camera device.</p> <pre><code>class InputTriggers:\n    acquisition_start: Trigger\n    exposure: Trigger\n    frame_start: Trigger\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the InputTriggers attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>acquisition_start</code>: An instance of the <code>Trigger</code> class representing the trigger for starting acquisition.</p> </li> <li> <p><code>exposure</code>: An instance of the <code>Trigger</code> class representing the trigger for exposure.</p> </li> <li> <p><code>frame_start</code>: An instance of the <code>Trigger</code> class representing the trigger for starting a frame.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>InputTriggers</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-outputtriggers","title":"Class <code>OutputTriggers</code>","text":"<p>The <code>OutputTriggers</code> class represents output triggers for a camera device.</p> <pre><code>class OutputTriggers:\n    exposure: Trigger\n    frame_start: Trigger\n    trigger_wait: Trigger\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the OutputTriggers attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>exposure</code>: An instance of the <code>Trigger</code> class representing the trigger for exposure.</p> </li> <li> <p><code>frame_start</code>: An instance of the <code>Trigger</code> class representing the trigger for starting a frame.</p> </li> <li> <p><code>trigger_wait</code>: An instance of the <code>Trigger</code> class representing the trigger for waiting before continuing acquisition.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>OutputTriggers</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-pid","title":"Class <code>PID</code>","text":"<p>The <code>PID</code> class represents proportional-integral-derivative (PID) values.</p> <pre><code>class PID:\n    derivative: float\n    integral: float\n    proportional: float\n\n    def __init__(self, *args: None, **kwargs: Any) -&gt; None: ...\n    \"\"\"Initializes a PID object with optional arguments.\"\"\"\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the PID attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>derivative</code>: The derivative value for the PID.</p> </li> <li> <p><code>integral</code>: The integral value for the PID.</p> </li> <li> <p><code>proportional</code>: The proportional value for the PID.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>PID</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-properties","title":"Class <code>Properties</code>","text":"<p>The <code>Properties</code> class represents properties related to video streams.</p> <pre><code>class Properties:\n    video: Tuple[VideoStream, VideoStream]\n\n    def __init__(self, *args: None, **kwargs: Any) -&gt; None: ..\n    \"\"\"Initializes a Properties object with optional arguments.\"\"\".\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the Properties attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>video</code>: A tuple containing two <code>VideoStream</code> instances since <code>Acquire</code> supports simultaneous streaming from 2 video sources. <code>VideoStream</code> objects have 2 attributes <code>camera</code> and <code>storage</code> to set the source and sink for the stream.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>Properties</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-runtime","title":"Class <code>Runtime</code>","text":"<p>The <code>Runtime</code> class coordinates the devices with the storage disc including selecting the devices, setting their properties, and starting and stopping acqusition.</p> <pre><code>class Runtime:\n    def __init__(self, *args: None, **kwargs: Any) -&gt; None:\n        \"\"\"Initializes the Runtime object with optional arguments.\"\"\"\n\n    def device_manager(self) -&gt; DeviceManager:\n        \"\"\"Returns the DeviceManager instance associated with this Runtime.\"\"\"\n\n    def get_available_data(self, stream_id: int) -&gt; AvailableData:\n        \"\"\"Returns the AvailableData instance for the given stream ID.\n\n        Args:\n            stream_id (int): The ID of the stream for which available data is requested.\n\n        Returns:\n            AvailableData: The AvailableData instance for the given VideoStream ID.\n        \"\"\"\n\n    def get_configuration(self) -&gt; Properties:\n        \"\"\"Returns the current configuration properties of the runtime.\"\"\"\n\n    def get_state(self) -&gt; DeviceState:\n        \"\"\"Returns the current state of the device.\"\"\"\n\n    def set_configuration(self, properties: Properties) -&gt; Properties:\n        \"\"\"Applies the provided configuration properties to the runtime.\n\n        Args:\n            properties (Properties): The properties to be set.\n\n        Returns:\n            Properties: The updated configuration properties.\n        \"\"\"\n\n    def start(self) -&gt; None:\n        \"\"\"Starts the runtime, allowing it to collect data.\"\"\"\n\n    def stop(self) -&gt; None:\n        \"\"\"Stops the runtime, ending data collection after the max number of frames is collected.\"\"\"\n\n    def abort(self) -&gt; None:\n        \"\"\"Aborts the runtime, terminating it immediately.\"\"\"\n</code></pre> <ul> <li> <p>Call <code>device_manager()</code> to return the <code>DeviceManager</code> object associated with this <code>Runtime</code> instance.</p> </li> <li> <p>Call <code>get_available_data</code> with a specific <code>stream_id</code>, 0 or 1, to return the <code>AvailableData</code> associated with the 1st or 2nd video source, respectively.</p> </li> <li> <p>Call <code>get_configuration()</code> to return the <code>Properties</code> object associated with this <code>Runtime</code> instance.</p> </li> <li> <p>Call <code>get_state()</code> to return the <code>DeviceState</code> object associated with this <code>Runtime</code> instance.</p> </li> <li> <p>Call <code>set_configuration</code> with a <code>Properties</code> object to change the properties of this <code>Runtime</code> instance.</p> </li> <li> <p>Call <code>start()</code> to begin data acquisition.</p> </li> <li> <p>Call <code>stop()</code> to end data acquisition once the max number of frames specified in <code>acquire.VideoStream.max_frame_count</code> is collected. All objects are deleted to free up disk space upon shutdown of <code>Runtime</code>.</p> </li> <li> <p>Call <code>abort()</code> to immediately end data acqusition. All objects are deleted to free up disk space upon shutdown of <code>Runtime</code>.</p> </li> </ul>"},{"location":"api_reference/#class-sampleratehz","title":"Class <code>SampleRateHz</code>","text":"<p>The <code>SampleRateHz</code> class represents the sampling rate in hertz.</p> <pre><code>class SampleRateHz:\n    numerator: int\n    denominator: int\n\n    def __init__(self, *args: None, **kwargs: Any) -&gt; None: ...\n    \"\"\"Initializes a SampleRateHz object with optional arguments.\"\"\"\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the SampleRateHz attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>numerator</code>: The numerator part of the sampling rate fraction.</p> </li> <li> <p><code>denominator</code>: The denominator part of the sampling rate fraction.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>SampleRateHz</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-sampletype","title":"Class <code>SampleType</code>","text":"<p>The <code>SampleType</code> class defines the type of the values in the streamed data.</p> <pre><code>class SampleType:\n    F32: ClassVar[SampleType] = SampleType.F32\n    I16: ClassVar[SampleType] = SampleType.I16\n    I8: ClassVar[SampleType] = SampleType.I8\n    U16: ClassVar[SampleType] = SampleType.U16\n    U8: ClassVar[SampleType] = SampleType.U8\n    U10: ClassVar[SampleType] = SampleType.U10\n    U12: ClassVar[SampleType] = SampleType.U12\n    U14: ClassVar[SampleType] = SampleType.U14\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two SampleType objects are equal.\"\"\"\n\n    def __ge__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this SampleType is greater than or equal to another.\"\"\"\n\n    def __gt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this SampleType is greater than another.\"\"\"\n\n    def __int__(self) -&gt; int:\n        \"\"\"Converts the SampleType to an integer.\"\"\"\n\n    def __le__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this SampleType is less than or equal to another.\"\"\"\n\n    def __lt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this SampleType is less than another.\"\"\"\n\n    def __ne__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two SampleType objects are not equal.\"\"\"\n</code></pre> <ul> <li> <p><code>F32</code>: Enum-type class variable of <code>SampleType</code> that specifies values of 32-bit floating point type.</p> </li> <li> <p><code>I16</code>: Enum-type class variable of <code>SampleType</code> that specifies values of 16-bit signed integer type.</p> </li> <li> <p><code>I8</code>: Enum-type class variable of <code>SampleType</code> that specifies values of 8-bit signed integer type.</p> </li> <li> <p><code>U16</code>: Enum-type class variable of <code>SampleType</code> that specifies values of 16-bit unsigned integer type.</p> </li> <li> <p><code>U8</code>: Enum-type class variable of <code>SampleType</code> that specifies values of 8-bit unsigned integer type.</p> </li> <li> <p><code>U10</code>: Enum-type class variable of <code>SampleType</code> that specifies values of 10-bit unsigned integer type.</p> </li> <li> <p><code>U12</code>: Enum-type class variable of <code>SampleType</code> that specifies values of 12-bit unsigned integer type.</p> </li> <li> <p><code>U14</code>: Enum-type class variable of <code>SampleType</code> that specifies values of 14-bit unsigned integer type.</p> </li> </ul>"},{"location":"api_reference/#class-signaliokind","title":"Class <code>SignalIOKind</code>","text":"<p>The <code>SignalIOKind</code> class defines the signal type, input or output, for a trigger.</p> <pre><code>class SignalIOKind:\n    Input: ClassVar[SignalIOKind] = SignalIOKind.Input\n    Output: ClassVar[SignalIOKind] = SignalIOKind.Output\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two SignalIOKind objects are equal.\"\"\"\n\n    def __ge__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this SignalIOKind is greater than or equal to another.\"\"\"\n\n    def __gt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this SignalIOKind is greater than another.\"\"\"\n\n    def __int__(self) -&gt; int:\n        \"\"\"Converts the SignalIOKind to an integer.\"\"\"\n\n    def __le__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this SignalIOKind is less than or equal to another.\"\"\"\n\n    def __lt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this SignalIOKind is less than another.\"\"\"\n\n    def __ne__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two SignalIOKind objects are not equal.\"\"\"\n</code></pre> <ul> <li> <p><code>Input</code>: Enum-type class variable of <code>SignalIOKind</code> that specifies signal coming in to the device.</p> </li> <li> <p><code>Output</code>: Enum-type class variable of <code>SignalIOKind</code> that specifies signal sent out of the device.</p> </li> </ul>"},{"location":"api_reference/#class-signaltype","title":"Class <code>SignalType</code>","text":"<p>The <code>SignalType</code> class specifies whether a signal is analog or digital.</p> <pre><code>class SignalType:\n    Analog: ClassVar[SignalType] = SignalType.Analog\n    Digital: ClassVar[SignalType] = SignalType.Digital\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two SignalType objects are equal.\"\"\"\n\n    def __ge__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this SignalType is greater than or equal to another.\"\"\"\n\n    def __gt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this SignalType is greater than another.\"\"\"\n\n    def __int__(self) -&gt; int:\n        \"\"\"Converts the SignalType to an integer.\"\"\"\n\n   def __le__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this SignalType is less than or equal to another.\"\"\"\n\n    def __lt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this SignalType is less than another.\"\"\"\n\n    def __ne__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two SignalType objects are not equal.\"\"\"\n</code></pre> <ul> <li> <p><code>Analog</code>: Enum-type class variable of <code>SignalType</code> that specifies a signal is analog.</p> </li> <li> <p><code>Input</code>: Enum-type class variable of <code>SignalType</code> that specifies signal coming in to the device.</p> </li> </ul>"},{"location":"api_reference/#class-storage","title":"Class <code>Storage</code>","text":"<p>The <code>Storage</code> class represents storage devices and their settings.</p> <pre><code>class Storage:\n    identifier: Optional[DeviceIdentifier]\n    settings: StorageProperties\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the Storage attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>identifier</code>: An optional attribute which contains an instance of the <code>DeviceIdentifier</code> class that describes the storage device if that device is natively supported. Otherwise, it is of type <code>None</code>.</p> </li> <li> <p><code>settings</code>: An instance of the <code>StorageProperties</code> class which contains the settings for the data storage.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>Storage</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-storageproperties","title":"Class <code>StorageProperties</code>","text":"<p>The <code>StorageProperties</code> class represents properties for data storage.</p> <pre><code>class StorageProperties:\n    external_metadata_json: Optional[str]\n    filename: Optional[str]\n    first_frame_id: int\n    pixel_scale_um: Tuple[float, float]\n    chunking: ChunkingProperties\n    enable_multiscale: bool\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the StorageProperties attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>external_metadata_json</code>: An optional attribute of the metadata JSON filename as a string.</p> </li> <li> <p><code>filename</code>: An optional attribute representing the filename for storing the image data.</p> </li> <li> <p><code>first_frame_id</code>: An integer representing the ID of the first frame for a given acquisition.</p> </li> <li> <p><code>pixel_scale_um</code>: A tuple of two floats representing the pixel size of the camera in micrometers.</p> </li> <li> <p><code>chunking</code>: An instance of the <code>ChunkingProperties</code> class representing data chunking settings for Zarr storage.</p> </li> <li> <p><code>enable_multiscale</code>: A boolean indicating whether multiscale storage is enabled.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>StorageProperties</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-tileshape","title":"Class <code>TileShape</code>","text":"<p>The <code>TileShape</code> class represents the shape of data chunks for storage in Zarr containers.</p> <pre><code>class TileShape:\n    width: int\n    height: int\n    planes: int\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the TileShape attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>width</code>: The width of the chunk.</p> </li> <li> <p><code>height</code>: The height of the chunk.</p> </li> <li> <p><code>planes</code>: The number of planes in the chunk.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>TileShape</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-trigger","title":"Class <code>Trigger</code>","text":"<p>The <code>Trigger</code> class represents a trigger signal.</p> <pre><code>class Trigger:\n    edge: TriggerEdge\n    enable: bool\n    line: int\n    kind: SignalIOKind\n\n    def __init__(self, *args: None, **kwargs: Any) -&gt; None: ...\n    \"\"\"Initializes a Trigger object with optional arguments.\"\"\"\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the Trigger attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>edge</code>: An instance of the <code>TriggerEdge</code> class specifying if the trigger is on the rising or falling edge trigger signal.</p> </li> <li> <p><code>enable</code>: A boolean indicating whether the trigger is enabled.</p> </li> <li> <p><code>line</code>: An integer representing the max value of the trigger signal.</p> </li> <li> <p><code>kind</code>: An instance of the <code>SignalIOKind</code> class specifying if the signal is input or output.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>Trigger</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-triggeredge","title":"Class <code>TriggerEdge</code>","text":"<p>The <code>TriggerEdge</code> class represents what edge of the trigger function initiates the trigger.</p> <pre><code>class TriggerEdge:\n    Falling: ClassVar[TriggerEdge] = TriggerEdge.Falling\n    NotApplicable: ClassVar[TriggerEdge] = TriggerEdge.NotApplicable\n    Rising: ClassVar[TriggerEdge] = TriggerEdge.Rising\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two TriggerEdge objects are equal.\"\"\"\n\n    def __ge__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this TriggerEdge is greater than or equal to another.\"\"\"\n\n    def __gt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this TriggerEdge is greater than another.\"\"\"\n\n    def __int__(self) -&gt; int:\n        \"\"\"Converts the TriggerEdge to an integer.\"\"\"\n\n    def __le__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this TriggerEdge is less than or equal to another.\"\"\"\n\n    def __lt__(self, other: object) -&gt; bool:\n        \"\"\"Checks if this TriggerEdge is less than another.\"\"\"\n\n    def __ne__(self, other: object) -&gt; bool:\n        \"\"\"Checks if two TriggerEdge objects are not equal.\"\"\"\n</code></pre> <ul> <li> <p><code>Falling</code>: Enum-type class variable of <code>TriggerEdge</code> that defines the falling edge of the trigger.</p> </li> <li> <p><code>NotApplicable</code>: Enum-type class variable of <code>TriggerEdge</code> that defines if a trigger does not have a rising or falling edge.</p> </li> <li> <p><code>Rising</code>: Enum-type class variable of <code>TriggerEdge</code> that defines the rising edge of the trigger.</p> </li> </ul>"},{"location":"api_reference/#class-videoframe","title":"Class <code>VideoFrame</code>","text":"<p>The <code>VideoFrame</code> class represents data from acquisition of a frame.</p> <pre><code>class VideoFrame:\n    def data(self) -&gt; NDArray[Any]:\n        \"\"\"Returns the data of the video frame as an NDArray.\"\"\"\n\n    def metadata(self) -&gt; VideoFrameMetadata:\n        \"\"\"Returns the metadata associated with the video frame.\"\"\"\n</code></pre> <ul> <li> <p>Call <code>data()</code> to create an NDArray of the <code>VideoFrame</code> data.</p> </li> <li> <p>Call <code>metadata()</code> to create a <code>VideoFrameMetadata</code> object containing the metadata of <code>VideoFrame</code>.</p> </li> </ul>"},{"location":"api_reference/#class-videoframemetadata","title":"Class <code>VideoFrameMetadata</code>","text":"<p>The <code>VideoFrameMetadata</code> class represents metadata related to a video frame.</p> <pre><code>class VideoFrameMetadata:\n    frame_id: int\n    timestamps: VideoFrameTimestamps\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the VideoFrameMetadata attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>frame_id</code>: An integer representing the ID of the video frame.</p> </li> <li> <p><code>timestamps</code>: An instance of the <code>VideoFrameTimestamps</code> class specifying the video timestamps based on the hardware clock and the acquisition clock.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>VideoFrameTimestamps</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-videoframetimestamps","title":"Class <code>VideoFrameTimestamps</code>","text":"<p>The <code>VideoFrameTimestamps</code> class represents timestamps related to a video frame.</p> <pre><code>class VideoFrameTimestamps:\n    hardware: int\n    acq_thread: int\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the VideoFrameTimestamps attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>hardware</code>: An integer representing hardware timestamps.</p> </li> <li> <p><code>acq_thread</code>: An integer representing timestamps from the acquisition thread.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>VideoFrameTimestamps</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-videostream","title":"Class <code>VideoStream</code>","text":"<p>The <code>VideoStream</code> class represents a video stream.</p> <pre><code>class VideoStream:\n    camera: Camera\n    storage: Storage\n    max_frame_count: int\n    frame_average_count: int\n\n    def dict(self) -&gt; Dict[str, Any]: ...\n    \"\"\"Returns a dictionary of the VideoStream attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>camera</code>: An instance of the <code>Camera</code> class representing the camera device for the video stream.</p> </li> <li> <p><code>storage</code>: An instance of the <code>Storage</code> class representing the storage device for the video stream.</p> </li> <li> <p><code>max_frame_count</code>: An integer representing the maximum number of frames to acquire.</p> </li> <li> <p><code>frame_average_count</code>: An integer representing the number of frames to average, if any, before streaming. The default value is 0, which disables this feature. Setting this to 1 will also prevent averaging.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>VideoStream</code> object's attributes.</p> </li> </ul>"},{"location":"api_reference/#class-voltagerange","title":"Class <code>VoltageRange</code>","text":"<p>The <code>VoltageRange</code> class represents a range of voltage values.</p> <pre><code>class VoltageRange:\n    mn: float\n    mx: float\n\n    @overload\n    def __init__(self) -&gt; None: ...\n    \"\"\"Initializes a VoltageRange object\"\"\"\n\n    @overload\n    def __init__(self, mn: float, mx: float) -&gt; None: ...\n    \"\"\"Initializes a VoltageObject object with mn and mx provided.\"\"\"\n\n    def dict(self) -&gt; Dict[str, float]: ...\n    \"\"\"Returns a dictionary of the VoltageRange attributes.\"\"\"\n</code></pre> <ul> <li> <p><code>mn</code>: A float representing the minimum voltage value.</p> </li> <li> <p><code>mx</code>: A float representing the maximum voltage value.</p> </li> <li> <p>The <code>dict</code> method creates a dictionary of a <code>VoltageRange</code> object's attributes.</p> </li> </ul>"},{"location":"get_started/","title":"Getting Started with Acquire","text":"<p>Acquire (<code>acquire-imaging</code> on PyPI) is a Python package providing a multi-camera video streaming library focused on performant microscopy, with support for up to two simultaneous, independent, video streams.</p> <p>This tutorial covers Acquire installation and shows an example of using Acquire with its provided simulated cameras to demonstrate the acquisition process.</p>"},{"location":"get_started/#installation","title":"Installation","text":"<p>To install Acquire on Windows, macOS, or Ubuntu, simply run the following command:</p> <pre><code>python -m pip install acquire-imaging\n</code></pre> <p>We recommend installing <code>Acquire</code> in a fresh conda environment or virtualenv. For example, to install <code>Acquire</code> in a conda environment named <code>acquire</code>:</p> <pre><code>conda create -n acquire python=3.10 # follow the prompts and proceed with the defaults\nconda activate acquire\npython -m pip install acquire-imaging\n</code></pre> <p>or with virtualenv:</p> <pre><code>$ python -m venv venv\n$ . ./venv/bin/activate # or on Windows: .\\venv\\Scripts\\Activate.bat or .\\venv\\Scripts\\Activate.ps1\n(venv) $ python -m pip install acquire-imaging\n</code></pre> <p>Once you have Acquire installed, simply call <code>import acquire</code> in your script, notebook, or module to start utilizing the package.</p> <pre><code>import acquire\n</code></pre>"},{"location":"get_started/#supported-cameras-and-file-formats","title":"Supported Cameras and File Formats","text":"<p>Acquire supports the following cameras (currently only on Windows):</p> <ul> <li>Hamamatsu Orca Fusion BT (C15440-20UP)</li> <li>Vieworks VC-151MX-M6H00</li> <li>FLIR Blackfly USB3 (BFLY-U3-23S6M-C)</li> <li>FLIR Oryx 10GigE (ORX-10GS-51S5M-C)</li> </ul> <p>Acquire also supports the following output file formats:</p> <ul> <li>Tiff</li> <li>Zarr</li> </ul> <p>For testing and demonstration purposes, Acquire provides a few simulated cameras, as well as raw and trash output devices. To see all the devices that Acquire supports, you can run the following script:</p> <pre><code>import acquire\n\nfor device in acquire.Runtime().device_manager().devices():\n    print(device)\n</code></pre>"},{"location":"get_started/#tutorial-prerequisites","title":"Tutorial Prerequisites","text":"<p>We will be writing to and reading from the Zarr format, using the Dask library to load and inspect the data, and visualizing the data using napari.</p> <p>You can install these prerequisites with:</p> <pre><code>python -m pip install dask \"napari[all]\" zarr\n</code></pre>"},{"location":"get_started/#setup-for-acquisition","title":"Setup for Acquisition","text":"<p>We will use one of Acquire's simulated cameras to generate data and use Zarr for our output file format, which is called \"storage device\" in <code>Acquire</code>.</p> <p>To begin, instantiate <code>Runtime</code> and <code>DeviceManager</code> and list the currently supported devices.</p> <p><pre><code>import acquire\n\nruntime = acquire.Runtime()\ndm = runtime.device_manager()\n\nfor device in dm.devices():\n    print(device)\n</code></pre> The runtime is the main entry point in Acquire. Through the runtime, you configure your devices, start acquisition, check acquisition status, inspect data as it streams from your cameras, and terminate acquisition.</p> <p>Let's configure our devices now. To do this, we'll get a copy of the current runtime configuration. We can update the configuration with identifiers from the the runtime's device manager, but these devices won't instantiate until we start acquisition.</p> <p>Acquire supports up to two video streams. These streams consist of a source (i.e., a camera), optionally a filter, and a sink (an output, like a Zarr dataset or a Tiff file). Before configuring the streams, grab the current configuration of the <code>Runtime</code> object with:</p> <pre><code>config = runtime.get_configuration()\n</code></pre> <p>Video streams are configured independently. Configure the first video stream by setting properties on <code>config.video[0]</code> and the second video stream with <code>config.video[1]</code>. We'll be using simulated cameras, one generating a radial sine pattern and one generating a random pattern.</p> <pre><code>config.video[0].camera.identifier = dm.select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# how many adjacent pixels in each direction to combine by averaging; here, 1 means not to combine\nconfig.video[0].camera.settings.binning = 1\n\n# how long (in microseconds) your camera should collect light from the sample; for simulated cameras,\n# this is just a waiting period before generating the next frame\nconfig.video[0].camera.settings.exposure_time_us = 5e4  # 50 ms\n\n# the data type representing each pixel; here we choose unsigned 8-bit integer\nconfig.video[0].camera.settings.pixel_type = acquire.SampleType.U8\n\n# the shape, in pixels, of the image; width first, then height\nconfig.video[0].camera.settings.shape = (1024, 768)\n</code></pre> <pre><code>config.video[1].camera.identifier = dm.select(acquire.DeviceKind.Camera, \"simulated: uniform random\")\n\n# how many adjacent pixels in each direction to combine by averaging; here, 1 means not to combine\nconfig.video[1].camera.settings.binning = 1\n\n# how long (in microseconds) your camera should collect light from the sample; for simulated cameras,\n# this is just a waiting period before generating the next frame\nconfig.video[1].camera.settings.exposure_time_us = 1e4  # 10 ms\n\n# the data type representing each pixel; here we choose unsigned 8-bit integer\nconfig.video[1].camera.settings.pixel_type = acquire.SampleType.U8\n\n# the shape, in pixels, of the image; width first, then height\nconfig.video[1].camera.settings.shape = (1280, 720)\n</code></pre> <p>Now we'll configure each output, or sink device. For both simulated cameras, we'll be writing to Zarr, a format which supports chunked arrays.</p> <pre><code>config.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"Zarr\")\n\n# what file or directory to write the data to\nconfig.video[0].storage.settings.filename = \"output1.zarr\"\n\n# where applicable, how large should a chunk file get before opening the next chunk file\nconfig.video[0].storage.settings.chunking.max_bytes_per_chunk = 32 * 2**20  # 32 MiB chunk sizes\n</code></pre> <pre><code>config.video[1].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"Zarr\")\n\n# what file or directory to write the data to\nconfig.video[1].storage.settings.filename = \"output2.zarr\"\n\n# where applicable, how large should a chunk file get before opening the next chunk file\nconfig.video[1].storage.settings.chunking.max_bytes_per_chunk = 64 * 2**20  # 64 MiB chunk sizes\n</code></pre> <p>Finally, let's specify how many frames to generate for each camera before stopping our simulated acquisition. We also need to register our configuration with the runtime using the <code>set_configuration</code> method.</p> <p>If you want to let the runtime just keep acquiring effectively forever, you can set <code>max_frame_count</code> to <code>2**64 - 1</code>.</p> <pre><code>config.video[0].max_frame_count = 100 # collect 100 frames\nconfig.video[1].max_frame_count = 150 # collect 150 frames\n\nconfig = runtime.set_configuration(config)\n</code></pre> <p>Note</p> <p>If you run this tutorial multiple times, you can clear output from previous runs with:</p> <pre><code>import os\nimport shutil\n\nif config.video[0].storage.settings.filename in os.listdir(\".\"):\n    shutil.rmtree(config.video[0].storage.settings.filename)\n\nif config.video[1].storage.settings.filename in os.listdir(\".\"):\n    shutil.rmtree(config.video[1].storage.settings.filename)\n</code></pre>"},{"location":"get_started/#acquire-data","title":"Acquire Data","text":"<p>To start aquiring data:</p> <pre><code>runtime.start()\n</code></pre> <p>Acquisition happens in a separate thread, so at any point we can check on the status by calling the  <code>get_state</code> method.</p> <pre><code>runtime.get_state()\n</code></pre> <p>Finally, once we're done acquiring, we call <code>runtime.stop()</code>. This method will wait until you've reached the number of frames to collect specified in <code>config.video[0].max_frame_count</code> or <code>config.video[1].max_frame_count</code>, whichever is larger.</p> <pre><code>runtime.stop()\n</code></pre>"},{"location":"get_started/#visualizing-the-data-with-napari","title":"Visualizing the Data with napari","text":"<p>Let's take a look at what we've written. We'll load each Zarr dataset as a Dask array and inspect its dimensions, then we'll use napari to view it.</p> <pre><code>import dask.array as da\nimport napari\n\ndata1 = da.from_zarr(config.video[0].storage.settings.filename, component=\"0\")\ndata1\n\ndata2 = da.from_zarr(config.video[1].storage.settings.filename, component=\"0\")\n\nviewer1 = napari.view_image(data1)\n\nviewer2 = napari.view_image(data2)\n</code></pre>"},{"location":"get_started/#conclusion","title":"Conclusion","text":"<p>For more examples of using Acquire, check out our tutorials page.</p>"},{"location":"for_contributors/","title":"For contributors","text":"<p>Thank you for your interest in contributing to <code>Acquire</code>! We welcome contributions to the code base and to documentation.</p> <ul> <li><code>Acquire</code> project</li> <li><code>acquire-imaging</code></li> <li><code>Acquire</code> documentation</li> </ul>"},{"location":"for_contributors/docs_contribution_quickstart/","title":"Acquire Docs Contribution Quickstart","text":"<ol> <li>Make sure you have a fresh environment with the latest mkdocs and mkdocs-material installed. You can install them with <code>pip install -r requirements.txt</code> from the root of the repository.</li> <li>Your pages should be written as markdown files, using the basic markdown syntax or following the mkdocs or material for mkdocs syntax.</li> <li>Pages can be added to the top level menu or submenus by editing the <code>mkdocs.yml</code> file. The order of the pages in the menu is determined by the order of the pages in the <code>mkdocs.yml</code> file. Subpages can be added by creating subfolders in the <code>docs/</code> folder (see, for example, the <code>docs/tutorials/</code> folder).</li> <li>To add images, place them in the <code>docs/images/</code> folder and reference them in your markdown files using the relative path <code>../images/your_image.png</code>.</li> <li>Custom CSS configuration goes into the <code>docs/stylesheets/custom.css</code> file.</li> <li>To build the website locally, after activating your environment (either using <code>conda activate &lt;your-environment&gt;</code> or <code>source activate &lt;your-env&gt;</code>, for example), run <code>mkdocs serve</code> to start a local server. You can then view the website at the URL indicated on your console.</li> </ol>"},{"location":"tutorials/","title":"Tutorials","text":"<p>These tutorials will help you explore the main use cases of Acquire and show examples of using the API. Please submit an issue on Github if you'd like to request a tutorial, or if you are also interested in contributing to a tutorial to this documentation please visit our contribution guide.</p>"},{"location":"tutorials/chunked/","title":"Chunking Data for Zarr Storage","text":"<p>This tutorial will provide an example of writing chunked data to a Zarr storage device.</p> <p>Zarr has additional capabilities relative to the basic storage devices, namely chunking, compression, and multiscale storage. To enable chunking, set the attributes in an instance of the <code>ChunkingProperties</code> class. You can learn more about the Zarr capabilities in <code>Acquire</code> here.</p>"},{"location":"tutorials/chunked/#configure-runtime","title":"Configure <code>Runtime</code>","text":"<p>To start, we'll create a <code>Runtime</code> object and configure the streaming process, selecting <code>Zarr</code> as the storage device to enable chunking the data.</p> <p><pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# Initialize the device manager\ndm = runtime.device_manager()\n\n# Grab the current configuration\nconfig = runtime.get_configuration()\n\n# Select the radial sine simulated camera as the video source\nconfig.video[0].camera.identifier = dm.select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# Set the storage to Zarr to take advantage of chunking\nconfig.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"Zarr\")\n\n# Set the time for collecting data for a each frame\nconfig.video[0].camera.settings.exposure_time_us = 5e4  # 50 ms\n\n# size of image region of interest on the camera (x, y)\nconfig.video[0].camera.settings.shape = (1920, 1080)\n\n# specify the pixel datatype as a uint8\nconfig.video[0].camera.settings.pixel_type = acquire.SampleType.U8\n\n# Set the max frame count\nconfig.video[0].max_frame_count = 10 # collect 10 frames\n\n# Set the output file to out.zarr\nconfig.video[0].storage.settings.filename = \"out.zarr\"\n</code></pre> Below we'll configure the chunking specific settings and update all settings with the <code>set_configuration</code> method.</p> <pre><code># Chunk size may need to be optimized for each acquisition.\n# See Zarr documentation for further guidance:\n# https://zarr.readthedocs.io/en/stable/tutorial.html#chunk-optimizations\nconfig.video[0].storage.settings.chunking.max_bytes_per_chunk = 32 * 2**20 # 32 MB\n\n# x, y dimensions of each chunk\n# 1/2 of the width and height of the image, generating 4 chunks\nconfig.video[0].storage.settings.chunking.tile.width = 1920 // 2\nconfig.video[0].storage.settings.chunking.tile.height = 1080 // 2\n\n# Update the configuration with the chosen parameters\nconfig = runtime.set_configuration(config)\n</code></pre>"},{"location":"tutorials/chunked/#collect-and-inspect-the-data","title":"Collect and Inspect the Data","text":"<pre><code># collect data\nruntime.start()\nruntime.stop()\n</code></pre> <p>You can inspect the Zarr file directory to check that the data saved as expected. Alternatively, you can inspect the data programmatically with:</p> <pre><code># Utilize the zarr library to open the data\nimport zarr\n\n# create a zarr Group object\ngroup = zarr.open(config.video[0].storage.settings.filename)\n\n# check for the expected # of directories in the zarr container\nassert len(group) == 1\n\n# inspect the characteristics of the data\ngroup[\"0\"]\n</code></pre> <p>The output will be: <pre><code>&lt;zarr.core.Array '/0' (10, 1, 1080, 1920) uint8&gt;\n</code></pre> As expected, we have only 1 top level directory, corresponding to the single array in the group. We would expect more than 1 array only if we were writing multiscale data. The overall array shape is (10, 1, 1080, 1920), corresponding to 10 frames, 1 channel, and a height and width of 1080 and 1920, respectively, per frame.</p> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/compressed/","title":"Writing to Compressed Zarr Files","text":"<p>This tutorial will provide an example of writing compressed data to a Zarr file.</p> <p><code>Acquire</code> supports streaming compressed data to the <code>ZarrBlosc1*</code> storage devices. Compression is done via Blosc. Supported codecs are lz4 and zstd, available with ZarrBlosc1Lz4ByteShuffle and ZarrBlosc1ZstdByteShuffle devices, respectively. For a comparison of these codecs, please refer to the Blosc docs. You can learn more about the Zarr capabilities in <code>Acquire</code> here.</p>"},{"location":"tutorials/compressed/#configure-runtime","title":"Configure <code>Runtime</code>","text":"<p>To start, we'll create a <code>Runtime</code> object and configure the streaming process, selecting <code>ZarrBlosc1ZstdByteShuffle</code> as the storage device to enable compressing the data.</p> <pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# Initialize the device manager\ndm = runtime.device_manager()\n\n# Grab the current configuration\nconfig = runtime.get_configuration()\n\n# Select the radial sine simulated camera as the video source\nconfig.video[0].camera.identifier = dm.select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# Set the storage to ZarrBlosc1ZstdByteShuffle to avoid saving the data\nconfig.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"ZarrBlosc1ZstdByteShuffle\")\n\n# Set the time for collecting data for a each frame\nconfig.video[0].camera.settings.exposure_time_us = 5e4  # 50 ms\n\nconfig.video[0].camera.settings.shape = (1024, 768)\n\n# Set the max frame count\nconfig.video[0].max_frame_count = 100 # collect 100 frames\n\n# Set the output file to out.zarr\nconfig.video[0].storage.settings.filename = \"out.zarr\"\n\n# Update the configuration with the chosen parameters\nconfig = runtime.set_configuration(config)\n</code></pre>"},{"location":"tutorials/compressed/#inspect-acquired-data","title":"Inspect Acquired Data","text":"<p>Now that the configuration is set to utilize the <code>ZarrBlosc1ZstdByteShuffle</code> storage device, we can acquire data, which will be compressed before it is stored to <code>out.zarr</code>. Since we did not specify the size of chunks, the data will be saved as a single chunk that is the size of the image data. You may specify chunk sizes using the <code>TileShape</code> class. For example, using <code>acquire.StorageProperties.chunking.tile.width</code> to set the width of the chunks.</p> <pre><code># acquire data\nruntime.start()\nruntime.stop()\n</code></pre> <p>We'll use the Zarr Python package to read the data in <code>out.zarr</code> file.</p> <pre><code># We'll utilize the Zarr python package to read the data\nimport zarr\n\n# load from Zarr\ncompressed = zarr.open(config.video[0].storage.settings.filename)\n</code></pre> <p>We'll print some of the data properties to illustrate how the data was compressed. Since we have not enabled multiscale output, <code>out.zarr</code> will only have one top level array<code>\"0\"</code>.</p> <pre><code># All of the data is stored in the \"0\" directory since the data was stored as a single chunk.\ndata = compressed[\"0\"]\n\nprint(data.compressor.cname)\nprint(data.compressor.clevel)\nprint(data.compressor.shuffle)\n</code></pre> <p>Output:</p> <pre><code>zstd\n1\n1\n</code></pre> <p>As expected, the data was compressed using the <code>zstd</code> codec.</p> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/configure/","title":"Configure an Acquisition","text":"<p>This tutorial will provide an in-depth explanation of setting configuration properites and demonstrate the relationships between various <code>Acquire</code> classes, such as <code>CameraProperties</code> and <code>StorageProperties</code>, used in the configuration process. In this example, we'll only configure one video source.</p>"},{"location":"tutorials/configure/#initialize-runtime","title":"Initialize <code>Runtime</code>","text":"<p><code>Runtime</code> is the main entry point in <code>Acquire</code>. Through the runtime, you configure your devices, start acquisition, check acquisition status, inspect data as it streams from your cameras, and terminate acquisition. The <code>device_manager</code> method in <code>Runtime</code> creates an instance of the <code>DeviceManager</code> class. The <code>get_configuration</code> method in <code>Runtime</code> creates an instance of the <code>Properties</code> class. To configure the acquisition, we'll use those two methods to grab the configuration and to initialize a <code>DeviceManager</code> object to set the attributes of <code>Properties</code> and related classes.</p> <pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# Initialize the device manager\ndm = runtime.device_manager()\n\n# Grab the current configuration\nconfig = runtime.get_configuration()\n</code></pre>"},{"location":"tutorials/configure/#utilize-devicemanager","title":"Utilize <code>DeviceManager</code>","text":"<p><code>DeviceManager</code> contains a <code>devices</code> method which creates a list of <code>DeviceIdentifier</code> objects each representing a discovered camera or storage device. Each <code>DeviceIdentifier</code> has an attribute <code>kind</code> that is a <code>DeviceKind</code> object, which has attributes specifying whether the device is a camera or storage device, as well as <code>Signals</code> and <code>StageAxes</code> attributes. The <code>Signals</code> and <code>StageAxes</code> attributes would apply to device kinds such as stages, which are not yet supported by <code>Acquire</code>.</p> <p><code>DeviceManager</code> has 2 methods for selecting devices for the camera and storage. For more information on these methods, check out the Device Selection tutorial. We'll use the <code>select</code> method in this example to choose a specific device for the camera and storage.</p> <pre><code># Select the radial sine simulated camera as the video source\nconfig.video[0].camera.identifier = dm.select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# Set the storage to Tiff\nconfig.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"Tiff\")\n</code></pre>"},{"location":"tutorials/configure/#properties-class-explanation","title":"<code>Properties</code> Class Explanation","text":"<p>Using <code>Runtime</code>'s <code>get_configuration</code> method we created <code>config</code>, an instance of the <code>Properties</code> class. <code>Properties</code> contains only one attribute <code>video</code> which is a tuple of <code>VideoStream</code> objects since <code>Acquire</code> currently supports 2 camera streaming. To configure the first video stream, we'll index this tuple to select the first <code>VideoStream</code> object <code>config.video[0]</code>.</p> <p><code>VideoStream</code> objects have 2 attributes <code>camera</code> and <code>storage</code> which are instances of the <code>Camera</code> and <code>Storage</code> classes, respectively, and will be used to set the attributes of the selected camera device <code>simulated: radial sin</code> and storage device <code>Tiff</code>. The other attributes of <code>VideoStream</code> are integers that specify the maximum number of frames to collect and how many frames to average, if any, before storing the data. The <code>frame_average_count</code> has a default value of <code>0</code>, which disables this feature. We'll specify the max frame count, but keep the frame averaging disabled with:</p> <pre><code># Set the maximum number of frames to collect to 100\nconfig.video[0].max_frame_count = 100\n</code></pre>"},{"location":"tutorials/configure/#configure-camera","title":"Configure <code>Camera</code>","text":"<p><code>Camera</code> class objects have 2 attributes, <code>settings</code>, a <code>CameraProperties</code> object, and an optional attribute <code>identifier</code>, which is a <code>DeviceIdentifier</code> object.</p> <p><code>CameraProperties</code> has 5 attributes that are numbers and specify the exposure time and line interval in microseconds, how many pixels, if any, to bin (set to 1 by default to disable), and tuples for the image size and location on the camera chip. The other attributes are all instances of different classes. The <code>pixel_type</code> attribute is a <code>SampleType</code> object which indicates the data type of the pixel values in the image, such as Uint8. The <code>readout_direction</code> attribute is a <code>Direction</code> object specifying whether the data is read forwards or backwards from the camera. The <code>input_triggers</code> attribute is an <code>InputTriggers</code> object that details the characteristics of any input triggers in the system. The <code>output_triggers</code> attribute is an <code>OutputTriggers</code> object that details the characteristics of any output triggers in the system. All of the attributes of <code>InputTriggers</code> and <code>OutputTriggers</code> objects are instances of the <code>Trigger</code> class. The <code>Trigger</code> class is described in this tutorial.</p> <p>We'll configure some camera settings below.</p> <pre><code># Set the time for collecting data for a each frame\nconfig.video[0].camera.settings.exposure_time_us = 5e4  # 50 ms\n\n# (x, y) size of the image in pixels\nconfig.video[0].camera.settings.shape = (1024, 768)\n\n# Specify the pixel type as Uint32\nconfig.video[0].camera.settings.pixel_type = acquire.SampleType.U32\n</code></pre>"},{"location":"tutorials/configure/#configure-storage","title":"Configure <code>Storage</code>","text":"<p><code>Storage</code> objects have 2 attributes, <code>settings</code>, a <code>StorageProperties</code> object, and an optional attribute <code>identifier</code>, which is an instance of the <code>DeviceIdentifier</code> class described above.</p> <p><code>StorageProperties</code> has 2 attributes <code>external_metadata_json</code> and <code>filename</code> which are strings of the filename or filetree of the output metadata in JSON format and image data in whatever format corresponds to the selected storage device, respectively. <code>first_frame_id</code> is an integer ID that corresponds to the first frame of the current acquisition and is typically 0. <code>pixel_scale_um</code> is the camera pixel size in microns. <code>enable_multiscale</code> is a boolean used to specify if the data should be saved as an image pyramid. See the multiscale tutorial for more information. The <code>chunking</code> attribute is an instance of the <code>ChunkingProperties</code> class, used for Zarr storage. See the chunking tutorial for more information.</p> <p>We'll specify the name of the output image file below.</p> <pre><code># Set the output file to out.tiff\nconfig.video[0].storage.settings.filename = \"out.tiff\"\n</code></pre>"},{"location":"tutorials/configure/#update-configuration-settings","title":"Update Configuration Settings","text":"<p>None of the configuration settings are updated in <code>Runtime</code> until the <code>set_configuration</code> method is called. We'll be creating a new <code>Properties</code> object with the <code>set_configuration</code> method. For simplicity, we'll reuse <code>config</code> for the name of that object as well, but note that <code>new_config = runtime.set_configuration(config)</code> also works here.</p> <pre><code># Update the configuration with the chosen parameters\nconfig = runtime.set_configuration(config)\n</code></pre> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/drivers/","title":"Test Camera Drivers","text":"<p>This tutorial will cover testing that your cameras, or video sources, has been properly identified.</p> <p>Acquire supports the following cameras (currently only on Windows):</p> <ul> <li>Hamamatsu Orca Fusion BT (C15440-20UP)</li> <li>Vieworks VC-151MX-M6H00</li> <li>FLIR Blackfly USB3 (BFLY-U3-23S6M-C)</li> <li>FLIR Oryx 10GigE (ORX-10GS-51S5M-C)</li> </ul> <p>Acquire provides the following simulated cameras:</p> <ul> <li>simulated: uniform random - Produces uniform random noise for each pixel.</li> <li>simulated: radial sin - Produces an animated radial sine wave pattern.</li> <li>simulated: empty - Produces no data, leaving a blank image. This camera simulates acquiring as fast as possible.</li> </ul> <p>Acquire will only identify cameras whose drivers are present on your machine. The <code>DeviceManager</code> class manages selection of cameras and storage. We can create a <code>DeviceManager</code> object using the following:</p> <pre><code>import acquire\n\n# Instantiate a Runtime object\nruntime = acquire.Runtime()\n\n# Instantiate a DeviceManager object for the Runtime\ndm = runtime.device_manager()\n</code></pre> <p><code>DeviceManager</code> objects have <code>device</code> methods which lists the identifiers for discovered devices. You can iterate over this list to determine which cameras were discovered.</p> <pre><code>for device in dm.devices():\n    print(device)\n</code></pre> <p>The output of this code is below. All discovered devices, both cameras and storage devices, will be listed. In this tutorial, no cameras were connected to the machine, so only simulated cameras were found. Note that the storage devices also printed.</p> <pre><code>&lt;DeviceIdentifier Camera \"simulated: uniform random\"&gt;\n&lt;DeviceIdentifier Camera \"simulated: radial sin\"&gt;\n&lt;DeviceIdentifier Camera \"simulated: empty\"&gt;\n&lt;DeviceIdentifier Storage \"raw\"&gt;\n&lt;DeviceIdentifier Storage \"tiff\"&gt;\n&lt;DeviceIdentifier Storage \"trash\"&gt;\n&lt;DeviceIdentifier Storage \"tiff-json\"&gt;\n&lt;DeviceIdentifier Storage \"Zarr\"&gt;\n&lt;DeviceIdentifier Storage \"ZarrBlosc1ZstdByteShuffle\"&gt;\n&lt;DeviceIdentifier Storage \"ZarrBlosc1Lz4ByteShuffle\"&gt;\n</code></pre> <p>For cameras that weren't discovered you will see an error like the one below. These errors will not affect performance and can be ignored.</p> <pre><code>ERROR acquire.runtime 2023-10-20 19:03:17,917 runtime.rs:40 C:\\actions-runner\\_work\\acquire-driver-hdcam\\acquire-driver-hdcam\\src\\acquire-core-libs\\src\\acquire-device-hal\\device\\hal\\loader.c:114 - driver_load(): Failed to load driver at \"acquire-driver-hdcam\".\n</code></pre> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/framedata/","title":"Accessing Data during Acquisition","text":"<p>This tutorial will provide an example of accessing data from a video source during acquisition.</p>"},{"location":"tutorials/framedata/#configure-runtime","title":"Configure <code>Runtime</code>","text":"<p>To start, we'll create a <code>Runtime</code> object and configure the streaming process.</p> <pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# Initialize the device manager\ndm = runtime.device_manager()\n\n# Grab the current configuration\nconfig = runtime.get_configuration()\n\n# Select the radial sine simulated camera as the video source\nconfig.video[0].camera.identifier = dm.select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# Set the storage to trash to avoid saving the data\nconfig.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"Trash\")\n\n# Set the time for collecting data for a each frame\nconfig.video[0].camera.settings.exposure_time_us = 5e4  # 50 ms\n\nconfig.video[0].camera.settings.shape = (1024, 768)\n\n# Set the max frame count to 2**(64-1) the largest number supported by Uint64 for essentially infinite acquisition\nconfig.video[0].max_frame_count = 100 # collect 100 frames\n\n# Update the configuration with the chosen parameters\nconfig = runtime.set_configuration(config)\n</code></pre>"},{"location":"tutorials/framedata/#working-with-availabledata-objects","title":"Working with <code>AvailableData</code> objects","text":"<p>During Acquisition, the <code>AvailableData</code> object is the streaming interface, and this class has a <code>frames</code> method which iterates over the <code>VideoFrame</code> objects in <code>AvailableData</code>. Once we start acquisition, we'll utilize this iterator method to list the frames. To increase the likelihood of <code>AvailableData</code> containing data, we'll utilize the time python package to introduce a delay before we create our <code>AvailableData</code> object</p> <pre><code># package for introducing time delays\nimport time\n\n# start acquisition\nruntime.start()\n\n# time delay of 0.5 seconds\ntime.sleep(0.5)\n\n# grab the packet of data available on disk for video stream 0.\n# This is an AvailableData object.\navailable_data = runtime.get_available_data(0)\n</code></pre> <p>There may not be data available, in which case our variable <code>available_data</code> would be <code>None</code>. To avoid errors associated with this circumstance, we'll only grab data if <code>available_data</code> is not <code>None</code>.</p> <p>Once <code>get_available_data()</code> is called the <code>AvailableData</code> object will be locked into memory, so the circular buffer that stores the available data will overflow if <code>AvailableData</code> isn\u2019t released, so we'll delete the object with <code>del available_data</code> if there is no data available.</p> <p><pre><code># NoneType if there is no available data.\n# We can only grab frames if data is available.\nif available_data is not None:\n\n\n    # frames is an iterator over available_data\n    # we'll use this iterator to make a list of the frames\n    video_frames = list(available_data.frames())\n\nelse:\n    # delete the available_data variable\n    # if there is no data in the packet to free up RAM\n    del available_data\n</code></pre> <code>video_frames</code> is a list with each element being an instance of the <code>VideoFrame</code> class. <code>VideoFrame</code> has a <code>data</code> method which provides the frame as an <code>NDArray</code>. The shape of this NDArray corresponds to the image dimensions used internally by Acquire with (planes, height, width, channels). Since we have a single channel, both the first and the last dimensions will be 1. The interior dimensions are height and width, respectively.</p> <p><pre><code># grab the first VideoStream object in frames and convert it to an NDArray\nfirst_frame = video_frames[0].data()\n\nprint(first_frame.shape)\n</code></pre> Output: <pre><code>(1, 768, 1024, 1)\n</code></pre></p> <p>We can use the <code>numpy.squeeze</code> method to grab the desired NDArray image data from <code>first_frame</code> since the other dimensions are 1. This is equivalent to <code>image = first_frame[0][:, :, 0]</code>.</p> <p><pre><code>image = first_frame.squeeze()\n\n\nprint(image.shape)\n</code></pre> Output: <pre><code>(768, 1024)\n</code></pre> Finally, delete the <code>available_data</code> to unlock the region in the circular buffer.</p> <pre><code># delete the available_data to free up disk space\ndel available_data\n\n# stop runtime\nruntime.stop()\n</code></pre> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/livestream/","title":"Livestream to napari","text":"<p>The below script can be used to livestream data to the napari viewer. You may also utilize the <code>Acquire</code> napari plugin, which is provided in the package upon install. You can access the plugin in the napari plugins menu once <code>Acquire</code> is installed. You can review the plugin code here. You may also stream using other packages such at <code>matplotlib</code>.</p> <pre><code>\"\"\"\nThis script will livestream data to the [napari viewer](https://napari.org/stable/). You may also utilize the `Acquire` napari plugin, which is provided in the `acquire-imaging` package on PyPI upon install. You can access the plugin in the napari plugins menu once `Acquire` is installed. You can review the [plugin code here](https://github.com/acquire-project/acquire-python/blob/main/python/acquire/__init__.py).\n\"\"\"\n\nimport acquire\nruntime = acquire.Runtime()\n\n# Initialize the device manager\ndm = runtime.device_manager()\n\n# Grab the current configuration\nconfig = runtime.get_configuration()\n\n# Select the uniform random camera as the video source\nconfig.video[0].camera.identifier = dm.select(acquire.DeviceKind.Camera, \".*random.*\")\n\n# Set the storage to trash to avoid saving the data\nconfig.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"Trash\")\n\n# Set the time for collecting data for a each frame\nconfig.video[0].camera.settings.exposure_time_us = 5e4  # 500 ms\n\nconfig.video[0].camera.settings.shape = (300, 200)\n\n# Set the max frame count to 100 frames\nconfig.video[0].max_frame_count = 100\n\n# Update the configuration with the chosen parameters\nconfig = runtime.set_configuration(config)\n\n# import napari and open a viewer to stream the data\nimport napari\nviewer = napari.Viewer()\n\nimport time\nfrom napari.qt.threading import thread_worker\n\ndef update_layer(args) -&gt; None:\n    (new_image, stream_id) = args\n    print(f\"update layer: {new_image.shape=}, {stream_id=}\")\n    layer_key = f\"Video {stream_id}\"\n    try:\n        layer = viewer.layers[layer_key]\n        layer._slice.image._view = new_image\n        layer.data = new_image\n        # you can use the private api with layer.events.set_data() to speed up by 1-2 ms/frame\n\n    except KeyError:\n        viewer.add_image(new_image, name=layer_key)\n\n@thread_worker(connect={\"yielded\": update_layer})\ndef do_acquisition():\n    time.sleep(5)\n    runtime.start()\n\n    nframes = [0, 0]\n    stream_id = 0\n\n    def is_not_done() -&gt; bool:\n        return (nframes[0] &lt; config.video[0].max_frame_count) or (\n                nframes[1] &lt; config.video[1].max_frame_count\n                )\n\n    def next_frame(): #-&gt; Optional[npt.NDArray[Any]]:\n        \"\"\"Get the next frame from the current stream.\"\"\"\n        if nframes[stream_id] &lt; config.video[stream_id].max_frame_count:\n            if packet := runtime.get_available_data(stream_id):\n                n = packet.get_frame_count()\n                nframes[stream_id] += n\n                f = next(packet.frames())\n                return f.data().squeeze().copy()\n        return None\n\n    stream = 1\n    # loop to continue to update the data in napari while acquisition is running\n    while is_not_done():\n        if (frame := next_frame()) is not None:\n            yield frame, stream_id\n        time.sleep(0.1)\n\ndo_acquisition()\n\nnapari.run()\n</code></pre> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/multiscale/","title":"Multiscale Data Acqusition","text":"<p>This tutorial will provide an example of writing multiscale data to a Zarr file.</p> <p>Zarr has additional capabilities relative to Acquire's basic storage devices, namely chunking, compression, and multiscale storage. To enable chunking and multiscale storage, set those attributes in instances of the <code>ChunkingProperties</code> and <code>StorageProperties</code> classes, respectively. You can learn more about the Zarr capabilities in <code>Acquire</code> here.</p>"},{"location":"tutorials/multiscale/#configure-runtime","title":"Configure <code>Runtime</code>","text":"<p>To start, we'll create a <code>Runtime</code> object and begin to configure the streaming process, selecting <code>Zarr</code> as the storage device so that writing multiscale data is possible.</p> <pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# Initialize the device manager\ndm = runtime.device_manager()\n\n# Grab the current configuration\nconfig = runtime.get_configuration()\n\n# Select the radial sine simulated camera as the video source\nconfig.video[0].camera.identifier = dm.select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# Set the storage to Zarr to have the option to save multiscale data\nconfig.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"Zarr\")\n\n# Set the time for collecting data for a each frame\nconfig.video[0].camera.settings.exposure_time_us = 5e4  # 50 ms\n\n# Set the size of image region of interest on the camera (x, y)\nconfig.video[0].camera.settings.shape = (1920, 1080)\n\n# Set the max frame count\nconfig.video[0].max_frame_count = 5 # collect 5 frames\n\n# Set the image data type as a Uint8\nconfig.video[0].camera.settings.pixel_type = acquire.SampleType.U8\n\n# Set the scale of the pixels\nconfig.video[0].storage.settings.pixel_scale_um = (1, 1) # 1 micron by 1 micron\n\n# Set the output file to out.zarr\nconfig.video[0].storage.settings.filename = \"out.zarr\"\n</code></pre> <p>To complete configuration, we'll configure the multiscale specific settings and update all settings with the <code>set_configuration</code> method.</p> <pre><code># Chunk size may need to be optimized for each acquisition.\n# See Zarr documentation for further guidance:\n# https://zarr.readthedocs.io/en/stable/tutorial.html#chunk-optimizations\nconfig.video[0].storage.settings.chunking.max_bytes_per_chunk = 16 * 2**20 # 16 MB\n\n# x, y dimensions of each chunk\n# 1/3 of the width and height of the image, generating 9 chunks\nconfig.video[0].storage.settings.chunking.tile.width = (config.video[0].camera.settings.shape[0] // 3)\nconfig.video[0].storage.settings.chunking.tile.height = (config.video[0].camera.settings.shape[1] // 3)\n\n# turn on multiscale mode\nconfig.video[0].storage.settings.enable_multiscale = True\n\n# Update the configuration with the chosen parameters\nconfig = runtime.set_configuration(config)\n</code></pre>"},{"location":"tutorials/multiscale/#collect-and-inspect-the-data","title":"Collect and Inspect the Data","text":"<pre><code># collect data\nruntime.start()\nruntime.stop()\n</code></pre> <p>You can inspect the Zarr file directory to check that the data saved as expected. This zarr file should have multiple subdirectories, one for each resolution in the multiscale data. Alternatively, you can inspect the data programmatically with:</p> <p><pre><code># Utilize the zarr python library to read the data\nimport zarr\n\n# Open the data to create a zarr Group\ngroup = zarr.open(\"out.zarr\")\n</code></pre> With multiscale mode enabled, an image pyramid will be formed by rescaling the data by a factor of 2 progressively until the rescaled image is smaller than the specified zarr chunk size in both dimensions. In this example, the original image dimensions are (1920, 1080), and we chunked the data using tiles 1/3 of the size of the image, namely (640, 360). To illustrate this point, we'll inspect the sizes of the various levels in the multiscale data and compare it to our specified chunk size.</p> <pre><code>group[\"0\"], group[\"1\"], group[\"2\"]\n</code></pre> <p>The output will be:</p> <pre><code>(&lt;zarr.core.Array '/0' (10, 1, 1080, 1920) uint8&gt;,\n &lt;zarr.core.Array '/1' (5, 1, 540, 960) uint8&gt;,\n &lt;zarr.core.Array '/2' (2, 1, 270, 480) uint8&gt;)\n</code></pre> <p>Here, the <code>\"0\"</code> directory contains the full-resolution array of frames of size 1920 x 1080, with a single channel, saving all 10 frames. The <code>\"1\"</code> directory contains the first rescaled array of frames of size 960 x 540, averaging every two frames, taking the frame count from 10 to 5. The <code>\"2\"</code> directory contains a further rescaled array of frames of size 480 x 270, averaging every four frames, taking the frame count from 10 to 2. Notice that both the frame width and frame height are now smaller than the chunk width and chunk height of 640 and 360, respectively, so this should be the last array in the group.</p> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/props_json/","title":"Properties from a JSON file","text":"<p>This tutorial will provide an example of saving and subsequently loading a <code>Properties</code> object from a JSON file.</p>"},{"location":"tutorials/props_json/#initialize-runtime","title":"Initialize Runtime","text":"<p>To start, we'll import <code>Acquire</code> and create a <code>Runtime</code> object, which coordinates the streaming process.</p> <pre><code>import acquire\nruntime = acquire.Runtime()\n</code></pre>"},{"location":"tutorials/props_json/#configure-camera","title":"Configure Camera","text":"<p>All camera settings are captured by an instance of the <code>Properties</code> class, which will be associated with a given camera acquisition.</p> <pre><code># Instantiate a Properties object for the Runtime\nconfig = runtime.get_configuration()\n</code></pre> <p>You can update any of the settings in this instance of <code>Properties</code>. To save any updated settings, use the <code>set_configuration</code> method.  For this tutorial, we'll simply specify a camera, and then save these new settings. Note that more settings must be provided before this <code>Properties</code> object could be used for an acquistion. Check out this tutorial for more information on configuring an acquisition.</p> <pre><code># set the radial sine simulated camera as the first video stream\nconfig.video[0].camera.identifier = runtime.device_manager().select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# save the updated settings\nconfig = runtime.set_configuration(config)\n</code></pre>"},{"location":"tutorials/props_json/#save-properties-to-a-json-file","title":"Save Properties to a JSON file","text":"<p>We'll utilize the json library to write our properties to a JSON file to save for subsequent acquisition.</p> <pre><code>import json\n\n# cast the properties to a dictionary\nconfig = config.dict()\n\n# convert the dictionary to json with \"human-readable\" formatting\nconfig = json.dumps(config, indent=4, sort_keys=True)\n\n# save the properties to file \"sample_props.json\" in the current directory\nwith open(\"sample_props.json\", \"w\") as outfile:\n    outfile.write(config)\n</code></pre>"},{"location":"tutorials/props_json/#example-json-file","title":"Example JSON file","text":"<p>The resulting sample_props.json file is below:</p> <pre><code>{\n  \"video\": [\n    {\n      \"camera\": {\n        \"identifier\": {\n          \"id\": [\n            0,\n            1\n          ],\n          \"kind\": \"Camera\",\n          \"name\": \"simulated: radial sin\"\n        },\n        \"settings\": {\n          \"binning\": 1,\n          \"exposure_time_us\": 0.0,\n          \"input_triggers\": {\n            \"acquisition_start\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"exposure\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"frame_start\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            }\n          },\n          \"line_interval_us\": 0.0,\n          \"offset\": [\n            0,\n            0\n          ],\n          \"output_triggers\": {\n            \"exposure\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"frame_start\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"trigger_wait\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            }\n          },\n          \"pixel_type\": \"U16\",\n          \"readout_direction\": \"Forward\",\n          \"shape\": [\n            1,\n            1\n          ]\n        }\n      },\n      \"frame_average_count\": 0,\n      \"max_frame_count\": 18446744073709551615,\n      \"storage\": {\n        \"identifier\": {\n          \"id\": [\n            0,\n            0\n          ],\n          \"kind\": \"NONE\",\n          \"name\": \"\"\n        },\n        \"settings\": {\n          \"chunking\": {\n            \"max_bytes_per_chunk\": 16777216,\n            \"tile\": {\n              \"height\": 0,\n              \"planes\": 0,\n              \"width\": 0\n            }\n          },\n          \"enable_multiscale\": false,\n          \"external_metadata_json\": \"\",\n          \"filename\": \"\",\n          \"first_frame_id\": 0,\n          \"pixel_scale_um\": [\n            0.0,\n            0.0\n          ]\n        },\n        \"write_delay_ms\": 0.0\n      }\n    },\n    {\n      \"camera\": {\n        \"identifier\": {\n          \"id\": [\n            0,\n            0\n          ],\n          \"kind\": \"NONE\",\n          \"name\": \"\"\n        },\n        \"settings\": {\n          \"binning\": 1,\n          \"exposure_time_us\": 0.0,\n          \"input_triggers\": {\n            \"acquisition_start\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"exposure\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"frame_start\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            }\n          },\n          \"line_interval_us\": 0.0,\n          \"offset\": [\n            0,\n            0\n          ],\n          \"output_triggers\": {\n            \"exposure\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"frame_start\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            },\n            \"trigger_wait\": {\n              \"edge\": \"Rising\",\n              \"enable\": false,\n              \"kind\": \"Input\",\n              \"line\": 0\n            }\n          },\n          \"pixel_type\": \"U16\",\n          \"readout_direction\": \"Forward\",\n          \"shape\": [\n            0,\n            0\n          ]\n        }\n      },\n      \"frame_average_count\": 0,\n      \"max_frame_count\": 18446744073709551615,\n      \"storage\": {\n        \"identifier\": {\n          \"id\": [\n            0,\n            0\n          ],\n          \"kind\": \"NONE\",\n          \"name\": \"\"\n        },\n        \"settings\": {\n          \"chunking\": {\n            \"max_bytes_per_chunk\": 16777216,\n            \"tile\": {\n              \"height\": 0,\n              \"planes\": 0,\n              \"width\": 0\n            }\n          },\n          \"enable_multiscale\": false,\n          \"external_metadata_json\": \"\",\n          \"filename\": \"\",\n          \"first_frame_id\": 0,\n          \"pixel_scale_um\": [\n            0.0,\n            0.0\n          ]\n        },\n        \"write_delay_ms\": 0.0\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"tutorials/props_json/#load-properties-from-a-json-file","title":"Load Properties from a JSON file","text":"<p>You can load the settings in the JSON file to a <code>Properties</code> object and set this configuration for your <code>Runtime</code> as shown below:</p> <pre><code>import acquire\nimport json\n\n# create a Runtime object\nruntime = acquire.Runtime()\n\n# Instantiate a `Properties` object from the settings in sample_props.json\nconfig = acquire.Properties(**json.load(open('sample_props.json')))\n\n# save the properties for this instance of Runtime\nconfig = runtime.set_configuration(config)\n</code></pre> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/select/","title":"Device Selection","text":"<p>This tutorial illustrates the difference between the <code>select</code> and <code>select_one_of</code> methods in the <code>DeviceManager</code> class. <code>select</code> chooses the first discovered device of a specific kind, camera or storage device. You can also, optionally, select a specific device by passing the device name as a string to <code>select</code>. Whereas, <code>select_one_of</code> requires that you specify both the kind of device to select and a list of possible device names. <code>select_one_of</code> will iterate through the list and select the first device in the list of names that is discovered on your machine.</p> <p>To start, instantiate <code>Runtime</code> and <code>DeviceManager</code> objects and subsequently print the discovered devices.</p> <pre><code>import acquire\n\n# Instantiate a Runtime object\nruntime = acquire.Runtime()\n\n# Instantiate a DeviceManager object for the Runtime\nmanager = runtime.device_manager()\n\n# List devices discovered by DeviceManager\nfor device in manager.devices():\n    print(device)\n</code></pre> <p>Output of the above code is below:</p> <pre><code>&lt;DeviceIdentifier Camera \"simulated: uniform random\"&gt;\n&lt;DeviceIdentifier Camera \"simulated: radial sin\"&gt;\n&lt;DeviceIdentifier Camera \"simulated: empty\"&gt;\n&lt;DeviceIdentifier Storage \"raw\"&gt;\n&lt;DeviceIdentifier Storage \"tiff\"&gt;\n&lt;DeviceIdentifier Storage \"trash\"&gt;\n&lt;DeviceIdentifier Storage \"tiff-json\"&gt;\n&lt;DeviceIdentifier Storage \"Zarr\"&gt;\n&lt;DeviceIdentifier Storage \"ZarrBlosc1ZstdByteShuffle\"&gt;\n&lt;DeviceIdentifier Storage \"ZarrBlosc1Lz4ByteShuffle\"&gt;\n</code></pre> <p>All identified devices will be listed, and in the case of this tutorial, no cameras were connected to the machine, so only simulated cameras were found. Note that discovered storage devices will also print.</p> <p>The order of those printed devices matters. Below are two examples of how the <code>select</code> method works. In the first, without a specific device name provided, <code>select</code> will choose the first device of the specified kind in the list of discovered devices. In the second example, a specific device name is provided, so <code>select</code> will grab that device if it was discovered by <code>Runtime</code>.</p> <p><pre><code># specify that the device should be a camera and not a storage device\nkind = acquire.DeviceKind.Camera\n\n# 1st example: select the first camera in the list of discovered devices\nselected = manager.select(kind)\n\n# 2nd example: select a specific camera\nspecific = manager.select(kind, \"simulated: empty\")\n\n# print the 2 devices\nprint(selected)\nprint(specific)\n</code></pre> The output of the code is below: <pre><code>&lt;DeviceIdentifier Camera \"simulated: uniform random\"&gt;\n&lt;DeviceIdentifier Camera \"simulated: empty\"&gt;\n</code></pre></p> <p>The <code>select_one_of</code> method allows more flexibility since you provide a list of names of acceptable devices for it to iterate through until a discovered device is located.</p> <p><pre><code># specify that the device should be a camera and not a storage device\nkind = acquire.DeviceKind.Camera\n\nselected = manager.select_one_of(kind, [\"Hamamatsu_DCAMSDK4_v22126552\",\n    \"simulated: radial sin\", \"simulated: empty\"])\n\n# print which camera was selected\nprint(selected)\n</code></pre> The output of the code is below. The Hamamatsu camera was not discovered by <code>Runtime</code>, so <code>select_one_of</code> iterates until it finds a device discovered by <code>Runtime</code>. In this case, the next item in the list is a simulated camera that was discovered by <code>Runtime</code>. <pre><code>&lt;DeviceIdentifier Camera \"simulated: radial sin\"&gt;\n</code></pre></p> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/setup/","title":"Utilizing the Setup Method","text":"<p>This tutorial will provide an example of utilizing the setup method to configure <code>Runtime</code> and specify some basic properties.</p>"},{"location":"tutorials/setup/#setup-function-definition","title":"Setup Function Definition","text":"<pre><code>def setup(\n    runtime: Runtime,\n    camera: Union[str, List[str]],\n    storage: Union[str, List[str]],\n    output_filename: Optional[str],\n) -&gt; Properties\n</code></pre> <p>The <code>setup</code> function can be used as a shorthand to simplify the <code>Runtime</code> configuration process. <code>setup</code> takes a <code>Runtime</code> object and strings of the camera and storage device names and returns a <code>Properties</code> object. You may also optionally specify the filename for writing the data.</p>"},{"location":"tutorials/setup/#example","title":"Example","text":"<p><pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# use setup to get configuration and set the camera, storage, and filename\nconfig = acquire.setup(runtime, \"simulated: radial sin\", \"Zarr\", \"out.zarr\")\n</code></pre> You can subsequently use <code>config</code> to specify additional settings and set those configurations before beginning acquisition.</p> <p>Without using setup, the process would take a few additional lines of codes. The below code is equivalent to the example above.</p> <pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# Grab the current configuration\nconfig = runtime.get_configuration()\n\n# Select the radial sine simulated camera as the video source\nconfig.video[0].camera.identifier = runtime.device_manager().select(acquire.DeviceKind.Camera, \"simulated: radial sin\")\n\n# Set the storage to Zarr to have the option to save multiscale data\nconfig.video[0].storage.identifier = runtime.device_manager().select(acquire.DeviceKind.Storage, \"Zarr\")\n\n# Set the output file to out.zarr\nconfig.video[0].storage.settings.filename = \"out.zarr\"\n</code></pre> <p>In either case, we can update the configuration settings using:</p> <pre><code>config = runtime.set_configuration(config)\n</code></pre> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/start_stop/","title":"Multiple Acquisitions","text":"<p>This tutorial will provide an example of starting, stopping, and restarting acquisition, or streaming from a video source.</p>"},{"location":"tutorials/start_stop/#configure-streaming","title":"Configure Streaming","text":"<p>To start, we'll create a <code>Runtime</code> object and configure the streaming process. To do this, we'll utilize the setup method. More information on that method is detailed in this tutorial.</p> <pre><code>import acquire\n\n# Initialize a Runtime object\nruntime = acquire.Runtime()\n\n# Grab current configuration and\n# Choose Video Source and Storage Device\nconfig = acquire.setup(runtime, \"simulated: radial sin\", \"Tiff\")\n\n# Specify settings\nconfig.video[0].storage.settings.filename == \"out.tif\"\nconfig.video[0].camera.settings.shape = (192, 108)\nconfig.video[0].camera.settings.exposure_time_us = 10e4\nconfig.video[0].max_frame_count = 10\n\n# Update the configuration with the chosen parameters\nconfig = runtime.set_configuration(config)\n</code></pre>"},{"location":"tutorials/start_stop/#start-stop-and-restart-acquisition","title":"Start, Stop, and Restart Acquisition","text":"<p>During Acquisition, the <code>AvailableData</code> object is the streaming interface. Upon shutdown, <code>Runtime</code> deletes all of the objects created during acquisition to free up resources, and you must stop acquisition by calling <code>runtime.stop()</code> to shutdown after the max frames is collected or <code>runtime.abort()</code> to shutdown immediately) between acquisitions. Otherwise, an exception will be raised.</p> <p>To understand how acquisition works, we'll start, stop, and repeat acquisition and print the <code>DeviceState</code>, which can be <code>Armed</code>, <code>AwaitingConfiguration</code>, <code>Closed</code>, or <code>Running</code>, as well as print the <code>AvailableData</code> object throughout the process.</p> <p>If acquisition has ended, all of the objects are deleted, including <code>AvailableData</code> objects, so those will be <code>None</code> when not acquiring data. In addition, if enough time hasn't elapsed since acquisition started, <code>AvailableData</code> will also be <code>None</code>. We'll utilize the <code>time</code> python package to introduce time delays to account for these facts.</p> <pre><code># package used to introduce time delays\nimport time\n\n# start acquisition\nruntime.start()\n\nprint(runtime.get_state())\nprint(runtime.get_available_data(0))\n\n# wait 0.5 seconds to allow time for data to be acquired\ntime.sleep(0.5)\n\nprint(runtime.get_state())\nprint(runtime.get_available_data(0))\n\n# stop acquisition\nruntime.stop()\n\nprint(runtime.get_state())\nprint(runtime.get_available_data(0))\n\n# start acquisition\nruntime.start()\n\n# time delay of 5 sec &gt; 1 sec acquisition time\ntime.sleep(5)\n\nprint(runtime.get_state())\nprint(runtime.get_available_data(0))\n\n# stop acquisition\nruntime.stop()\n</code></pre> <p>The output will be:</p> <p><pre><code>DeviceState.Running\nNone\nDeviceState.Running\n&lt;builtins.AvailableData object at 0x00000218D685E5B0&gt;\nDeviceState.Armed\nNone\nDeviceState.Armed\n&lt;builtins.AvailableData object at 0x00000218D685E3D0&gt;\n</code></pre> 1. The first time we print is immediately after starting acquisition, so no time has elapsed for data collection as compared to the camera exposure time, so while the camera is running, <code>Running</code>, there is no data available.</p> <ol> <li> <p>The next print happens after waiting 0.5 seconds, so acquisition is still runnning and now there is acquired data available.</p> </li> <li> <p>The subsequent print is following calling <code>runtime.stop()</code> which waits until the specified max number of frames is collected and then terminates acquisition. Thus, the device is no longer running and there is no available data, since all objects were deleted by calling the <code>stop</code> method. The device is in an <code>Armed</code> state ready for the next acquisition.</p> </li> <li> <p>The final print occurs after waiting 5 seconds following the start of acquisition. This waiting period is longer than the 1 second acqusition time (0.1 seconds/frame and 10 frames), so the device is no longer collecting data. However, <code>runtime.stop()</code> hasn't been called, so the <code>AvailableData</code> object has not yet been deleted.</p> </li> </ol> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/storage/","title":"Storage Device Selection","text":"<p>This tutorial describes the storage device options in <code>Acquire</code>.</p>"},{"location":"tutorials/storage/#description-of-storage-devices","title":"Description of Storage Devices","text":"<p>To start, we'll create a <code>Runtime</code> object and print the storage device options.</p> <p><pre><code>import acquire\n\n# Instantiate a Runtime object\nruntime = acquire.Runtime()\n\n# Instantiate a DeviceManager object for the Runtime\ndm = runtime.device_manager()\n\n# Print devices in DeviceManager of kind Storage\nfor device in dm.devices():\n    if device.kind == acquire.DeviceKind.Storage:\n        print(device)\n</code></pre> The output of that script will be:</p> <pre><code>&lt;DeviceIdentifier Storage \"raw\"&gt;\n&lt;DeviceIdentifier Storage \"tiff\"&gt;\n&lt;DeviceIdentifier Storage \"trash\"&gt;\n&lt;DeviceIdentifier Storage \"tiff-json\"&gt;\n&lt;DeviceIdentifier Storage \"Zarr\"&gt;\n&lt;DeviceIdentifier Storage \"ZarrBlosc1ZstdByteShuffle\"&gt;\n&lt;DeviceIdentifier Storage \"ZarrBlosc1Lz4ByteShuffle\"&gt;\n</code></pre> <p><code>Acquire</code> supports streaming data to bigtiff and Zarr V2.</p> <p>Zarr has additional capabilities relative to the basic storage devices, namely chunking, compression, and multiscale storage. You can learn more about the Zarr capabilities in <code>Acquire</code> here.</p> <ul> <li> <p>raw - Streams to a raw binary file.</p> </li> <li> <p>tiff - Streams to a bigtiff file. Metadata is stored in the <code>ImageDescription</code> tag for each frame as a <code>JSON</code> string.</p> </li> <li> <p>trash - Writes nothing. Discards incoming data. Useful for live streaming applications.</p> </li> <li> <p>tiff-json - Stores the video stream in a bigtiff, and stores metadata in a <code>JSON</code> file. Both are located in a folder identified by the <code>filename</code> property.</p> </li> <li> <p>Zarr - Streams data to a Zarr V2 file with associated metadata.</p> </li> <li> <p>ZarrBlosc1ZstdByteShuffle - Streams compressed data (zstd codec) to a Zarr V2 file with associated metadata.</p> </li> <li> <p>ZarrBlosc1Lz4ByteShuffle - Streams compressed data (lz4 codec) to a Zarr V2 file with associated metadata.</p> </li> </ul>"},{"location":"tutorials/storage/#configure-the-storage-device","title":"Configure the Storage Device","text":"<p>In the example below, the the <code>tiff</code> storage device is selected, and the data from one video source will be streamed to a file <code>out.tif</code>.</p> <pre><code># get the current configuration\nconfig = runtime.get_configuration()\n\n# Select the tiff storage device\nconfig.video[0].storage.identifier = dm.select(acquire.DeviceKind.Storage, \"tiff\")\n\n# Set the data filename to out.tif in your current directory (provide the whole filetree to save to a different directory)\nconfig.video[0].storage.settings.filename = \"out.tif\"\n</code></pre> <p>Before proceeding, complete the <code>Camera</code> setup and call <code>set_configuration</code> to save those new configuration settings.</p> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/trig_json/","title":"Triggers from a JSON file","text":"<p>This tutorial will provide an example of saving and subsequently loading a <code>Trigger</code> object from a JSON file.</p>"},{"location":"tutorials/trig_json/#initialize-runtime","title":"Initialize Runtime","text":"<p>To start, we'll import <code>Acquire</code> and create a <code>Runtime</code> object, which coordinates the streaming process.</p> <pre><code>import acquire\nruntime = acquire.Runtime()\n</code></pre>"},{"location":"tutorials/trig_json/#create-a-trigger-object","title":"Create a Trigger Object","text":"<p><code>Trigger</code> objects have 4 attributes: edge, enable, line, and kind. In this example, will only adjust the edge attribute.</p> <pre><code># Instantiate a Trigger object\ntrig = acquire.Trigger()\n\n# change the edge attribute from the default Rising to Falling\ntrig.edge = acquire.TriggerEdge.Falling\n</code></pre>"},{"location":"tutorials/trig_json/#save-properties-to-a-json-file","title":"Save Properties to a JSON file","text":"<p>We'll utilize the json library to write our <code>Trigger</code> to a JSON file to save for subsequent acquisition.</p> <pre><code>import json\n\n# cast the properties to a dictionary\ntrig = trig.dict()\n\n# convert the dictionary to json with \"human-readable\" formatting\ntrig = json.dumps(trig, indent=4, sort_keys=True)\n\n# save the trigger to file \"sample_trig.json\" in the current directory\nwith open(\"sample_trig.json\", \"w\") as outfile:\n    outfile.write(trig)\n</code></pre>"},{"location":"tutorials/trig_json/#example-json-file","title":"Example JSON file","text":"<p>The resulting sample_trig.json file is below:</p> <pre><code>{\n  \"edge\": \"Falling\",\n  \"enable\": false,\n  \"kind\": \"Input\",\n  \"line\": 0\n}\n</code></pre>"},{"location":"tutorials/trig_json/#load-properties-from-a-json-file","title":"Load Properties from a JSON file","text":"<p>You can load the trigger attributes in the JSON file to a <code>Trigger</code> object as shown below:</p> <pre><code># Instantiate a `Trigger` object from the settings in sample_trig.json\ntrig = acquire.Trigger(**json.load(open('sample_trig.json')))\n</code></pre> <p>Download this tutorial as a Python script</p>"},{"location":"tutorials/trigger/","title":"Finite Triggered Acquisition","text":"<p>Acquire (<code>acquire-imaging</code> on PyPI) is a Python package providing a multi-camera video streaming library focused on performant microscopy, with support for up to two simultaneous, independent, video streams.</p> <p>This tutorial shows an example of setting up triggered acquisition of a finite number of frames with one of Acquire's supported devices and saving the data to a Zarr file.</p>"},{"location":"tutorials/trigger/#initialize-acquisition","title":"Initialize Acquisition","text":"<p>To start, we'll import <code>Acquire</code> and create an acquisition <code>Runtime</code> object, which initializes the driver adaptors needed for the supported cameras.</p> <pre><code>import acquire\nruntime = acquire.Runtime()\n</code></pre>"},{"location":"tutorials/trigger/#configure-camera","title":"Configure Camera","text":"<p>All camera settings can be captured by an instance of the <code>Properties</code> class, which will be associated with a given camera acquisition. The settings can be stored in a dictionary (e.g: <code>Properties.dict()</code>). These settings can be saved to a JSON file to be subsequently loaded, (e.g. <code>Properties(**json.load(open('acquire.json')))</code>), using the json library. Check out this tutorial for a more detailed example, but in brief, you would use something like:</p> <pre><code>config = runtime.get_configuration()\n\nimport json\nwith open(\"/path/to/acquire.json\", \"w\") as f:\n    json.dump(config.dict(), f)\n</code></pre> <p>The current configuration settings can be checked and assigned to an instance of the <code>Properties</code> class with:</p> <pre><code>config = runtime.get_configuration()\n</code></pre> <p>Since <code>Acquire</code> supports 2 video streams, each camera, or source, must be configured separately. In this example, we will only use 1 source for the acquisition, so we will only need to configure <code>config.video[0]</code>. To set the first video stream to Hamamatsu Orca Fusion BT (C15440-20UP), you can use the following with a regular expression to grab the Hamamatsu camera:</p> <pre><code>config.video[0].camera.identifier = runtime.device_manager().select(acquire.DeviceKind.Camera, 'Hamamatsu C15440.*')\n</code></pre> <p>Next we'll choose the settings for the Hamamatsu camera. The <code>CameraProperties</code> class describes the available settings, which include exposure time (in microseconds), binning, pixel data type (e.g. u16), and how many frames to acquire.</p> <p>Every property can be set using the following syntax, but in this example, we will only change a few of the available settings. Check out this tutorial for an explanation of camera properties.</p> <pre><code>config.video[0].camera.settings.binning = 1 # no pixels will be combined\nconfig.video[0].camera.settings.shape = (1700, 512) # shape of the image to be acquired in pixels\nconfig.video[0].camera.settings.offset = (302, 896) # centers the image region of interest on the camera sensor\nconfig.video[0].camera.settings.pixel_type = acquire.SampleType.U16 # sets the pixel data type to a 16-bit unsigned integer\nconfig.video[0].max_frame_count = 10 # finite acquisition of 10 frames. Use 0 for infinite acquisition.\n</code></pre> <p>Triggers can also be set in the <code>CameraProperties</code> object. The parameters can be stored in a dictionary (e.g: <code>Trigger.dict()</code>). You can construct a <code>Trigger</code> from a JSON file (e.g.  <code>acquire.Trigger(**json.loads(open('trigger.json')))</code> ), using the json library. Check out this tutorial for a more detailed example, but in brief, you would use something like:</p> <pre><code>trig = acquire.Trigger()\n\nimport json\nwith open(\"/path/to/trigger.json\", \"w\") as f:\n    json.dump(trig.dict(), f)\n</code></pre> <p>In this example, we'll only utilize output triggers. By default, the camera's internal triggering is used, but you may explicitly disable external input triggers using:</p> <pre><code>config.video[0].camera.settings.input_triggers = acquire.InputTriggers() # default: disabled\n</code></pre> <p>Output triggers can be set to begin exposure, start a new frame, or wait before acquiring. We can enable an exposure trigger to start on the rising edge with:</p> <pre><code>config.video[0].camera.settings.output_triggers.exposure = acquire.Trigger(\n    enable=True, line=1, edge=\"Rising\"\n)\n</code></pre>"},{"location":"tutorials/trigger/#select-storage","title":"Select Storage","text":"<p><code>Storage</code> objects have identifiers which specify the file type (e.g. Zarr or tiff) and settings described by an instance of the <code>StorageProperties</code> class. We can set the file type to Zarr and set the file name to \"out\" with:</p> <pre><code>config.video[0].storage.identifier = runtime.device_manager().select(acquire.DeviceKind.Storage,'zarr')\nconfig.video[0].storage.settings.filename=\"out.zarr\"\n</code></pre>"},{"location":"tutorials/trigger/#save-configuration","title":"Save configuration","text":"<p>None of these settings will be updated in the <code>Properties</code> object until you call the <code>set_configuration</code> method. This method updates what the current configuration settings are on the device.</p> <p>We'll set the configuration with:</p> <pre><code>config = runtime.set_configuration(config)\n</code></pre> <p>You can optionally print out these settings using the Rich python library to save for your records with:</p> <pre><code>from rich.pretty import pprint\npprint(config.dict())\n</code></pre> <p>Check out this tutorial for a more detailed example of saving <code>Properties</code>.</p>"},{"location":"tutorials/trigger/#acquire-data","title":"Acquire data","text":"<p>To begin acquisition:</p> <pre><code>runtime.start()\n</code></pre> <p>You can stop acquisition with <code>runtime.stop()</code> to stop after the max number of frames is collected or <code>runtime.abort()</code> to immediately stop acquisition. You must call one of these methods at the end of an acquisition, as <code>Runtime</code> deletes all of the objects created during acquisition to free up resources upon shutdown. Otherwise, an exception will be raised when trying to restart acquisition.</p> <p>Download this tutorial as a Python script</p>"}]}